{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.losses import sparse_categorical_crossentropy\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop, SGD\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "import keras_tuner as kt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('Unit_1.csv')\n",
    "df = df.drop(['Unnamed: 0'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multivariate Regression using DNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop('U1_ActivePower', axis=1)\n",
    "y = df['U1_ActivePower']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.3, random_state=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.fit_transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(128, input_dim=4, activation='relu'))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_4 (Dense)             (None, 128)               640       \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 64)                8256      \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 13,121\n",
      "Trainable params: 13,121\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss='mse', optimizer=Adam(0.0001), metrics=['mae'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\deguz\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\engine\\data_adapter.py:1699: FutureWarning: The behavior of `series[i:j]` with an integer-dtype index is deprecated. In a future version, this will be treated as *label-based* indexing, consistent with e.g. `series[i]` lookups. To retain the old behavior, use `series.iloc[i:j]`. To get the future behavior, use `series.loc[i:j]`.\n",
      "  return t[start:end]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "159/159 [==============================] - 2s 4ms/step - loss: 6908.3389 - mae: 83.0918 - val_loss: 6599.5103 - val_mae: 81.2138\n",
      "Epoch 2/300\n",
      "159/159 [==============================] - 0s 3ms/step - loss: 5582.1792 - mae: 74.4184 - val_loss: 4104.4438 - val_mae: 63.6110\n",
      "Epoch 3/300\n",
      "159/159 [==============================] - 0s 3ms/step - loss: 2379.8743 - mae: 45.9746 - val_loss: 1058.4993 - val_mae: 29.0267\n",
      "Epoch 4/300\n",
      "159/159 [==============================] - 0s 3ms/step - loss: 715.4733 - mae: 22.1496 - val_loss: 569.1254 - val_mae: 19.5535\n",
      "Epoch 5/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 516.5609 - mae: 18.7126 - val_loss: 470.2533 - val_mae: 17.9432\n",
      "Epoch 6/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 434.4402 - mae: 17.2149 - val_loss: 396.2138 - val_mae: 16.4381\n",
      "Epoch 7/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 367.8452 - mae: 15.8262 - val_loss: 333.5494 - val_mae: 15.0736\n",
      "Epoch 8/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 311.1577 - mae: 14.5078 - val_loss: 280.0924 - val_mae: 13.7834\n",
      "Epoch 9/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 261.5323 - mae: 13.2669 - val_loss: 232.9186 - val_mae: 12.4815\n",
      "Epoch 10/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 217.1277 - mae: 12.0296 - val_loss: 191.0393 - val_mae: 11.2876\n",
      "Epoch 11/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 177.8310 - mae: 10.8406 - val_loss: 154.5222 - val_mae: 10.0998\n",
      "Epoch 12/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 143.5579 - mae: 9.6936 - val_loss: 123.3621 - val_mae: 8.9795\n",
      "Epoch 13/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 114.0596 - mae: 8.5877 - val_loss: 96.7212 - val_mae: 7.9311\n",
      "Epoch 14/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 89.1619 - mae: 7.5495 - val_loss: 74.8180 - val_mae: 6.9175\n",
      "Epoch 15/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 68.9299 - mae: 6.5854 - val_loss: 57.3578 - val_mae: 6.0173\n",
      "Epoch 16/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 52.6336 - mae: 5.7125 - val_loss: 43.2185 - val_mae: 5.1785\n",
      "Epoch 17/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 39.7729 - mae: 4.9246 - val_loss: 32.4417 - val_mae: 4.4616\n",
      "Epoch 18/300\n",
      "159/159 [==============================] - 0s 3ms/step - loss: 29.8882 - mae: 4.2299 - val_loss: 24.1800 - val_mae: 3.8089\n",
      "Epoch 19/300\n",
      "159/159 [==============================] - 0s 3ms/step - loss: 22.4028 - mae: 3.6114 - val_loss: 18.0812 - val_mae: 3.2695\n",
      "Epoch 20/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 16.8736 - mae: 3.0931 - val_loss: 13.5095 - val_mae: 2.7699\n",
      "Epoch 21/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 12.8020 - mae: 2.6423 - val_loss: 10.2377 - val_mae: 2.3715\n",
      "Epoch 22/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 9.8172 - mae: 2.2693 - val_loss: 7.8831 - val_mae: 2.0523\n",
      "Epoch 23/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 7.6155 - mae: 1.9692 - val_loss: 6.1402 - val_mae: 1.7866\n",
      "Epoch 24/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 6.0121 - mae: 1.7236 - val_loss: 4.8616 - val_mae: 1.5656\n",
      "Epoch 25/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 4.8364 - mae: 1.5225 - val_loss: 3.9506 - val_mae: 1.3916\n",
      "Epoch 26/300\n",
      "159/159 [==============================] - 0s 3ms/step - loss: 3.9717 - mae: 1.3679 - val_loss: 3.2667 - val_mae: 1.2495\n",
      "Epoch 27/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 3.3177 - mae: 1.2393 - val_loss: 2.7635 - val_mae: 1.1488\n",
      "Epoch 28/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 2.8309 - mae: 1.1368 - val_loss: 2.3773 - val_mae: 1.0502\n",
      "Epoch 29/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 2.4561 - mae: 1.0540 - val_loss: 2.1053 - val_mae: 0.9983\n",
      "Epoch 30/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 2.1728 - mae: 0.9880 - val_loss: 1.8689 - val_mae: 0.9198\n",
      "Epoch 31/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 1.9518 - mae: 0.9360 - val_loss: 1.7016 - val_mae: 0.8809\n",
      "Epoch 32/300\n",
      "159/159 [==============================] - 0s 3ms/step - loss: 1.7512 - mae: 0.8785 - val_loss: 1.5152 - val_mae: 0.8212\n",
      "Epoch 33/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 1.5816 - mae: 0.8297 - val_loss: 1.3961 - val_mae: 0.7867\n",
      "Epoch 34/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 1.4541 - mae: 0.7932 - val_loss: 1.2956 - val_mae: 0.7470\n",
      "Epoch 35/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 1.3317 - mae: 0.7564 - val_loss: 1.1647 - val_mae: 0.7117\n",
      "Epoch 36/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 1.2151 - mae: 0.7211 - val_loss: 1.0694 - val_mae: 0.6669\n",
      "Epoch 37/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 1.1027 - mae: 0.6764 - val_loss: 0.9704 - val_mae: 0.6394\n",
      "Epoch 38/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 1.0106 - mae: 0.6383 - val_loss: 0.8985 - val_mae: 0.6082\n",
      "Epoch 39/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.9408 - mae: 0.6090 - val_loss: 0.8375 - val_mae: 0.5749\n",
      "Epoch 40/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.8837 - mae: 0.5797 - val_loss: 0.8020 - val_mae: 0.5527\n",
      "Epoch 41/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.8403 - mae: 0.5569 - val_loss: 0.7727 - val_mae: 0.5226\n",
      "Epoch 42/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.8078 - mae: 0.5382 - val_loss: 0.7310 - val_mae: 0.5006\n",
      "Epoch 43/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.7800 - mae: 0.5204 - val_loss: 0.7177 - val_mae: 0.4981\n",
      "Epoch 44/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.7537 - mae: 0.5051 - val_loss: 0.6900 - val_mae: 0.4857\n",
      "Epoch 45/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.7322 - mae: 0.4917 - val_loss: 0.6770 - val_mae: 0.4735\n",
      "Epoch 46/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.7155 - mae: 0.4830 - val_loss: 0.6636 - val_mae: 0.4624\n",
      "Epoch 47/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.7009 - mae: 0.4735 - val_loss: 0.6493 - val_mae: 0.4400\n",
      "Epoch 48/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.6855 - mae: 0.4623 - val_loss: 0.6700 - val_mae: 0.4834\n",
      "Epoch 49/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.6757 - mae: 0.4550 - val_loss: 0.6330 - val_mae: 0.4515\n",
      "Epoch 50/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.6636 - mae: 0.4484 - val_loss: 0.6249 - val_mae: 0.4336\n",
      "Epoch 51/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.6542 - mae: 0.4412 - val_loss: 0.6125 - val_mae: 0.4160\n",
      "Epoch 52/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.6470 - mae: 0.4330 - val_loss: 0.6030 - val_mae: 0.4097\n",
      "Epoch 53/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.6413 - mae: 0.4308 - val_loss: 0.6000 - val_mae: 0.4159\n",
      "Epoch 54/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.6337 - mae: 0.4270 - val_loss: 0.6131 - val_mae: 0.4438\n",
      "Epoch 55/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.6264 - mae: 0.4213 - val_loss: 0.5828 - val_mae: 0.3975\n",
      "Epoch 56/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.6174 - mae: 0.4129 - val_loss: 0.5776 - val_mae: 0.3913\n",
      "Epoch 57/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.6138 - mae: 0.4109 - val_loss: 0.5864 - val_mae: 0.4133\n",
      "Epoch 58/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.6101 - mae: 0.4109 - val_loss: 0.5718 - val_mae: 0.3907\n",
      "Epoch 59/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.6078 - mae: 0.4085 - val_loss: 0.5764 - val_mae: 0.3913\n",
      "Epoch 60/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5999 - mae: 0.4000 - val_loss: 0.5666 - val_mae: 0.3948\n",
      "Epoch 61/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.6013 - mae: 0.4037 - val_loss: 0.5621 - val_mae: 0.3805\n",
      "Epoch 62/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5983 - mae: 0.4016 - val_loss: 0.5881 - val_mae: 0.4329\n",
      "Epoch 63/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5941 - mae: 0.3979 - val_loss: 0.5586 - val_mae: 0.3770\n",
      "Epoch 64/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5904 - mae: 0.3958 - val_loss: 0.5651 - val_mae: 0.3850\n",
      "Epoch 65/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5927 - mae: 0.3975 - val_loss: 0.5564 - val_mae: 0.3711\n",
      "Epoch 66/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5860 - mae: 0.3893 - val_loss: 0.5498 - val_mae: 0.3644\n",
      "Epoch 67/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5836 - mae: 0.3887 - val_loss: 0.6073 - val_mae: 0.4438\n",
      "Epoch 68/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5806 - mae: 0.3862 - val_loss: 0.5459 - val_mae: 0.3696\n",
      "Epoch 69/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5787 - mae: 0.3855 - val_loss: 0.5606 - val_mae: 0.3883\n",
      "Epoch 70/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5852 - mae: 0.3950 - val_loss: 0.6397 - val_mae: 0.5202\n",
      "Epoch 71/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5856 - mae: 0.3928 - val_loss: 0.5448 - val_mae: 0.3667\n",
      "Epoch 72/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5744 - mae: 0.3799 - val_loss: 0.5531 - val_mae: 0.3621\n",
      "Epoch 73/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5771 - mae: 0.3829 - val_loss: 0.5443 - val_mae: 0.3730\n",
      "Epoch 74/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5756 - mae: 0.3812 - val_loss: 0.5889 - val_mae: 0.4112\n",
      "Epoch 75/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5689 - mae: 0.3774 - val_loss: 0.5393 - val_mae: 0.3563\n",
      "Epoch 76/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5741 - mae: 0.3824 - val_loss: 0.5749 - val_mae: 0.4371\n",
      "Epoch 77/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5802 - mae: 0.3921 - val_loss: 0.5617 - val_mae: 0.4100\n",
      "Epoch 78/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5700 - mae: 0.3797 - val_loss: 0.5357 - val_mae: 0.3619\n",
      "Epoch 79/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5708 - mae: 0.3785 - val_loss: 0.5538 - val_mae: 0.3842\n",
      "Epoch 80/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5679 - mae: 0.3772 - val_loss: 0.5431 - val_mae: 0.3734\n",
      "Epoch 81/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5654 - mae: 0.3737 - val_loss: 0.5393 - val_mae: 0.3594\n",
      "Epoch 82/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5653 - mae: 0.3721 - val_loss: 0.5371 - val_mae: 0.3571\n",
      "Epoch 83/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5691 - mae: 0.3790 - val_loss: 0.5430 - val_mae: 0.3553\n",
      "Epoch 84/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5647 - mae: 0.3736 - val_loss: 0.5412 - val_mae: 0.3808\n",
      "Epoch 85/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5634 - mae: 0.3731 - val_loss: 0.5324 - val_mae: 0.3484\n",
      "Epoch 86/300\n",
      "159/159 [==============================] - 0s 3ms/step - loss: 0.5633 - mae: 0.3709 - val_loss: 0.5584 - val_mae: 0.3845\n",
      "Epoch 87/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5638 - mae: 0.3727 - val_loss: 0.5347 - val_mae: 0.3599\n",
      "Epoch 88/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5631 - mae: 0.3713 - val_loss: 0.5426 - val_mae: 0.3801\n",
      "Epoch 89/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5658 - mae: 0.3745 - val_loss: 0.5777 - val_mae: 0.4185\n",
      "Epoch 90/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5674 - mae: 0.3775 - val_loss: 0.5363 - val_mae: 0.3522\n",
      "Epoch 91/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5631 - mae: 0.3745 - val_loss: 0.5816 - val_mae: 0.4384\n",
      "Epoch 92/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5660 - mae: 0.3752 - val_loss: 0.5363 - val_mae: 0.3646\n",
      "Epoch 93/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5612 - mae: 0.3705 - val_loss: 0.5519 - val_mae: 0.4067\n",
      "Epoch 94/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5613 - mae: 0.3716 - val_loss: 0.5352 - val_mae: 0.3574\n",
      "Epoch 95/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5584 - mae: 0.3682 - val_loss: 0.5378 - val_mae: 0.3505\n",
      "Epoch 96/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5599 - mae: 0.3718 - val_loss: 0.5392 - val_mae: 0.3717\n",
      "Epoch 97/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5587 - mae: 0.3684 - val_loss: 0.5378 - val_mae: 0.3805\n",
      "Epoch 98/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5612 - mae: 0.3702 - val_loss: 0.5280 - val_mae: 0.3550\n",
      "Epoch 99/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5621 - mae: 0.3723 - val_loss: 0.5464 - val_mae: 0.3544\n",
      "Epoch 100/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5571 - mae: 0.3673 - val_loss: 0.5429 - val_mae: 0.3789\n",
      "Epoch 101/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5564 - mae: 0.3634 - val_loss: 0.5258 - val_mae: 0.3439\n",
      "Epoch 102/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5577 - mae: 0.3664 - val_loss: 0.5294 - val_mae: 0.3550\n",
      "Epoch 103/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5551 - mae: 0.3639 - val_loss: 0.5283 - val_mae: 0.3454\n",
      "Epoch 104/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5574 - mae: 0.3660 - val_loss: 0.5390 - val_mae: 0.3794\n",
      "Epoch 105/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5571 - mae: 0.3687 - val_loss: 0.5293 - val_mae: 0.3476\n",
      "Epoch 106/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5569 - mae: 0.3661 - val_loss: 0.5274 - val_mae: 0.3419\n",
      "Epoch 107/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5544 - mae: 0.3630 - val_loss: 0.5349 - val_mae: 0.3590\n",
      "Epoch 108/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5600 - mae: 0.3690 - val_loss: 0.5337 - val_mae: 0.3515\n",
      "Epoch 109/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5610 - mae: 0.3735 - val_loss: 0.5325 - val_mae: 0.3531\n",
      "Epoch 110/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5513 - mae: 0.3610 - val_loss: 0.5307 - val_mae: 0.3433\n",
      "Epoch 111/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5562 - mae: 0.3687 - val_loss: 0.5374 - val_mae: 0.3714\n",
      "Epoch 112/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5628 - mae: 0.3727 - val_loss: 0.5763 - val_mae: 0.4335\n",
      "Epoch 113/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5506 - mae: 0.3578 - val_loss: 0.5299 - val_mae: 0.3657\n",
      "Epoch 114/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5530 - mae: 0.3631 - val_loss: 0.5487 - val_mae: 0.3823\n",
      "Epoch 115/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5569 - mae: 0.3657 - val_loss: 0.5247 - val_mae: 0.3396\n",
      "Epoch 116/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5564 - mae: 0.3691 - val_loss: 0.5286 - val_mae: 0.3511\n",
      "Epoch 117/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5573 - mae: 0.3665 - val_loss: 0.5324 - val_mae: 0.3588\n",
      "Epoch 118/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5544 - mae: 0.3633 - val_loss: 0.5574 - val_mae: 0.4251\n",
      "Epoch 119/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5537 - mae: 0.3647 - val_loss: 0.5466 - val_mae: 0.4063\n",
      "Epoch 120/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5510 - mae: 0.3608 - val_loss: 0.5296 - val_mae: 0.3622\n",
      "Epoch 121/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5551 - mae: 0.3654 - val_loss: 0.5232 - val_mae: 0.3349\n",
      "Epoch 122/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5524 - mae: 0.3628 - val_loss: 0.5397 - val_mae: 0.3876\n",
      "Epoch 123/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5545 - mae: 0.3621 - val_loss: 0.5193 - val_mae: 0.3378\n",
      "Epoch 124/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5568 - mae: 0.3681 - val_loss: 0.5371 - val_mae: 0.3637\n",
      "Epoch 125/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5512 - mae: 0.3605 - val_loss: 0.5213 - val_mae: 0.3450\n",
      "Epoch 126/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5498 - mae: 0.3582 - val_loss: 0.5272 - val_mae: 0.3377\n",
      "Epoch 127/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5556 - mae: 0.3663 - val_loss: 0.5256 - val_mae: 0.3373\n",
      "Epoch 128/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5514 - mae: 0.3579 - val_loss: 0.5541 - val_mae: 0.4118\n",
      "Epoch 129/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5529 - mae: 0.3662 - val_loss: 0.5257 - val_mae: 0.3567\n",
      "Epoch 130/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5521 - mae: 0.3613 - val_loss: 0.5229 - val_mae: 0.3478\n",
      "Epoch 131/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5514 - mae: 0.3610 - val_loss: 0.5219 - val_mae: 0.3413\n",
      "Epoch 132/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5510 - mae: 0.3590 - val_loss: 0.5224 - val_mae: 0.3497\n",
      "Epoch 133/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5491 - mae: 0.3557 - val_loss: 0.5557 - val_mae: 0.4066\n",
      "Epoch 134/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5547 - mae: 0.3638 - val_loss: 0.5303 - val_mae: 0.3427\n",
      "Epoch 135/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5594 - mae: 0.3715 - val_loss: 0.5608 - val_mae: 0.3693\n",
      "Epoch 136/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5502 - mae: 0.3608 - val_loss: 0.5256 - val_mae: 0.3518\n",
      "Epoch 137/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5489 - mae: 0.3582 - val_loss: 0.5319 - val_mae: 0.3645\n",
      "Epoch 138/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5475 - mae: 0.3537 - val_loss: 0.5324 - val_mae: 0.3704\n",
      "Epoch 139/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5489 - mae: 0.3593 - val_loss: 0.5310 - val_mae: 0.3482\n",
      "Epoch 140/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5479 - mae: 0.3588 - val_loss: 0.5395 - val_mae: 0.3535\n",
      "Epoch 141/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5501 - mae: 0.3604 - val_loss: 0.5262 - val_mae: 0.3629\n",
      "Epoch 142/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5469 - mae: 0.3548 - val_loss: 0.5475 - val_mae: 0.3646\n",
      "Epoch 143/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5517 - mae: 0.3627 - val_loss: 0.5264 - val_mae: 0.3473\n",
      "Epoch 144/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5522 - mae: 0.3631 - val_loss: 0.5255 - val_mae: 0.3352\n",
      "Epoch 145/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5503 - mae: 0.3573 - val_loss: 0.5260 - val_mae: 0.3653\n",
      "Epoch 146/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5487 - mae: 0.3604 - val_loss: 0.5351 - val_mae: 0.3505\n",
      "Epoch 147/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5518 - mae: 0.3617 - val_loss: 0.5246 - val_mae: 0.3405\n",
      "Epoch 148/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5495 - mae: 0.3593 - val_loss: 0.5275 - val_mae: 0.3668\n",
      "Epoch 149/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5485 - mae: 0.3617 - val_loss: 0.5195 - val_mae: 0.3339\n",
      "Epoch 150/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5482 - mae: 0.3578 - val_loss: 0.5246 - val_mae: 0.3392\n",
      "Epoch 151/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5511 - mae: 0.3606 - val_loss: 0.5356 - val_mae: 0.3399\n",
      "Epoch 152/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5487 - mae: 0.3594 - val_loss: 0.5191 - val_mae: 0.3410\n",
      "Epoch 153/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5459 - mae: 0.3566 - val_loss: 0.5171 - val_mae: 0.3350\n",
      "Epoch 154/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5481 - mae: 0.3597 - val_loss: 0.5249 - val_mae: 0.3354\n",
      "Epoch 155/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5494 - mae: 0.3599 - val_loss: 0.5244 - val_mae: 0.3531\n",
      "Epoch 156/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5461 - mae: 0.3546 - val_loss: 0.5201 - val_mae: 0.3340\n",
      "Epoch 157/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5471 - mae: 0.3554 - val_loss: 0.5375 - val_mae: 0.3822\n",
      "Epoch 158/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5454 - mae: 0.3573 - val_loss: 0.5230 - val_mae: 0.3368\n",
      "Epoch 159/300\n",
      "159/159 [==============================] - 0s 3ms/step - loss: 0.5523 - mae: 0.3653 - val_loss: 0.5249 - val_mae: 0.3361\n",
      "Epoch 160/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5489 - mae: 0.3616 - val_loss: 0.5209 - val_mae: 0.3323\n",
      "Epoch 161/300\n",
      "159/159 [==============================] - 0s 3ms/step - loss: 0.5515 - mae: 0.3621 - val_loss: 0.5237 - val_mae: 0.3484\n",
      "Epoch 162/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5453 - mae: 0.3533 - val_loss: 0.5323 - val_mae: 0.3641\n",
      "Epoch 163/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5431 - mae: 0.3514 - val_loss: 0.5270 - val_mae: 0.3504\n",
      "Epoch 164/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5497 - mae: 0.3600 - val_loss: 0.5229 - val_mae: 0.3536\n",
      "Epoch 165/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5488 - mae: 0.3604 - val_loss: 0.5202 - val_mae: 0.3395\n",
      "Epoch 166/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5415 - mae: 0.3500 - val_loss: 0.5349 - val_mae: 0.3464\n",
      "Epoch 167/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5477 - mae: 0.3582 - val_loss: 0.5227 - val_mae: 0.3444\n",
      "Epoch 168/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5487 - mae: 0.3577 - val_loss: 0.5442 - val_mae: 0.3472\n",
      "Epoch 169/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5440 - mae: 0.3515 - val_loss: 0.5398 - val_mae: 0.3363\n",
      "Epoch 170/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5456 - mae: 0.3551 - val_loss: 0.5461 - val_mae: 0.3491\n",
      "Epoch 171/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5436 - mae: 0.3510 - val_loss: 0.5255 - val_mae: 0.3467\n",
      "Epoch 172/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5443 - mae: 0.3521 - val_loss: 0.5238 - val_mae: 0.3331\n",
      "Epoch 173/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5455 - mae: 0.3592 - val_loss: 0.5233 - val_mae: 0.3340\n",
      "Epoch 174/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5447 - mae: 0.3534 - val_loss: 0.5216 - val_mae: 0.3325\n",
      "Epoch 175/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5474 - mae: 0.3590 - val_loss: 0.5172 - val_mae: 0.3273\n",
      "Epoch 176/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5448 - mae: 0.3547 - val_loss: 0.5382 - val_mae: 0.3710\n",
      "Epoch 177/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5466 - mae: 0.3575 - val_loss: 0.5343 - val_mae: 0.3670\n",
      "Epoch 178/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5470 - mae: 0.3564 - val_loss: 0.5233 - val_mae: 0.3478\n",
      "Epoch 179/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5466 - mae: 0.3565 - val_loss: 0.5207 - val_mae: 0.3477\n",
      "Epoch 180/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5487 - mae: 0.3615 - val_loss: 0.5177 - val_mae: 0.3405\n",
      "Epoch 181/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5503 - mae: 0.3598 - val_loss: 0.5370 - val_mae: 0.3877\n",
      "Epoch 182/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5450 - mae: 0.3564 - val_loss: 0.5221 - val_mae: 0.3515\n",
      "Epoch 183/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5434 - mae: 0.3535 - val_loss: 0.5408 - val_mae: 0.3671\n",
      "Epoch 184/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5519 - mae: 0.3649 - val_loss: 0.5371 - val_mae: 0.3711\n",
      "Epoch 185/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5444 - mae: 0.3567 - val_loss: 0.5169 - val_mae: 0.3285\n",
      "Epoch 186/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5433 - mae: 0.3551 - val_loss: 0.5173 - val_mae: 0.3360\n",
      "Epoch 187/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5454 - mae: 0.3545 - val_loss: 0.5227 - val_mae: 0.3483\n",
      "Epoch 188/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5471 - mae: 0.3602 - val_loss: 0.5437 - val_mae: 0.3557\n",
      "Epoch 189/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5430 - mae: 0.3533 - val_loss: 0.5455 - val_mae: 0.3890\n",
      "Epoch 190/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5461 - mae: 0.3565 - val_loss: 0.5354 - val_mae: 0.3390\n",
      "Epoch 191/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5421 - mae: 0.3509 - val_loss: 0.5229 - val_mae: 0.3394\n",
      "Epoch 192/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5449 - mae: 0.3529 - val_loss: 0.5201 - val_mae: 0.3244\n",
      "Epoch 193/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5481 - mae: 0.3586 - val_loss: 0.5173 - val_mae: 0.3412\n",
      "Epoch 194/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5447 - mae: 0.3557 - val_loss: 0.5205 - val_mae: 0.3304\n",
      "Epoch 195/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5446 - mae: 0.3574 - val_loss: 0.5235 - val_mae: 0.3437\n",
      "Epoch 196/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5434 - mae: 0.3535 - val_loss: 0.5314 - val_mae: 0.3545\n",
      "Epoch 197/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5466 - mae: 0.3568 - val_loss: 0.5173 - val_mae: 0.3277\n",
      "Epoch 198/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5455 - mae: 0.3558 - val_loss: 0.5185 - val_mae: 0.3323\n",
      "Epoch 199/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5426 - mae: 0.3547 - val_loss: 0.5274 - val_mae: 0.3438\n",
      "Epoch 200/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5442 - mae: 0.3543 - val_loss: 0.5349 - val_mae: 0.3507\n",
      "Epoch 201/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5446 - mae: 0.3545 - val_loss: 0.5273 - val_mae: 0.3344\n",
      "Epoch 202/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5429 - mae: 0.3538 - val_loss: 0.5265 - val_mae: 0.3474\n",
      "Epoch 203/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5409 - mae: 0.3493 - val_loss: 0.5450 - val_mae: 0.3462\n",
      "Epoch 204/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5415 - mae: 0.3517 - val_loss: 0.5210 - val_mae: 0.3379\n",
      "Epoch 205/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5467 - mae: 0.3551 - val_loss: 0.5409 - val_mae: 0.3968\n",
      "Epoch 206/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5436 - mae: 0.3538 - val_loss: 0.5298 - val_mae: 0.3514\n",
      "Epoch 207/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5452 - mae: 0.3545 - val_loss: 0.5337 - val_mae: 0.3638\n",
      "Epoch 208/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5484 - mae: 0.3596 - val_loss: 0.5275 - val_mae: 0.3367\n",
      "Epoch 209/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5435 - mae: 0.3551 - val_loss: 0.5204 - val_mae: 0.3357\n",
      "Epoch 210/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5496 - mae: 0.3634 - val_loss: 0.5438 - val_mae: 0.3507\n",
      "Epoch 211/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5420 - mae: 0.3536 - val_loss: 0.5202 - val_mae: 0.3441\n",
      "Epoch 212/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5407 - mae: 0.3488 - val_loss: 0.5329 - val_mae: 0.3799\n",
      "Epoch 213/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5411 - mae: 0.3506 - val_loss: 0.5324 - val_mae: 0.3441\n",
      "Epoch 214/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5395 - mae: 0.3475 - val_loss: 0.5211 - val_mae: 0.3262\n",
      "Epoch 215/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5421 - mae: 0.3512 - val_loss: 0.5163 - val_mae: 0.3310\n",
      "Epoch 216/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5392 - mae: 0.3477 - val_loss: 0.5165 - val_mae: 0.3343\n",
      "Epoch 217/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5468 - mae: 0.3545 - val_loss: 0.5307 - val_mae: 0.3661\n",
      "Epoch 218/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5424 - mae: 0.3546 - val_loss: 0.5231 - val_mae: 0.3326\n",
      "Epoch 219/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5441 - mae: 0.3553 - val_loss: 0.5190 - val_mae: 0.3278\n",
      "Epoch 220/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5448 - mae: 0.3554 - val_loss: 0.5357 - val_mae: 0.3700\n",
      "Epoch 221/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5451 - mae: 0.3571 - val_loss: 0.5195 - val_mae: 0.3292\n",
      "Epoch 222/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5416 - mae: 0.3526 - val_loss: 0.5255 - val_mae: 0.3296\n",
      "Epoch 223/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5408 - mae: 0.3526 - val_loss: 0.5475 - val_mae: 0.4063\n",
      "Epoch 224/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5398 - mae: 0.3497 - val_loss: 0.5216 - val_mae: 0.3419\n",
      "Epoch 225/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5414 - mae: 0.3516 - val_loss: 0.5249 - val_mae: 0.3372\n",
      "Epoch 226/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5417 - mae: 0.3534 - val_loss: 0.5203 - val_mae: 0.3478\n",
      "Epoch 227/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5409 - mae: 0.3530 - val_loss: 0.5487 - val_mae: 0.3966\n",
      "Epoch 228/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5439 - mae: 0.3545 - val_loss: 0.5217 - val_mae: 0.3337\n",
      "Epoch 229/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5390 - mae: 0.3510 - val_loss: 0.5261 - val_mae: 0.3533\n",
      "Epoch 230/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5482 - mae: 0.3605 - val_loss: 0.5273 - val_mae: 0.3560\n",
      "Epoch 231/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5391 - mae: 0.3496 - val_loss: 0.5195 - val_mae: 0.3343\n",
      "Epoch 232/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5424 - mae: 0.3550 - val_loss: 0.5338 - val_mae: 0.3490\n",
      "Epoch 233/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5419 - mae: 0.3525 - val_loss: 0.5219 - val_mae: 0.3395\n",
      "Epoch 234/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5424 - mae: 0.3518 - val_loss: 0.5282 - val_mae: 0.3549\n",
      "Epoch 235/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5418 - mae: 0.3538 - val_loss: 0.5259 - val_mae: 0.3633\n",
      "Epoch 236/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5489 - mae: 0.3640 - val_loss: 0.5164 - val_mae: 0.3274\n",
      "Epoch 237/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5435 - mae: 0.3562 - val_loss: 0.5277 - val_mae: 0.3556\n",
      "Epoch 238/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5419 - mae: 0.3518 - val_loss: 0.5182 - val_mae: 0.3429\n",
      "Epoch 239/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5401 - mae: 0.3476 - val_loss: 0.5200 - val_mae: 0.3331\n",
      "Epoch 240/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5414 - mae: 0.3540 - val_loss: 0.5341 - val_mae: 0.3624\n",
      "Epoch 241/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5431 - mae: 0.3563 - val_loss: 0.5239 - val_mae: 0.3548\n",
      "Epoch 242/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5462 - mae: 0.3597 - val_loss: 0.5315 - val_mae: 0.3474\n",
      "Epoch 243/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5407 - mae: 0.3497 - val_loss: 0.5174 - val_mae: 0.3358\n",
      "Epoch 244/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5398 - mae: 0.3520 - val_loss: 0.5208 - val_mae: 0.3395\n",
      "Epoch 245/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5399 - mae: 0.3493 - val_loss: 0.5176 - val_mae: 0.3378\n",
      "Epoch 246/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5386 - mae: 0.3477 - val_loss: 0.5173 - val_mae: 0.3265\n",
      "Epoch 247/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5402 - mae: 0.3517 - val_loss: 0.5197 - val_mae: 0.3295\n",
      "Epoch 248/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5404 - mae: 0.3496 - val_loss: 0.5359 - val_mae: 0.3516\n",
      "Epoch 249/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5364 - mae: 0.3465 - val_loss: 0.5241 - val_mae: 0.3458\n",
      "Epoch 250/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5385 - mae: 0.3483 - val_loss: 0.5245 - val_mae: 0.3508\n",
      "Epoch 251/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5489 - mae: 0.3620 - val_loss: 0.5555 - val_mae: 0.4086\n",
      "Epoch 252/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5423 - mae: 0.3565 - val_loss: 0.5292 - val_mae: 0.3717\n",
      "Epoch 253/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5394 - mae: 0.3492 - val_loss: 0.5168 - val_mae: 0.3216\n",
      "Epoch 254/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5403 - mae: 0.3496 - val_loss: 0.5289 - val_mae: 0.3641\n",
      "Epoch 255/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5432 - mae: 0.3542 - val_loss: 0.5266 - val_mae: 0.3610\n",
      "Epoch 256/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5444 - mae: 0.3603 - val_loss: 0.5317 - val_mae: 0.3473\n",
      "Epoch 257/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5439 - mae: 0.3560 - val_loss: 0.5211 - val_mae: 0.3474\n",
      "Epoch 258/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5354 - mae: 0.3419 - val_loss: 0.5267 - val_mae: 0.3444\n",
      "Epoch 259/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5449 - mae: 0.3556 - val_loss: 0.5208 - val_mae: 0.3452\n",
      "Epoch 260/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5414 - mae: 0.3540 - val_loss: 0.5231 - val_mae: 0.3449\n",
      "Epoch 261/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5419 - mae: 0.3545 - val_loss: 0.5411 - val_mae: 0.3643\n",
      "Epoch 262/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5451 - mae: 0.3580 - val_loss: 0.5301 - val_mae: 0.3578\n",
      "Epoch 263/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5383 - mae: 0.3524 - val_loss: 0.5286 - val_mae: 0.3673\n",
      "Epoch 264/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5350 - mae: 0.3424 - val_loss: 0.5262 - val_mae: 0.3506\n",
      "Epoch 265/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5354 - mae: 0.3462 - val_loss: 0.5221 - val_mae: 0.3300\n",
      "Epoch 266/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5379 - mae: 0.3495 - val_loss: 0.5209 - val_mae: 0.3518\n",
      "Epoch 267/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5394 - mae: 0.3500 - val_loss: 0.5186 - val_mae: 0.3390\n",
      "Epoch 268/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5400 - mae: 0.3532 - val_loss: 0.5761 - val_mae: 0.3950\n",
      "Epoch 269/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5510 - mae: 0.3640 - val_loss: 0.5422 - val_mae: 0.3377\n",
      "Epoch 270/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5387 - mae: 0.3472 - val_loss: 0.5203 - val_mae: 0.3241\n",
      "Epoch 271/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5375 - mae: 0.3475 - val_loss: 0.5401 - val_mae: 0.3905\n",
      "Epoch 272/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5394 - mae: 0.3505 - val_loss: 0.5259 - val_mae: 0.3497\n",
      "Epoch 273/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5428 - mae: 0.3520 - val_loss: 0.5143 - val_mae: 0.3255\n",
      "Epoch 274/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5386 - mae: 0.3474 - val_loss: 0.5264 - val_mae: 0.3454\n",
      "Epoch 275/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5411 - mae: 0.3554 - val_loss: 0.5157 - val_mae: 0.3295\n",
      "Epoch 276/300\n",
      "159/159 [==============================] - 0s 3ms/step - loss: 0.5381 - mae: 0.3454 - val_loss: 0.5273 - val_mae: 0.3574\n",
      "Epoch 277/300\n",
      "159/159 [==============================] - 0s 3ms/step - loss: 0.5406 - mae: 0.3523 - val_loss: 0.5210 - val_mae: 0.3304\n",
      "Epoch 278/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5399 - mae: 0.3516 - val_loss: 0.5169 - val_mae: 0.3399\n",
      "Epoch 279/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5393 - mae: 0.3515 - val_loss: 0.5749 - val_mae: 0.3906\n",
      "Epoch 280/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5476 - mae: 0.3603 - val_loss: 0.5623 - val_mae: 0.4343\n",
      "Epoch 281/300\n",
      "159/159 [==============================] - 0s 3ms/step - loss: 0.5380 - mae: 0.3466 - val_loss: 0.5176 - val_mae: 0.3245\n",
      "Epoch 282/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5386 - mae: 0.3492 - val_loss: 0.5322 - val_mae: 0.3795\n",
      "Epoch 283/300\n",
      "159/159 [==============================] - 0s 3ms/step - loss: 0.5493 - mae: 0.3608 - val_loss: 0.5156 - val_mae: 0.3278\n",
      "Epoch 284/300\n",
      "159/159 [==============================] - 0s 3ms/step - loss: 0.5378 - mae: 0.3466 - val_loss: 0.5503 - val_mae: 0.4077\n",
      "Epoch 285/300\n",
      "159/159 [==============================] - 0s 3ms/step - loss: 0.5376 - mae: 0.3502 - val_loss: 0.5303 - val_mae: 0.3595\n",
      "Epoch 286/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5411 - mae: 0.3520 - val_loss: 0.5183 - val_mae: 0.3277\n",
      "Epoch 287/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5364 - mae: 0.3491 - val_loss: 0.5162 - val_mae: 0.3325\n",
      "Epoch 288/300\n",
      "159/159 [==============================] - 0s 3ms/step - loss: 0.5410 - mae: 0.3492 - val_loss: 0.5453 - val_mae: 0.3648\n",
      "Epoch 289/300\n",
      "159/159 [==============================] - 0s 3ms/step - loss: 0.5419 - mae: 0.3555 - val_loss: 0.5230 - val_mae: 0.3313\n",
      "Epoch 290/300\n",
      "159/159 [==============================] - 0s 3ms/step - loss: 0.5363 - mae: 0.3465 - val_loss: 0.5361 - val_mae: 0.3372\n",
      "Epoch 291/300\n",
      "159/159 [==============================] - 0s 3ms/step - loss: 0.5329 - mae: 0.3402 - val_loss: 0.5169 - val_mae: 0.3253\n",
      "Epoch 292/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5392 - mae: 0.3510 - val_loss: 0.5288 - val_mae: 0.3476\n",
      "Epoch 293/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5378 - mae: 0.3484 - val_loss: 0.5257 - val_mae: 0.3448\n",
      "Epoch 294/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5424 - mae: 0.3589 - val_loss: 0.5565 - val_mae: 0.3554\n",
      "Epoch 295/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5437 - mae: 0.3577 - val_loss: 0.5693 - val_mae: 0.3888\n",
      "Epoch 296/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5426 - mae: 0.3554 - val_loss: 0.5427 - val_mae: 0.3881\n",
      "Epoch 297/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5405 - mae: 0.3510 - val_loss: 0.5205 - val_mae: 0.3229\n",
      "Epoch 298/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5372 - mae: 0.3482 - val_loss: 0.5357 - val_mae: 0.3747\n",
      "Epoch 299/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5380 - mae: 0.3483 - val_loss: 0.5430 - val_mae: 0.3882\n",
      "Epoch 300/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5350 - mae: 0.3473 - val_loss: 0.5208 - val_mae: 0.3443\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train, validation_split=0.2, epochs=300, batch_size=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparameter tuning using Keras Tuner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_builder(hp):\n",
    "  model = tf.keras.Sequential()\n",
    "  hp_units1 = hp.Int('units1', min_value=32, max_value=1024, step=32)\n",
    "  hp_units2 = hp.Int('units2', min_value=32, max_value=1024, step=32)\n",
    "  hp_units3 = hp.Int('units3', min_value=32, max_value=1024, step=32)\n",
    "  model.add(tf.keras.layers.Dense(units=hp_units1, input_dim=4, activation='relu'))\n",
    "  model.add(tf.keras.layers.Dense(units=hp_units2, activation='relu'))\n",
    "  model.add(tf.keras.layers.Dense(units=hp_units3, activation='relu'))\n",
    "  model.add(tf.keras.layers.Dense(1))\n",
    "  \n",
    "  hp_learning_rate = hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4])\n",
    "\n",
    "  model.compile(optimizer=Adam(learning_rate=hp_learning_rate),\n",
    "                loss='mse',\n",
    "                metrics=['mae'])\n",
    "\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reloading Oracle from existing project tuner_dir\\magat_kt\\oracle.json\n",
      "INFO:tensorflow:Reloading Tuner from tuner_dir\\magat_kt\\tuner0.json\n"
     ]
    }
   ],
   "source": [
    "tuner = kt.Hyperband(model_builder,\n",
    "                     objective='val_mae',\n",
    "                     max_epochs=10,\n",
    "                     directory='tuner_dir',\n",
    "                     project_name='magat_kt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_early = tf.keras.callbacks.EarlyStopping(monitor='val_mae', patience=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 90 Complete [00h 00m 14s]\n",
      "val_mae: 0.430368036031723\n",
      "\n",
      "Best val_mae So Far: 0.34634190797805786\n",
      "Total elapsed time: 00h 17m 36s\n",
      "INFO:tensorflow:Oracle triggered exit\n"
     ]
    }
   ],
   "source": [
    "tuner.search(X_train, y_train, epochs=10, validation_split=0.2, callbacks=[stop_early])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "units1 896\n",
      "units2 256\n",
      "units3 224\n",
      "learning_rate 0.001\n"
     ]
    }
   ],
   "source": [
    "for h_param in [f\"units{i}\" for i in range(1,4)] + ['learning_rate']:\n",
    "  print(h_param, tuner.get_best_hyperparameters()[0].get(h_param))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Retraining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(896, input_dim=4, activation='relu'))\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dense(224, activation='relu'))\n",
    "model.add(Dense(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_12 (Dense)            (None, 896)               4480      \n",
      "                                                                 \n",
      " dense_13 (Dense)            (None, 256)               229632    \n",
      "                                                                 \n",
      " dense_14 (Dense)            (None, 224)               57568     \n",
      "                                                                 \n",
      " dense_15 (Dense)            (None, 1)                 225       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 291,905\n",
      "Trainable params: 291,905\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss='mse', optimizer=Adam(0.001), metrics=['mae'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpointer = ModelCheckpoint(filepath='checkpoint_dir/training_0.ckpt', \n",
    "                               monitor='val_mae', mode='min',\n",
    "                               verbose=1, save_best_only=True)\n",
    "early_stopping = EarlyStopping(monitor='val_mae', mode='min', verbose=1, patience=20)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_mae', factor=0.1, patience=7, min_lr=0.00001, verbose=1)\n",
    "callbacks=[early_stopping, reduce_lr, checkpointer]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\deguz\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\engine\\data_adapter.py:1699: FutureWarning: The behavior of `series[i:j]` with an integer-dtype index is deprecated. In a future version, this will be treated as *label-based* indexing, consistent with e.g. `series[i]` lookups. To retain the old behavior, use `series.iloc[i:j]`. To get the future behavior, use `series.loc[i:j]`.\n",
      "  return t[start:end]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "625/636 [============================>.] - ETA: 0s - loss: 271.1884 - mae: 6.7633\n",
      "Epoch 1: val_mae improved from inf to 0.59608, saving model to checkpoint_dir\\training_0.ckpt\n",
      "INFO:tensorflow:Assets written to: checkpoint_dir\\training_0.ckpt\\assets\n",
      "636/636 [==============================] - 2s 3ms/step - loss: 266.5719 - mae: 6.6609 - val_loss: 0.9418 - val_mae: 0.5961 - lr: 0.0010\n",
      "Epoch 2/1000\n",
      "621/636 [============================>.] - ETA: 0s - loss: 0.7893 - mae: 0.5208\n",
      "Epoch 2: val_mae improved from 0.59608 to 0.40949, saving model to checkpoint_dir\\training_0.ckpt\n",
      "INFO:tensorflow:Assets written to: checkpoint_dir\\training_0.ckpt\\assets\n",
      "636/636 [==============================] - 2s 3ms/step - loss: 0.7891 - mae: 0.5193 - val_loss: 0.5930 - val_mae: 0.4095 - lr: 0.0010\n",
      "Epoch 3/1000\n",
      "624/636 [============================>.] - ETA: 0s - loss: 0.7376 - mae: 0.5076\n",
      "Epoch 3: val_mae did not improve from 0.40949\n",
      "636/636 [==============================] - 1s 2ms/step - loss: 0.7440 - mae: 0.5101 - val_loss: 1.1410 - val_mae: 0.7254 - lr: 0.0010\n",
      "Epoch 4/1000\n",
      "617/636 [============================>.] - ETA: 0s - loss: 0.7799 - mae: 0.5458\n",
      "Epoch 4: val_mae improved from 0.40949 to 0.38099, saving model to checkpoint_dir\\training_0.ckpt\n",
      "INFO:tensorflow:Assets written to: checkpoint_dir\\training_0.ckpt\\assets\n",
      "636/636 [==============================] - 2s 3ms/step - loss: 0.7738 - mae: 0.5427 - val_loss: 0.5655 - val_mae: 0.3810 - lr: 0.0010\n",
      "Epoch 5/1000\n",
      "615/636 [============================>.] - ETA: 0s - loss: 0.8862 - mae: 0.6180\n",
      "Epoch 5: val_mae did not improve from 0.38099\n",
      "636/636 [==============================] - 1s 2ms/step - loss: 0.8839 - mae: 0.6183 - val_loss: 0.6281 - val_mae: 0.4378 - lr: 0.0010\n",
      "Epoch 6/1000\n",
      "613/636 [===========================>..] - ETA: 0s - loss: 0.9228 - mae: 0.6275\n",
      "Epoch 6: val_mae did not improve from 0.38099\n",
      "636/636 [==============================] - 2s 2ms/step - loss: 0.9343 - mae: 0.6327 - val_loss: 1.4705 - val_mae: 0.9748 - lr: 0.0010\n",
      "Epoch 7/1000\n",
      "631/636 [============================>.] - ETA: 0s - loss: 0.9808 - mae: 0.6671\n",
      "Epoch 7: val_mae did not improve from 0.38099\n",
      "636/636 [==============================] - 1s 2ms/step - loss: 0.9831 - mae: 0.6680 - val_loss: 0.7699 - val_mae: 0.6019 - lr: 0.0010\n",
      "Epoch 8/1000\n",
      "622/636 [============================>.] - ETA: 0s - loss: 1.0007 - mae: 0.6834\n",
      "Epoch 8: val_mae did not improve from 0.38099\n",
      "636/636 [==============================] - 1s 2ms/step - loss: 0.9967 - mae: 0.6816 - val_loss: 0.7700 - val_mae: 0.6014 - lr: 0.0010\n",
      "Epoch 9/1000\n",
      "616/636 [============================>.] - ETA: 0s - loss: 1.0536 - mae: 0.7075\n",
      "Epoch 9: val_mae did not improve from 0.38099\n",
      "636/636 [==============================] - 1s 2ms/step - loss: 1.0595 - mae: 0.7123 - val_loss: 0.9229 - val_mae: 0.7269 - lr: 0.0010\n",
      "Epoch 10/1000\n",
      "625/636 [============================>.] - ETA: 0s - loss: 1.0704 - mae: 0.7225\n",
      "Epoch 10: val_mae did not improve from 0.38099\n",
      "636/636 [==============================] - 1s 2ms/step - loss: 1.0775 - mae: 0.7247 - val_loss: 1.2022 - val_mae: 0.8092 - lr: 0.0010\n",
      "Epoch 11/1000\n",
      "616/636 [============================>.] - ETA: 0s - loss: 0.9734 - mae: 0.6612\n",
      "Epoch 11: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "\n",
      "Epoch 11: val_mae did not improve from 0.38099\n",
      "636/636 [==============================] - 1s 2ms/step - loss: 0.9763 - mae: 0.6641 - val_loss: 2.7599 - val_mae: 1.4048 - lr: 0.0010\n",
      "Epoch 12/1000\n",
      "623/636 [============================>.] - ETA: 0s - loss: 0.5937 - mae: 0.3896\n",
      "Epoch 12: val_mae did not improve from 0.38099\n",
      "636/636 [==============================] - 1s 2ms/step - loss: 0.5959 - mae: 0.3899 - val_loss: 0.5649 - val_mae: 0.4098 - lr: 1.0000e-04\n",
      "Epoch 13/1000\n",
      "632/636 [============================>.] - ETA: 0s - loss: 0.5779 - mae: 0.3828\n",
      "Epoch 13: val_mae improved from 0.38099 to 0.34413, saving model to checkpoint_dir\\training_0.ckpt\n",
      "INFO:tensorflow:Assets written to: checkpoint_dir\\training_0.ckpt\\assets\n",
      "636/636 [==============================] - 2s 3ms/step - loss: 0.5776 - mae: 0.3831 - val_loss: 0.5382 - val_mae: 0.3441 - lr: 1.0000e-04\n",
      "Epoch 14/1000\n",
      "620/636 [============================>.] - ETA: 0s - loss: 0.5807 - mae: 0.3864\n",
      "Epoch 14: val_mae did not improve from 0.34413\n",
      "636/636 [==============================] - 1s 2ms/step - loss: 0.5851 - mae: 0.3872 - val_loss: 0.5645 - val_mae: 0.4267 - lr: 1.0000e-04\n",
      "Epoch 15/1000\n",
      "628/636 [============================>.] - ETA: 0s - loss: 0.5891 - mae: 0.3911\n",
      "Epoch 15: val_mae improved from 0.34413 to 0.32743, saving model to checkpoint_dir\\training_0.ckpt\n",
      "INFO:tensorflow:Assets written to: checkpoint_dir\\training_0.ckpt\\assets\n",
      "636/636 [==============================] - 2s 3ms/step - loss: 0.5904 - mae: 0.3913 - val_loss: 0.5181 - val_mae: 0.3274 - lr: 1.0000e-04\n",
      "Epoch 16/1000\n",
      "634/636 [============================>.] - ETA: 0s - loss: 0.5942 - mae: 0.4026\n",
      "Epoch 16: val_mae did not improve from 0.32743\n",
      "636/636 [==============================] - 2s 2ms/step - loss: 0.5937 - mae: 0.4024 - val_loss: 0.5451 - val_mae: 0.3557 - lr: 1.0000e-04\n",
      "Epoch 17/1000\n",
      "612/636 [===========================>..] - ETA: 0s - loss: 0.5979 - mae: 0.4054\n",
      "Epoch 17: val_mae did not improve from 0.32743\n",
      "636/636 [==============================] - 1s 2ms/step - loss: 0.5980 - mae: 0.4052 - val_loss: 0.6844 - val_mae: 0.5745 - lr: 1.0000e-04\n",
      "Epoch 18/1000\n",
      "632/636 [============================>.] - ETA: 0s - loss: 0.5990 - mae: 0.4074\n",
      "Epoch 18: val_mae did not improve from 0.32743\n",
      "636/636 [==============================] - 1s 2ms/step - loss: 0.5981 - mae: 0.4070 - val_loss: 0.5409 - val_mae: 0.3761 - lr: 1.0000e-04\n",
      "Epoch 19/1000\n",
      "624/636 [============================>.] - ETA: 0s - loss: 0.5980 - mae: 0.4089\n",
      "Epoch 19: val_mae did not improve from 0.32743\n",
      "636/636 [==============================] - 1s 2ms/step - loss: 0.6013 - mae: 0.4105 - val_loss: 0.5920 - val_mae: 0.4073 - lr: 1.0000e-04\n",
      "Epoch 20/1000\n",
      "613/636 [===========================>..] - ETA: 0s - loss: 0.5986 - mae: 0.4053\n",
      "Epoch 20: val_mae did not improve from 0.32743\n",
      "636/636 [==============================] - 1s 2ms/step - loss: 0.5945 - mae: 0.4039 - val_loss: 0.5602 - val_mae: 0.4041 - lr: 1.0000e-04\n",
      "Epoch 21/1000\n",
      "616/636 [============================>.] - ETA: 0s - loss: 0.6119 - mae: 0.4161\n",
      "Epoch 21: val_mae did not improve from 0.32743\n",
      "636/636 [==============================] - 1s 2ms/step - loss: 0.6094 - mae: 0.4149 - val_loss: 0.5898 - val_mae: 0.4008 - lr: 1.0000e-04\n",
      "Epoch 22/1000\n",
      "615/636 [============================>.] - ETA: 0s - loss: 0.6122 - mae: 0.4169\n",
      "Epoch 22: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "\n",
      "Epoch 22: val_mae did not improve from 0.32743\n",
      "636/636 [==============================] - 1s 2ms/step - loss: 0.6095 - mae: 0.4179 - val_loss: 0.5490 - val_mae: 0.4080 - lr: 1.0000e-04\n",
      "Epoch 23/1000\n",
      "625/636 [============================>.] - ETA: 0s - loss: 0.5501 - mae: 0.3462\n",
      "Epoch 23: val_mae did not improve from 0.32743\n",
      "636/636 [==============================] - 1s 2ms/step - loss: 0.5485 - mae: 0.3460 - val_loss: 0.5205 - val_mae: 0.3475 - lr: 1.0000e-05\n",
      "Epoch 24/1000\n",
      "621/636 [============================>.] - ETA: 0s - loss: 0.5486 - mae: 0.3438\n",
      "Epoch 24: val_mae improved from 0.32743 to 0.32235, saving model to checkpoint_dir\\training_0.ckpt\n",
      "INFO:tensorflow:Assets written to: checkpoint_dir\\training_0.ckpt\\assets\n",
      "636/636 [==============================] - 2s 3ms/step - loss: 0.5480 - mae: 0.3440 - val_loss: 0.5187 - val_mae: 0.3223 - lr: 1.0000e-05\n",
      "Epoch 25/1000\n",
      "635/636 [============================>.] - ETA: 0s - loss: 0.5458 - mae: 0.3460\n",
      "Epoch 25: val_mae did not improve from 0.32235\n",
      "636/636 [==============================] - 1s 2ms/step - loss: 0.5464 - mae: 0.3463 - val_loss: 0.5225 - val_mae: 0.3529 - lr: 1.0000e-05\n",
      "Epoch 26/1000\n",
      "619/636 [============================>.] - ETA: 0s - loss: 0.5484 - mae: 0.3450\n",
      "Epoch 26: val_mae did not improve from 0.32235\n",
      "636/636 [==============================] - 1s 2ms/step - loss: 0.5478 - mae: 0.3451 - val_loss: 0.5358 - val_mae: 0.3875 - lr: 1.0000e-05\n",
      "Epoch 27/1000\n",
      "611/636 [===========================>..] - ETA: 0s - loss: 0.5489 - mae: 0.3445\n",
      "Epoch 27: val_mae did not improve from 0.32235\n",
      "636/636 [==============================] - 2s 2ms/step - loss: 0.5470 - mae: 0.3436 - val_loss: 0.5175 - val_mae: 0.3368 - lr: 1.0000e-05\n",
      "Epoch 28/1000\n",
      "621/636 [============================>.] - ETA: 0s - loss: 0.5479 - mae: 0.3458\n",
      "Epoch 28: val_mae did not improve from 0.32235\n",
      "636/636 [==============================] - 1s 2ms/step - loss: 0.5473 - mae: 0.3449 - val_loss: 0.5146 - val_mae: 0.3229 - lr: 1.0000e-05\n",
      "Epoch 29/1000\n",
      "611/636 [===========================>..] - ETA: 0s - loss: 0.5483 - mae: 0.3454\n",
      "Epoch 29: val_mae did not improve from 0.32235\n",
      "636/636 [==============================] - 1s 2ms/step - loss: 0.5473 - mae: 0.3449 - val_loss: 0.5181 - val_mae: 0.3236 - lr: 1.0000e-05\n",
      "Epoch 30/1000\n",
      "617/636 [============================>.] - ETA: 0s - loss: 0.5455 - mae: 0.3432\n",
      "Epoch 30: val_mae did not improve from 0.32235\n",
      "636/636 [==============================] - 1s 2ms/step - loss: 0.5474 - mae: 0.3431 - val_loss: 0.5233 - val_mae: 0.3488 - lr: 1.0000e-05\n",
      "Epoch 31/1000\n",
      "636/636 [==============================] - ETA: 0s - loss: 0.5493 - mae: 0.3465\n",
      "Epoch 31: ReduceLROnPlateau reducing learning rate to 1e-05.\n",
      "\n",
      "Epoch 31: val_mae improved from 0.32235 to 0.32225, saving model to checkpoint_dir\\training_0.ckpt\n",
      "INFO:tensorflow:Assets written to: checkpoint_dir\\training_0.ckpt\\assets\n",
      "636/636 [==============================] - 2s 3ms/step - loss: 0.5493 - mae: 0.3465 - val_loss: 0.5140 - val_mae: 0.3222 - lr: 1.0000e-05\n",
      "Epoch 32/1000\n",
      "623/636 [============================>.] - ETA: 0s - loss: 0.5453 - mae: 0.3424\n",
      "Epoch 32: val_mae improved from 0.32225 to 0.31921, saving model to checkpoint_dir\\training_0.ckpt\n",
      "INFO:tensorflow:Assets written to: checkpoint_dir\\training_0.ckpt\\assets\n",
      "636/636 [==============================] - 2s 3ms/step - loss: 0.5469 - mae: 0.3428 - val_loss: 0.5143 - val_mae: 0.3192 - lr: 1.0000e-05\n",
      "Epoch 33/1000\n",
      "629/636 [============================>.] - ETA: 0s - loss: 0.5446 - mae: 0.3432\n",
      "Epoch 33: val_mae did not improve from 0.31921\n",
      "636/636 [==============================] - 1s 2ms/step - loss: 0.5476 - mae: 0.3447 - val_loss: 0.5298 - val_mae: 0.3723 - lr: 1.0000e-05\n",
      "Epoch 34/1000\n",
      "615/636 [============================>.] - ETA: 0s - loss: 0.5497 - mae: 0.3446\n",
      "Epoch 34: val_mae did not improve from 0.31921\n",
      "636/636 [==============================] - 1s 2ms/step - loss: 0.5477 - mae: 0.3440 - val_loss: 0.5134 - val_mae: 0.3220 - lr: 1.0000e-05\n",
      "Epoch 35/1000\n",
      "630/636 [============================>.] - ETA: 0s - loss: 0.5477 - mae: 0.3461\n",
      "Epoch 35: val_mae did not improve from 0.31921\n",
      "636/636 [==============================] - 2s 2ms/step - loss: 0.5472 - mae: 0.3459 - val_loss: 0.5217 - val_mae: 0.3483 - lr: 1.0000e-05\n",
      "Epoch 36/1000\n",
      "617/636 [============================>.] - ETA: 0s - loss: 0.5521 - mae: 0.3483\n",
      "Epoch 36: val_mae did not improve from 0.31921\n",
      "636/636 [==============================] - 1s 2ms/step - loss: 0.5502 - mae: 0.3475 - val_loss: 0.5265 - val_mae: 0.3666 - lr: 1.0000e-05\n",
      "Epoch 37/1000\n",
      "635/636 [============================>.] - ETA: 0s - loss: 0.5461 - mae: 0.3454\n",
      "Epoch 37: val_mae did not improve from 0.31921\n",
      "636/636 [==============================] - 2s 2ms/step - loss: 0.5461 - mae: 0.3455 - val_loss: 0.5183 - val_mae: 0.3221 - lr: 1.0000e-05\n",
      "Epoch 38/1000\n",
      "625/636 [============================>.] - ETA: 0s - loss: 0.5446 - mae: 0.3416\n",
      "Epoch 38: val_mae did not improve from 0.31921\n",
      "636/636 [==============================] - 1s 2ms/step - loss: 0.5465 - mae: 0.3423 - val_loss: 0.5206 - val_mae: 0.3457 - lr: 1.0000e-05\n",
      "Epoch 39/1000\n",
      "631/636 [============================>.] - ETA: 0s - loss: 0.5486 - mae: 0.3442\n",
      "Epoch 39: val_mae did not improve from 0.31921\n",
      "636/636 [==============================] - 2s 2ms/step - loss: 0.5477 - mae: 0.3440 - val_loss: 0.5171 - val_mae: 0.3390 - lr: 1.0000e-05\n",
      "Epoch 40/1000\n",
      "623/636 [============================>.] - ETA: 0s - loss: 0.5447 - mae: 0.3426\n",
      "Epoch 40: val_mae did not improve from 0.31921\n",
      "636/636 [==============================] - 1s 2ms/step - loss: 0.5467 - mae: 0.3438 - val_loss: 0.5238 - val_mae: 0.3587 - lr: 1.0000e-05\n",
      "Epoch 41/1000\n",
      "631/636 [============================>.] - ETA: 0s - loss: 0.5460 - mae: 0.3428\n",
      "Epoch 41: val_mae did not improve from 0.31921\n",
      "636/636 [==============================] - 1s 2ms/step - loss: 0.5458 - mae: 0.3425 - val_loss: 0.5158 - val_mae: 0.3225 - lr: 1.0000e-05\n",
      "Epoch 42/1000\n",
      "633/636 [============================>.] - ETA: 0s - loss: 0.5476 - mae: 0.3454\n",
      "Epoch 42: val_mae did not improve from 0.31921\n",
      "636/636 [==============================] - 1s 2ms/step - loss: 0.5478 - mae: 0.3455 - val_loss: 0.5209 - val_mae: 0.3372 - lr: 1.0000e-05\n",
      "Epoch 43/1000\n",
      "628/636 [============================>.] - ETA: 0s - loss: 0.5446 - mae: 0.3423\n",
      "Epoch 43: val_mae did not improve from 0.31921\n",
      "636/636 [==============================] - 2s 2ms/step - loss: 0.5469 - mae: 0.3428 - val_loss: 0.5211 - val_mae: 0.3525 - lr: 1.0000e-05\n",
      "Epoch 44/1000\n",
      "624/636 [============================>.] - ETA: 0s - loss: 0.5492 - mae: 0.3448\n",
      "Epoch 44: val_mae did not improve from 0.31921\n",
      "636/636 [==============================] - 1s 2ms/step - loss: 0.5455 - mae: 0.3438 - val_loss: 0.5173 - val_mae: 0.3201 - lr: 1.0000e-05\n",
      "Epoch 45/1000\n",
      "628/636 [============================>.] - ETA: 0s - loss: 0.5477 - mae: 0.3436\n",
      "Epoch 45: val_mae did not improve from 0.31921\n",
      "636/636 [==============================] - 1s 2ms/step - loss: 0.5471 - mae: 0.3437 - val_loss: 0.5145 - val_mae: 0.3256 - lr: 1.0000e-05\n",
      "Epoch 46/1000\n",
      "614/636 [===========================>..] - ETA: 0s - loss: 0.5457 - mae: 0.3440\n",
      "Epoch 46: val_mae did not improve from 0.31921\n",
      "636/636 [==============================] - 1s 2ms/step - loss: 0.5467 - mae: 0.3446 - val_loss: 0.5178 - val_mae: 0.3291 - lr: 1.0000e-05\n",
      "Epoch 47/1000\n",
      "617/636 [============================>.] - ETA: 0s - loss: 0.5477 - mae: 0.3465\n",
      "Epoch 47: val_mae did not improve from 0.31921\n",
      "636/636 [==============================] - 1s 2ms/step - loss: 0.5474 - mae: 0.3463 - val_loss: 0.5135 - val_mae: 0.3241 - lr: 1.0000e-05\n",
      "Epoch 48/1000\n",
      "632/636 [============================>.] - ETA: 0s - loss: 0.5474 - mae: 0.3446\n",
      "Epoch 48: val_mae did not improve from 0.31921\n",
      "636/636 [==============================] - 1s 2ms/step - loss: 0.5467 - mae: 0.3444 - val_loss: 0.5331 - val_mae: 0.3280 - lr: 1.0000e-05\n",
      "Epoch 49/1000\n",
      "619/636 [============================>.] - ETA: 0s - loss: 0.5480 - mae: 0.3429\n",
      "Epoch 49: val_mae did not improve from 0.31921\n",
      "636/636 [==============================] - 2s 2ms/step - loss: 0.5465 - mae: 0.3428 - val_loss: 0.5161 - val_mae: 0.3269 - lr: 1.0000e-05\n",
      "Epoch 50/1000\n",
      "624/636 [============================>.] - ETA: 0s - loss: 0.5442 - mae: 0.3440\n",
      "Epoch 50: val_mae did not improve from 0.31921\n",
      "636/636 [==============================] - 1s 2ms/step - loss: 0.5469 - mae: 0.3447 - val_loss: 0.5197 - val_mae: 0.3479 - lr: 1.0000e-05\n",
      "Epoch 51/1000\n",
      "634/636 [============================>.] - ETA: 0s - loss: 0.5462 - mae: 0.3432\n",
      "Epoch 51: val_mae did not improve from 0.31921\n",
      "636/636 [==============================] - 1s 2ms/step - loss: 0.5461 - mae: 0.3433 - val_loss: 0.5176 - val_mae: 0.3345 - lr: 1.0000e-05\n",
      "Epoch 52/1000\n",
      "623/636 [============================>.] - ETA: 0s - loss: 0.5491 - mae: 0.3450\n",
      "Epoch 52: val_mae did not improve from 0.31921\n",
      "636/636 [==============================] - 2s 2ms/step - loss: 0.5478 - mae: 0.3447 - val_loss: 0.5216 - val_mae: 0.3529 - lr: 1.0000e-05\n",
      "Epoch 52: early stopping\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train, validation_split=0.2, epochs=1000, callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Going Deeper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(1024, input_dim=4, activation='relu'))\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_10\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_46 (Dense)            (None, 1024)              5120      \n",
      "                                                                 \n",
      " dense_47 (Dense)            (None, 512)               524800    \n",
      "                                                                 \n",
      " dense_48 (Dense)            (None, 256)               131328    \n",
      "                                                                 \n",
      " dense_49 (Dense)            (None, 128)               32896     \n",
      "                                                                 \n",
      " dense_50 (Dense)            (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 694,273\n",
      "Trainable params: 694,273\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss='mse', optimizer=Adam(0.01), metrics=['mae'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpointer = ModelCheckpoint(filepath='checkpoint_dir/training_1.ckpt', \n",
    "                               monitor='val_mae', mode='min',\n",
    "                               verbose=1, save_best_only=True)\n",
    "early_stopping = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=20)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_mae', factor=0.1, patience=3, min_lr=0.00001, verbose=1)\n",
    "callbacks=[early_stopping, checkpointer, reduce_lr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\deguz\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\engine\\data_adapter.py:1699: FutureWarning: The behavior of `series[i:j]` with an integer-dtype index is deprecated. In a future version, this will be treated as *label-based* indexing, consistent with e.g. `series[i]` lookups. To retain the old behavior, use `series.iloc[i:j]`. To get the future behavior, use `series.loc[i:j]`.\n",
      "  return t[start:end]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "620/636 [============================>.] - ETA: 0s - loss: 319.9737 - mae: 5.4202\n",
      "Epoch 1: val_mae improved from inf to 0.95133, saving model to checkpoint_dir\\training_1.ckpt\n",
      "INFO:tensorflow:Assets written to: checkpoint_dir\\training_1.ckpt\\assets\n",
      "636/636 [==============================] - 2s 3ms/step - loss: 312.0320 - mae: 5.3097 - val_loss: 1.4920 - val_mae: 0.9513 - lr: 0.0100\n",
      "Epoch 2/1000\n",
      "614/636 [===========================>..] - ETA: 0s - loss: 1.6679 - mae: 0.9464\n",
      "Epoch 2: val_mae did not improve from 0.95133\n",
      "636/636 [==============================] - 2s 2ms/step - loss: 1.7404 - mae: 0.9659 - val_loss: 2.5188 - val_mae: 1.2392 - lr: 0.0100\n",
      "Epoch 3/1000\n",
      "620/636 [============================>.] - ETA: 0s - loss: 2.4898 - mae: 1.1819\n",
      "Epoch 3: val_mae did not improve from 0.95133\n",
      "636/636 [==============================] - 2s 2ms/step - loss: 2.4757 - mae: 1.1781 - val_loss: 2.4256 - val_mae: 1.1278 - lr: 0.0100\n",
      "Epoch 4/1000\n",
      "623/636 [============================>.] - ETA: 0s - loss: 2.2517 - mae: 1.1094\n",
      "Epoch 4: val_mae did not improve from 0.95133\n",
      "\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 0.0009999999776482583.\n",
      "636/636 [==============================] - 2s 3ms/step - loss: 2.4439 - mae: 1.1427 - val_loss: 14.0555 - val_mae: 2.8504 - lr: 0.0100\n",
      "Epoch 5/1000\n",
      "625/636 [============================>.] - ETA: 0s - loss: 0.7546 - mae: 0.4717\n",
      "Epoch 5: val_mae improved from 0.95133 to 0.40150, saving model to checkpoint_dir\\training_1.ckpt\n",
      "INFO:tensorflow:Assets written to: checkpoint_dir\\training_1.ckpt\\assets\n",
      "636/636 [==============================] - 2s 3ms/step - loss: 0.7558 - mae: 0.4718 - val_loss: 0.5651 - val_mae: 0.4015 - lr: 1.0000e-03\n",
      "Epoch 6/1000\n",
      "627/636 [============================>.] - ETA: 0s - loss: 0.6110 - mae: 0.4207\n",
      "Epoch 6: val_mae did not improve from 0.40150\n",
      "636/636 [==============================] - 2s 2ms/step - loss: 0.6081 - mae: 0.4200 - val_loss: 0.5668 - val_mae: 0.4222 - lr: 1.0000e-03\n",
      "Epoch 7/1000\n",
      "635/636 [============================>.] - ETA: 0s - loss: 0.6434 - mae: 0.4526\n",
      "Epoch 7: val_mae improved from 0.40150 to 0.37951, saving model to checkpoint_dir\\training_1.ckpt\n",
      "INFO:tensorflow:Assets written to: checkpoint_dir\\training_1.ckpt\\assets\n",
      "636/636 [==============================] - 2s 3ms/step - loss: 0.6427 - mae: 0.4524 - val_loss: 0.5373 - val_mae: 0.3795 - lr: 1.0000e-03\n",
      "Epoch 8/1000\n",
      "629/636 [============================>.] - ETA: 0s - loss: 0.6733 - mae: 0.4780\n",
      "Epoch 8: val_mae did not improve from 0.37951\n",
      "636/636 [==============================] - 2s 3ms/step - loss: 0.6723 - mae: 0.4774 - val_loss: 0.6632 - val_mae: 0.4610 - lr: 1.0000e-03\n",
      "Epoch 9/1000\n",
      "618/636 [============================>.] - ETA: 0s - loss: 0.7052 - mae: 0.5115\n",
      "Epoch 9: val_mae did not improve from 0.37951\n",
      "636/636 [==============================] - 2s 3ms/step - loss: 0.7045 - mae: 0.5118 - val_loss: 0.6329 - val_mae: 0.4652 - lr: 1.0000e-03\n",
      "Epoch 10/1000\n",
      "636/636 [==============================] - ETA: 0s - loss: 0.7542 - mae: 0.5455\n",
      "Epoch 10: val_mae did not improve from 0.37951\n",
      "\n",
      "Epoch 10: ReduceLROnPlateau reducing learning rate to 9.999999310821295e-05.\n",
      "636/636 [==============================] - 2s 3ms/step - loss: 0.7542 - mae: 0.5455 - val_loss: 0.5876 - val_mae: 0.4341 - lr: 1.0000e-03\n",
      "Epoch 11/1000\n",
      "613/636 [===========================>..] - ETA: 0s - loss: 0.5427 - mae: 0.3545\n",
      "Epoch 11: val_mae improved from 0.37951 to 0.35223, saving model to checkpoint_dir\\training_1.ckpt\n",
      "INFO:tensorflow:Assets written to: checkpoint_dir\\training_1.ckpt\\assets\n",
      "636/636 [==============================] - 2s 3ms/step - loss: 0.5439 - mae: 0.3543 - val_loss: 0.5207 - val_mae: 0.3522 - lr: 1.0000e-04\n",
      "Epoch 12/1000\n",
      "630/636 [============================>.] - ETA: 0s - loss: 0.5420 - mae: 0.3566\n",
      "Epoch 12: val_mae did not improve from 0.35223\n",
      "636/636 [==============================] - 2s 3ms/step - loss: 0.5422 - mae: 0.3566 - val_loss: 0.5275 - val_mae: 0.3618 - lr: 1.0000e-04\n",
      "Epoch 13/1000\n",
      "617/636 [============================>.] - ETA: 0s - loss: 0.5540 - mae: 0.3681\n",
      "Epoch 13: val_mae improved from 0.35223 to 0.34749, saving model to checkpoint_dir\\training_1.ckpt\n",
      "INFO:tensorflow:Assets written to: checkpoint_dir\\training_1.ckpt\\assets\n",
      "636/636 [==============================] - 2s 4ms/step - loss: 0.5549 - mae: 0.3680 - val_loss: 0.5195 - val_mae: 0.3475 - lr: 1.0000e-04\n",
      "Epoch 14/1000\n",
      "619/636 [============================>.] - ETA: 0s - loss: 0.5510 - mae: 0.3694\n",
      "Epoch 14: val_mae did not improve from 0.34749\n",
      "636/636 [==============================] - 2s 3ms/step - loss: 0.5519 - mae: 0.3696 - val_loss: 0.5370 - val_mae: 0.3546 - lr: 1.0000e-04\n",
      "Epoch 15/1000\n",
      "628/636 [============================>.] - ETA: 0s - loss: 0.5552 - mae: 0.3749\n",
      "Epoch 15: val_mae did not improve from 0.34749\n",
      "636/636 [==============================] - 2s 3ms/step - loss: 0.5565 - mae: 0.3761 - val_loss: 0.5412 - val_mae: 0.3487 - lr: 1.0000e-04\n",
      "Epoch 16/1000\n",
      "632/636 [============================>.] - ETA: 0s - loss: 0.5558 - mae: 0.3735\n",
      "Epoch 16: val_mae improved from 0.34749 to 0.33960, saving model to checkpoint_dir\\training_1.ckpt\n",
      "INFO:tensorflow:Assets written to: checkpoint_dir\\training_1.ckpt\\assets\n",
      "636/636 [==============================] - 2s 4ms/step - loss: 0.5582 - mae: 0.3739 - val_loss: 0.5419 - val_mae: 0.3396 - lr: 1.0000e-04\n",
      "Epoch 17/1000\n",
      "620/636 [============================>.] - ETA: 0s - loss: 0.5572 - mae: 0.3746\n",
      "Epoch 17: val_mae did not improve from 0.33960\n",
      "636/636 [==============================] - 2s 3ms/step - loss: 0.5617 - mae: 0.3760 - val_loss: 0.5662 - val_mae: 0.4416 - lr: 1.0000e-04\n",
      "Epoch 18/1000\n",
      "629/636 [============================>.] - ETA: 0s - loss: 0.5605 - mae: 0.3814\n",
      "Epoch 18: val_mae did not improve from 0.33960\n",
      "636/636 [==============================] - 2s 3ms/step - loss: 0.5609 - mae: 0.3817 - val_loss: 0.5550 - val_mae: 0.3557 - lr: 1.0000e-04\n",
      "Epoch 19/1000\n",
      "624/636 [============================>.] - ETA: 0s - loss: 0.5529 - mae: 0.3692\n",
      "Epoch 19: val_mae improved from 0.33960 to 0.33925, saving model to checkpoint_dir\\training_1.ckpt\n",
      "INFO:tensorflow:Assets written to: checkpoint_dir\\training_1.ckpt\\assets\n",
      "636/636 [==============================] - 2s 3ms/step - loss: 0.5537 - mae: 0.3698 - val_loss: 0.5251 - val_mae: 0.3393 - lr: 1.0000e-04\n",
      "Epoch 20/1000\n",
      "620/636 [============================>.] - ETA: 0s - loss: 0.5571 - mae: 0.3756\n",
      "Epoch 20: val_mae did not improve from 0.33925\n",
      "636/636 [==============================] - 2s 2ms/step - loss: 0.5559 - mae: 0.3754 - val_loss: 0.5336 - val_mae: 0.3452 - lr: 1.0000e-04\n",
      "Epoch 21/1000\n",
      "618/636 [============================>.] - ETA: 0s - loss: 0.5558 - mae: 0.3769\n",
      "Epoch 21: val_mae did not improve from 0.33925\n",
      "636/636 [==============================] - 2s 2ms/step - loss: 0.5583 - mae: 0.3779 - val_loss: 0.5716 - val_mae: 0.4549 - lr: 1.0000e-04\n",
      "Epoch 22/1000\n",
      "633/636 [============================>.] - ETA: 0s - loss: 0.5530 - mae: 0.3699\n",
      "Epoch 22: val_mae did not improve from 0.33925\n",
      "\n",
      "Epoch 22: ReduceLROnPlateau reducing learning rate to 1e-05.\n",
      "636/636 [==============================] - 2s 3ms/step - loss: 0.5536 - mae: 0.3702 - val_loss: 0.6862 - val_mae: 0.5493 - lr: 1.0000e-04\n",
      "Epoch 23/1000\n",
      "628/636 [============================>.] - ETA: 0s - loss: 0.5243 - mae: 0.3320\n",
      "Epoch 23: val_mae improved from 0.33925 to 0.32069, saving model to checkpoint_dir\\training_1.ckpt\n",
      "INFO:tensorflow:Assets written to: checkpoint_dir\\training_1.ckpt\\assets\n",
      "636/636 [==============================] - 2s 3ms/step - loss: 0.5250 - mae: 0.3320 - val_loss: 0.5064 - val_mae: 0.3207 - lr: 1.0000e-05\n",
      "Epoch 24/1000\n",
      "625/636 [============================>.] - ETA: 0s - loss: 0.5198 - mae: 0.3272\n",
      "Epoch 24: val_mae did not improve from 0.32069\n",
      "636/636 [==============================] - 2s 2ms/step - loss: 0.5204 - mae: 0.3271 - val_loss: 0.5097 - val_mae: 0.3321 - lr: 1.0000e-05\n",
      "Epoch 25/1000\n",
      "636/636 [==============================] - ETA: 0s - loss: 0.5213 - mae: 0.3282\n",
      "Epoch 25: val_mae improved from 0.32069 to 0.31222, saving model to checkpoint_dir\\training_1.ckpt\n",
      "INFO:tensorflow:Assets written to: checkpoint_dir\\training_1.ckpt\\assets\n",
      "636/636 [==============================] - 2s 3ms/step - loss: 0.5213 - mae: 0.3282 - val_loss: 0.5061 - val_mae: 0.3122 - lr: 1.0000e-05\n",
      "Epoch 26/1000\n",
      "620/636 [============================>.] - ETA: 0s - loss: 0.5263 - mae: 0.3303\n",
      "Epoch 26: val_mae improved from 0.31222 to 0.31101, saving model to checkpoint_dir\\training_1.ckpt\n",
      "INFO:tensorflow:Assets written to: checkpoint_dir\\training_1.ckpt\\assets\n",
      "636/636 [==============================] - 2s 3ms/step - loss: 0.5212 - mae: 0.3290 - val_loss: 0.5072 - val_mae: 0.3110 - lr: 1.0000e-05\n",
      "Epoch 27/1000\n",
      "628/636 [============================>.] - ETA: 0s - loss: 0.5225 - mae: 0.3285\n",
      "Epoch 27: val_mae did not improve from 0.31101\n",
      "636/636 [==============================] - 2s 3ms/step - loss: 0.5209 - mae: 0.3283 - val_loss: 0.5074 - val_mae: 0.3149 - lr: 1.0000e-05\n",
      "Epoch 28/1000\n",
      "630/636 [============================>.] - ETA: 0s - loss: 0.5210 - mae: 0.3274\n",
      "Epoch 28: val_mae did not improve from 0.31101\n",
      "636/636 [==============================] - 2s 3ms/step - loss: 0.5208 - mae: 0.3272 - val_loss: 0.5080 - val_mae: 0.3154 - lr: 1.0000e-05\n",
      "Epoch 29/1000\n",
      "629/636 [============================>.] - ETA: 0s - loss: 0.5239 - mae: 0.3292\n",
      "Epoch 29: val_mae did not improve from 0.31101\n",
      "636/636 [==============================] - 2s 2ms/step - loss: 0.5213 - mae: 0.3284 - val_loss: 0.5079 - val_mae: 0.3163 - lr: 1.0000e-05\n",
      "Epoch 30/1000\n",
      "623/636 [============================>.] - ETA: 0s - loss: 0.5221 - mae: 0.3284\n",
      "Epoch 30: val_mae did not improve from 0.31101\n",
      "636/636 [==============================] - 2s 3ms/step - loss: 0.5214 - mae: 0.3282 - val_loss: 0.5085 - val_mae: 0.3308 - lr: 1.0000e-05\n",
      "Epoch 31/1000\n",
      "627/636 [============================>.] - ETA: 0s - loss: 0.5256 - mae: 0.3296\n",
      "Epoch 31: val_mae improved from 0.31101 to 0.31069, saving model to checkpoint_dir\\training_1.ckpt\n",
      "INFO:tensorflow:Assets written to: checkpoint_dir\\training_1.ckpt\\assets\n",
      "636/636 [==============================] - 2s 3ms/step - loss: 0.5213 - mae: 0.3282 - val_loss: 0.5060 - val_mae: 0.3107 - lr: 1.0000e-05\n",
      "Epoch 32/1000\n",
      "632/636 [============================>.] - ETA: 0s - loss: 0.5211 - mae: 0.3277\n",
      "Epoch 32: val_mae did not improve from 0.31069\n",
      "636/636 [==============================] - 2s 3ms/step - loss: 0.5206 - mae: 0.3277 - val_loss: 0.5137 - val_mae: 0.3419 - lr: 1.0000e-05\n",
      "Epoch 33/1000\n",
      "622/636 [============================>.] - ETA: 0s - loss: 0.5199 - mae: 0.3285\n",
      "Epoch 33: val_mae did not improve from 0.31069\n",
      "636/636 [==============================] - 2s 3ms/step - loss: 0.5213 - mae: 0.3290 - val_loss: 0.5142 - val_mae: 0.3484 - lr: 1.0000e-05\n",
      "Epoch 34/1000\n",
      "630/636 [============================>.] - ETA: 0s - loss: 0.5218 - mae: 0.3292\n",
      "Epoch 34: val_mae did not improve from 0.31069\n",
      "636/636 [==============================] - 2s 2ms/step - loss: 0.5206 - mae: 0.3290 - val_loss: 0.5103 - val_mae: 0.3124 - lr: 1.0000e-05\n",
      "Epoch 35/1000\n",
      "617/636 [============================>.] - ETA: 0s - loss: 0.5198 - mae: 0.3279\n",
      "Epoch 35: val_mae did not improve from 0.31069\n",
      "636/636 [==============================] - 2s 3ms/step - loss: 0.5212 - mae: 0.3284 - val_loss: 0.5055 - val_mae: 0.3201 - lr: 1.0000e-05\n",
      "Epoch 36/1000\n",
      "615/636 [============================>.] - ETA: 0s - loss: 0.5240 - mae: 0.3278\n",
      "Epoch 36: val_mae did not improve from 0.31069\n",
      "636/636 [==============================] - 2s 3ms/step - loss: 0.5202 - mae: 0.3268 - val_loss: 0.5143 - val_mae: 0.3196 - lr: 1.0000e-05\n",
      "Epoch 37/1000\n",
      "635/636 [============================>.] - ETA: 0s - loss: 0.5197 - mae: 0.3275\n",
      "Epoch 37: val_mae did not improve from 0.31069\n",
      "636/636 [==============================] - 2s 2ms/step - loss: 0.5197 - mae: 0.3275 - val_loss: 0.5059 - val_mae: 0.3183 - lr: 1.0000e-05\n",
      "Epoch 38/1000\n",
      "626/636 [============================>.] - ETA: 0s - loss: 0.5194 - mae: 0.3281\n",
      "Epoch 38: val_mae did not improve from 0.31069\n",
      "636/636 [==============================] - 2s 2ms/step - loss: 0.5209 - mae: 0.3287 - val_loss: 0.5144 - val_mae: 0.3480 - lr: 1.0000e-05\n",
      "Epoch 39/1000\n",
      "631/636 [============================>.] - ETA: 0s - loss: 0.5181 - mae: 0.3273\n",
      "Epoch 39: val_mae did not improve from 0.31069\n",
      "636/636 [==============================] - 2s 3ms/step - loss: 0.5192 - mae: 0.3275 - val_loss: 0.5076 - val_mae: 0.3207 - lr: 1.0000e-05\n",
      "Epoch 40/1000\n",
      "615/636 [============================>.] - ETA: 0s - loss: 0.5205 - mae: 0.3288\n",
      "Epoch 40: val_mae did not improve from 0.31069\n",
      "636/636 [==============================] - 2s 3ms/step - loss: 0.5203 - mae: 0.3284 - val_loss: 0.5117 - val_mae: 0.3398 - lr: 1.0000e-05\n",
      "Epoch 41/1000\n",
      "623/636 [============================>.] - ETA: 0s - loss: 0.5178 - mae: 0.3302\n",
      "Epoch 41: val_mae did not improve from 0.31069\n",
      "636/636 [==============================] - 2s 3ms/step - loss: 0.5214 - mae: 0.3312 - val_loss: 0.5074 - val_mae: 0.3172 - lr: 1.0000e-05\n",
      "Epoch 42/1000\n",
      "626/636 [============================>.] - ETA: 0s - loss: 0.5197 - mae: 0.3278\n",
      "Epoch 42: val_mae did not improve from 0.31069\n",
      "636/636 [==============================] - 2s 3ms/step - loss: 0.5200 - mae: 0.3281 - val_loss: 0.5173 - val_mae: 0.3472 - lr: 1.0000e-05\n",
      "Epoch 43/1000\n",
      "635/636 [============================>.] - ETA: 0s - loss: 0.5202 - mae: 0.3296\n",
      "Epoch 43: val_mae did not improve from 0.31069\n",
      "636/636 [==============================] - 2s 3ms/step - loss: 0.5201 - mae: 0.3295 - val_loss: 0.5047 - val_mae: 0.3146 - lr: 1.0000e-05\n",
      "Epoch 44/1000\n",
      "616/636 [============================>.] - ETA: 0s - loss: 0.5224 - mae: 0.3290\n",
      "Epoch 44: val_mae did not improve from 0.31069\n",
      "636/636 [==============================] - 2s 2ms/step - loss: 0.5202 - mae: 0.3282 - val_loss: 0.5143 - val_mae: 0.3429 - lr: 1.0000e-05\n",
      "Epoch 45/1000\n",
      "615/636 [============================>.] - ETA: 0s - loss: 0.5241 - mae: 0.3323\n",
      "Epoch 45: val_mae did not improve from 0.31069\n",
      "636/636 [==============================] - 2s 3ms/step - loss: 0.5207 - mae: 0.3315 - val_loss: 0.5054 - val_mae: 0.3208 - lr: 1.0000e-05\n",
      "Epoch 46/1000\n",
      "619/636 [============================>.] - ETA: 0s - loss: 0.5156 - mae: 0.3273\n",
      "Epoch 46: val_mae did not improve from 0.31069\n",
      "636/636 [==============================] - 2s 3ms/step - loss: 0.5201 - mae: 0.3290 - val_loss: 0.5182 - val_mae: 0.3561 - lr: 1.0000e-05\n",
      "Epoch 47/1000\n",
      "625/636 [============================>.] - ETA: 0s - loss: 0.5198 - mae: 0.3286\n",
      "Epoch 47: val_mae did not improve from 0.31069\n",
      "636/636 [==============================] - 2s 3ms/step - loss: 0.5196 - mae: 0.3285 - val_loss: 0.5093 - val_mae: 0.3339 - lr: 1.0000e-05\n",
      "Epoch 48/1000\n",
      "614/636 [===========================>..] - ETA: 0s - loss: 0.5236 - mae: 0.3291\n",
      "Epoch 48: val_mae did not improve from 0.31069\n",
      "636/636 [==============================] - 2s 3ms/step - loss: 0.5202 - mae: 0.3278 - val_loss: 0.5092 - val_mae: 0.3358 - lr: 1.0000e-05\n",
      "Epoch 49/1000\n",
      "624/636 [============================>.] - ETA: 0s - loss: 0.5219 - mae: 0.3292\n",
      "Epoch 49: val_mae did not improve from 0.31069\n",
      "636/636 [==============================] - 2s 3ms/step - loss: 0.5204 - mae: 0.3287 - val_loss: 0.5041 - val_mae: 0.3156 - lr: 1.0000e-05\n",
      "Epoch 50/1000\n",
      "619/636 [============================>.] - ETA: 0s - loss: 0.5218 - mae: 0.3299\n",
      "Epoch 50: val_mae did not improve from 0.31069\n",
      "636/636 [==============================] - 2s 3ms/step - loss: 0.5203 - mae: 0.3289 - val_loss: 0.5068 - val_mae: 0.3202 - lr: 1.0000e-05\n",
      "Epoch 51/1000\n",
      "621/636 [============================>.] - ETA: 0s - loss: 0.5178 - mae: 0.3279\n",
      "Epoch 51: val_mae did not improve from 0.31069\n",
      "636/636 [==============================] - 2s 2ms/step - loss: 0.5200 - mae: 0.3280 - val_loss: 0.5103 - val_mae: 0.3370 - lr: 1.0000e-05\n",
      "Epoch 52/1000\n",
      "623/636 [============================>.] - ETA: 0s - loss: 0.5155 - mae: 0.3275\n",
      "Epoch 52: val_mae did not improve from 0.31069\n",
      "636/636 [==============================] - 2s 3ms/step - loss: 0.5190 - mae: 0.3279 - val_loss: 0.5098 - val_mae: 0.3368 - lr: 1.0000e-05\n",
      "Epoch 53/1000\n",
      "635/636 [============================>.] - ETA: 0s - loss: 0.5207 - mae: 0.3298\n",
      "Epoch 53: val_mae did not improve from 0.31069\n",
      "636/636 [==============================] - 2s 3ms/step - loss: 0.5203 - mae: 0.3297 - val_loss: 0.5114 - val_mae: 0.3183 - lr: 1.0000e-05\n",
      "Epoch 54/1000\n",
      "614/636 [===========================>..] - ETA: 0s - loss: 0.5217 - mae: 0.3276\n",
      "Epoch 54: val_mae did not improve from 0.31069\n",
      "636/636 [==============================] - 2s 3ms/step - loss: 0.5194 - mae: 0.3267 - val_loss: 0.5110 - val_mae: 0.3371 - lr: 1.0000e-05\n",
      "Epoch 55/1000\n",
      "632/636 [============================>.] - ETA: 0s - loss: 0.5191 - mae: 0.3293\n",
      "Epoch 55: val_mae did not improve from 0.31069\n",
      "636/636 [==============================] - 2s 2ms/step - loss: 0.5201 - mae: 0.3295 - val_loss: 0.5161 - val_mae: 0.3247 - lr: 1.0000e-05\n",
      "Epoch 56/1000\n",
      "619/636 [============================>.] - ETA: 0s - loss: 0.5232 - mae: 0.3292\n",
      "Epoch 56: val_mae did not improve from 0.31069\n",
      "636/636 [==============================] - 2s 3ms/step - loss: 0.5195 - mae: 0.3284 - val_loss: 0.5059 - val_mae: 0.3153 - lr: 1.0000e-05\n",
      "Epoch 57/1000\n",
      "614/636 [===========================>..] - ETA: 0s - loss: 0.5150 - mae: 0.3252\n",
      "Epoch 57: val_mae did not improve from 0.31069\n",
      "636/636 [==============================] - 2s 2ms/step - loss: 0.5189 - mae: 0.3264 - val_loss: 0.5147 - val_mae: 0.3468 - lr: 1.0000e-05\n",
      "Epoch 58/1000\n",
      "618/636 [============================>.] - ETA: 0s - loss: 0.5201 - mae: 0.3288\n",
      "Epoch 58: val_mae did not improve from 0.31069\n",
      "636/636 [==============================] - 2s 3ms/step - loss: 0.5200 - mae: 0.3288 - val_loss: 0.5069 - val_mae: 0.3180 - lr: 1.0000e-05\n",
      "Epoch 59/1000\n",
      "625/636 [============================>.] - ETA: 0s - loss: 0.5210 - mae: 0.3296\n",
      "Epoch 59: val_mae did not improve from 0.31069\n",
      "636/636 [==============================] - 2s 3ms/step - loss: 0.5195 - mae: 0.3291 - val_loss: 0.5047 - val_mae: 0.3141 - lr: 1.0000e-05\n",
      "Epoch 60/1000\n",
      "624/636 [============================>.] - ETA: 0s - loss: 0.5183 - mae: 0.3264\n",
      "Epoch 60: val_mae did not improve from 0.31069\n",
      "636/636 [==============================] - 2s 3ms/step - loss: 0.5188 - mae: 0.3273 - val_loss: 0.5237 - val_mae: 0.3646 - lr: 1.0000e-05\n",
      "Epoch 61/1000\n",
      "622/636 [============================>.] - ETA: 0s - loss: 0.5241 - mae: 0.3298\n",
      "Epoch 61: val_mae did not improve from 0.31069\n",
      "636/636 [==============================] - 2s 3ms/step - loss: 0.5205 - mae: 0.3288 - val_loss: 0.5103 - val_mae: 0.3196 - lr: 1.0000e-05\n",
      "Epoch 62/1000\n",
      "629/636 [============================>.] - ETA: 0s - loss: 0.5180 - mae: 0.3269\n",
      "Epoch 62: val_mae did not improve from 0.31069\n",
      "636/636 [==============================] - 2s 2ms/step - loss: 0.5194 - mae: 0.3269 - val_loss: 0.5053 - val_mae: 0.3211 - lr: 1.0000e-05\n",
      "Epoch 63/1000\n",
      "626/636 [============================>.] - ETA: 0s - loss: 0.5203 - mae: 0.3302\n",
      "Epoch 63: val_mae did not improve from 0.31069\n",
      "636/636 [==============================] - 2s 3ms/step - loss: 0.5191 - mae: 0.3297 - val_loss: 0.5065 - val_mae: 0.3123 - lr: 1.0000e-05\n",
      "Epoch 64/1000\n",
      "632/636 [============================>.] - ETA: 0s - loss: 0.5201 - mae: 0.3269\n",
      "Epoch 64: val_mae did not improve from 0.31069\n",
      "636/636 [==============================] - 2s 2ms/step - loss: 0.5193 - mae: 0.3265 - val_loss: 0.5083 - val_mae: 0.3122 - lr: 1.0000e-05\n",
      "Epoch 65/1000\n",
      "631/636 [============================>.] - ETA: 0s - loss: 0.5176 - mae: 0.3264\n",
      "Epoch 65: val_mae did not improve from 0.31069\n",
      "636/636 [==============================] - 2s 2ms/step - loss: 0.5181 - mae: 0.3265 - val_loss: 0.5048 - val_mae: 0.3200 - lr: 1.0000e-05\n",
      "Epoch 66/1000\n",
      "612/636 [===========================>..] - ETA: 0s - loss: 0.5142 - mae: 0.3266\n",
      "Epoch 66: val_mae did not improve from 0.31069\n",
      "636/636 [==============================] - 2s 3ms/step - loss: 0.5178 - mae: 0.3282 - val_loss: 0.5081 - val_mae: 0.3116 - lr: 1.0000e-05\n",
      "Epoch 67/1000\n",
      "620/636 [============================>.] - ETA: 0s - loss: 0.5222 - mae: 0.3274\n",
      "Epoch 67: val_mae did not improve from 0.31069\n",
      "636/636 [==============================] - 2s 2ms/step - loss: 0.5185 - mae: 0.3265 - val_loss: 0.5052 - val_mae: 0.3182 - lr: 1.0000e-05\n",
      "Epoch 68/1000\n",
      "636/636 [==============================] - ETA: 0s - loss: 0.5185 - mae: 0.3280\n",
      "Epoch 68: val_mae did not improve from 0.31069\n",
      "636/636 [==============================] - 2s 2ms/step - loss: 0.5185 - mae: 0.3280 - val_loss: 0.5086 - val_mae: 0.3312 - lr: 1.0000e-05\n",
      "Epoch 69/1000\n",
      "625/636 [============================>.] - ETA: 0s - loss: 0.5206 - mae: 0.3280\n",
      "Epoch 69: val_mae did not improve from 0.31069\n",
      "636/636 [==============================] - 2s 2ms/step - loss: 0.5185 - mae: 0.3274 - val_loss: 0.5058 - val_mae: 0.3195 - lr: 1.0000e-05\n",
      "Epoch 69: early stopping\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train, validation_split=0.2, epochs=1000, callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading the saved weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(1024, input_dim=4, activation='relu'))\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='mse', optimizer=Adam(0.01), metrics=['mae'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.checkpoint.checkpoint.CheckpointLoadStatus at 0x272869e1580>"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_weights('checkpoint_dir/training_1.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "341/341 [==============================] - 1s 2ms/step - loss: 0.5640 - mae: 0.3285\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Saving the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: saved_model/dnn_unit1\\assets\n"
     ]
    }
   ],
   "source": [
    "model.save('saved_model/dnn_unit1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model = tf.keras.models.load_model('saved_model/dnn_unit1')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_22\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_80 (Dense)            (None, 128)               768       \n",
      "                                                                 \n",
      " dense_81 (Dense)            (None, 256)               33024     \n",
      "                                                                 \n",
      " dense_82 (Dense)            (None, 512)               131584    \n",
      "                                                                 \n",
      " dense_83 (Dense)            (None, 1024)              525312    \n",
      "                                                                 \n",
      " dense_84 (Dense)            (None, 1)                 1025      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 691,713\n",
      "Trainable params: 691,713\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "new_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "341/341 [==============================] - 1s 2ms/step - loss: 0.5603 - mae: 0.3266\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.560325562953949, 0.3265897035598755]"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_model.evaluate(X_test, y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "cd38e5a88334fb4f0a3a7cb4807489c1d2190f8b1b2649dc0fd7c6a431f8e87e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
