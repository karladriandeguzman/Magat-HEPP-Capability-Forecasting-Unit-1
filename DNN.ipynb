{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.losses import sparse_categorical_crossentropy\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop, SGD\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "import keras_tuner as kt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('Unit_1.csv')\n",
    "df = df.drop(['Unnamed: 0'], axis=1)\n",
    "df['Hydraulic_head'] = df.Magat_level - df.Maris_level"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Univariate Regression using DNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(df['Hydraulic_head'])\n",
    "y = df['U1_ActivePower']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.3, random_state=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalizer = tf.keras.layers.Normalization(input_shape=[1,], axis=None)\n",
    "normalizer.adapt(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    normalizer,\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(1)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_19\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " normalization_3 (Normalizat  (None, 1)                3         \n",
      " ion)                                                            \n",
      "                                                                 \n",
      " dense_68 (Dense)            (None, 64)                128       \n",
      "                                                                 \n",
      " dense_69 (Dense)            (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_70 (Dense)            (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4,356\n",
      "Trainable params: 4,353\n",
      "Non-trainable params: 3\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss='mse', optimizer=Adam(0.001), metrics=['mae'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "636/636 [==============================] - 2s 2ms/step - loss: 1355.7616 - mae: 23.1097 - val_loss: 15.2504 - val_mae: 2.5056\n",
      "Epoch 2/20\n",
      "636/636 [==============================] - 1s 2ms/step - loss: 6.2180 - mae: 1.4491 - val_loss: 2.0058 - val_mae: 0.8973\n",
      "Epoch 3/20\n",
      "636/636 [==============================] - 1s 2ms/step - loss: 1.5098 - mae: 0.7639 - val_loss: 1.0553 - val_mae: 0.6667\n",
      "Epoch 4/20\n",
      "636/636 [==============================] - 1s 2ms/step - loss: 1.0393 - mae: 0.6380 - val_loss: 0.9046 - val_mae: 0.5929\n",
      "Epoch 5/20\n",
      "636/636 [==============================] - 1s 2ms/step - loss: 0.9618 - mae: 0.6064 - val_loss: 0.8763 - val_mae: 0.5918\n",
      "Epoch 6/20\n",
      "636/636 [==============================] - 1s 2ms/step - loss: 0.9479 - mae: 0.5979 - val_loss: 0.8666 - val_mae: 0.5874\n",
      "Epoch 7/20\n",
      "636/636 [==============================] - 1s 2ms/step - loss: 0.9454 - mae: 0.5964 - val_loss: 0.8659 - val_mae: 0.5886\n",
      "Epoch 8/20\n",
      "636/636 [==============================] - 1s 2ms/step - loss: 0.9512 - mae: 0.6012 - val_loss: 0.9267 - val_mae: 0.6737\n",
      "Epoch 9/20\n",
      "636/636 [==============================] - 1s 2ms/step - loss: 0.9521 - mae: 0.6028 - val_loss: 0.8616 - val_mae: 0.5752\n",
      "Epoch 10/20\n",
      "636/636 [==============================] - 1s 2ms/step - loss: 0.9664 - mae: 0.6087 - val_loss: 0.8571 - val_mae: 0.5431\n",
      "Epoch 11/20\n",
      "636/636 [==============================] - 1s 2ms/step - loss: 0.9623 - mae: 0.6100 - val_loss: 1.0229 - val_mae: 0.5760\n",
      "Epoch 12/20\n",
      "636/636 [==============================] - 1s 2ms/step - loss: 0.9821 - mae: 0.6218 - val_loss: 0.8587 - val_mae: 0.5382\n",
      "Epoch 13/20\n",
      "636/636 [==============================] - 1s 2ms/step - loss: 0.9955 - mae: 0.6282 - val_loss: 0.8756 - val_mae: 0.6151\n",
      "Epoch 14/20\n",
      "636/636 [==============================] - 1s 2ms/step - loss: 0.9965 - mae: 0.6294 - val_loss: 0.8779 - val_mae: 0.5327\n",
      "Epoch 15/20\n",
      "636/636 [==============================] - 1s 2ms/step - loss: 0.9716 - mae: 0.6151 - val_loss: 0.8520 - val_mae: 0.5660\n",
      "Epoch 16/20\n",
      "636/636 [==============================] - 1s 2ms/step - loss: 0.9850 - mae: 0.6218 - val_loss: 0.8498 - val_mae: 0.5527\n",
      "Epoch 17/20\n",
      "636/636 [==============================] - 1s 2ms/step - loss: 0.9587 - mae: 0.6032 - val_loss: 1.0162 - val_mae: 0.6158\n",
      "Epoch 18/20\n",
      "636/636 [==============================] - 1s 2ms/step - loss: 0.9826 - mae: 0.6218 - val_loss: 0.9140 - val_mae: 0.6412\n",
      "Epoch 19/20\n",
      "636/636 [==============================] - 1s 2ms/step - loss: 0.9738 - mae: 0.6187 - val_loss: 0.9042 - val_mae: 0.5982\n",
      "Epoch 20/20\n",
      "636/636 [==============================] - 1s 2ms/step - loss: 0.9759 - mae: 0.6190 - val_loss: 0.8817 - val_mae: 0.6064\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train, validation_split=0.2, epochs=20 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multivariate Regression using DNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop('U1_ActivePower', axis=1)\n",
    "y = df['U1_ActivePower']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.3, random_state=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.fit_transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(128, input_dim=5, activation='relu'))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_20\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_71 (Dense)            (None, 128)               768       \n",
      "                                                                 \n",
      " dense_72 (Dense)            (None, 64)                8256      \n",
      "                                                                 \n",
      " dense_73 (Dense)            (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_74 (Dense)            (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 13,249\n",
      "Trainable params: 13,249\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss='mse', optimizer=Adam(0.0001), metrics=['mae'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\deguz\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\engine\\data_adapter.py:1699: FutureWarning: The behavior of `series[i:j]` with an integer-dtype index is deprecated. In a future version, this will be treated as *label-based* indexing, consistent with e.g. `series[i]` lookups. To retain the old behavior, use `series.iloc[i:j]`. To get the future behavior, use `series.loc[i:j]`.\n",
      "  return t[start:end]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "159/159 [==============================] - 2s 4ms/step - loss: 6918.5093 - mae: 83.1531 - val_loss: 6616.0747 - val_mae: 81.3136\n",
      "Epoch 2/300\n",
      "159/159 [==============================] - 0s 3ms/step - loss: 5638.5352 - mae: 74.8158 - val_loss: 4196.2388 - val_mae: 64.3678\n",
      "Epoch 3/300\n",
      "159/159 [==============================] - 0s 3ms/step - loss: 2386.6355 - mae: 45.8980 - val_loss: 1002.9837 - val_mae: 28.2223\n",
      "Epoch 4/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 679.6467 - mae: 21.7927 - val_loss: 541.9293 - val_mae: 19.1059\n",
      "Epoch 5/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 510.3015 - mae: 18.6817 - val_loss: 463.8701 - val_mae: 17.8344\n",
      "Epoch 6/300\n",
      "159/159 [==============================] - 0s 3ms/step - loss: 445.9137 - mae: 17.4476 - val_loss: 405.7924 - val_mae: 16.6240\n",
      "Epoch 7/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 391.9080 - mae: 16.2723 - val_loss: 354.1948 - val_mae: 15.4132\n",
      "Epoch 8/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 341.9914 - mae: 15.0771 - val_loss: 306.4230 - val_mae: 14.2320\n",
      "Epoch 9/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 295.4792 - mae: 13.9002 - val_loss: 262.0997 - val_mae: 13.0391\n",
      "Epoch 10/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 251.6747 - mae: 12.7197 - val_loss: 220.6041 - val_mae: 11.8887\n",
      "Epoch 11/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 210.0139 - mae: 11.5405 - val_loss: 181.5464 - val_mae: 10.7435\n",
      "Epoch 12/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 170.6465 - mae: 10.3577 - val_loss: 144.9626 - val_mae: 9.5578\n",
      "Epoch 13/300\n",
      "159/159 [==============================] - 0s 3ms/step - loss: 134.7203 - mae: 9.1681 - val_loss: 112.4513 - val_mae: 8.3973\n",
      "Epoch 14/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 103.1630 - mae: 7.9925 - val_loss: 84.2598 - val_mae: 7.2296\n",
      "Epoch 15/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 76.3718 - mae: 6.8348 - val_loss: 61.3490 - val_mae: 6.1415\n",
      "Epoch 16/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 55.2323 - mae: 5.7819 - val_loss: 43.6971 - val_mae: 5.1544\n",
      "Epoch 17/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 38.9119 - mae: 4.8228 - val_loss: 30.2991 - val_mae: 4.2744\n",
      "Epoch 18/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 26.7977 - mae: 3.9728 - val_loss: 20.6267 - val_mae: 3.4972\n",
      "Epoch 19/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 18.2769 - mae: 3.2456 - val_loss: 13.9809 - val_mae: 2.8559\n",
      "Epoch 20/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 12.4267 - mae: 2.6293 - val_loss: 9.5473 - val_mae: 2.3063\n",
      "Epoch 21/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 8.5974 - mae: 2.1267 - val_loss: 6.6158 - val_mae: 1.8820\n",
      "Epoch 22/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 6.0405 - mae: 1.7070 - val_loss: 4.6404 - val_mae: 1.4894\n",
      "Epoch 23/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 4.3865 - mae: 1.3765 - val_loss: 3.4737 - val_mae: 1.2389\n",
      "Epoch 24/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 3.4141 - mae: 1.1788 - val_loss: 2.8287 - val_mae: 1.1113\n",
      "Epoch 25/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 2.8343 - mae: 1.0682 - val_loss: 2.3824 - val_mae: 1.0095\n",
      "Epoch 26/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 2.4475 - mae: 0.9890 - val_loss: 2.0869 - val_mae: 0.9446\n",
      "Epoch 27/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 2.1651 - mae: 0.9241 - val_loss: 1.8840 - val_mae: 0.8789\n",
      "Epoch 28/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 1.9439 - mae: 0.8681 - val_loss: 1.6778 - val_mae: 0.8182\n",
      "Epoch 29/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 1.7618 - mae: 0.8180 - val_loss: 1.5301 - val_mae: 0.7682\n",
      "Epoch 30/300\n",
      "159/159 [==============================] - 0s 3ms/step - loss: 1.6080 - mae: 0.7709 - val_loss: 1.4050 - val_mae: 0.7214\n",
      "Epoch 31/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 1.4861 - mae: 0.7358 - val_loss: 1.3020 - val_mae: 0.6967\n",
      "Epoch 32/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 1.3726 - mae: 0.7027 - val_loss: 1.2130 - val_mae: 0.6580\n",
      "Epoch 33/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 1.2856 - mae: 0.6766 - val_loss: 1.1516 - val_mae: 0.6470\n",
      "Epoch 34/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 1.2085 - mae: 0.6558 - val_loss: 1.0914 - val_mae: 0.6269\n",
      "Epoch 35/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 1.1438 - mae: 0.6373 - val_loss: 1.0328 - val_mae: 0.6063\n",
      "Epoch 36/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 1.0860 - mae: 0.6190 - val_loss: 0.9828 - val_mae: 0.5823\n",
      "Epoch 37/300\n",
      "159/159 [==============================] - 0s 3ms/step - loss: 1.0339 - mae: 0.6010 - val_loss: 0.9436 - val_mae: 0.5609\n",
      "Epoch 38/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.9915 - mae: 0.5874 - val_loss: 0.9108 - val_mae: 0.5674\n",
      "Epoch 39/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.9523 - mae: 0.5728 - val_loss: 0.8648 - val_mae: 0.5395\n",
      "Epoch 40/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.9144 - mae: 0.5596 - val_loss: 0.8475 - val_mae: 0.5397\n",
      "Epoch 41/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.8813 - mae: 0.5467 - val_loss: 0.8137 - val_mae: 0.5271\n",
      "Epoch 42/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.8540 - mae: 0.5366 - val_loss: 0.7835 - val_mae: 0.5024\n",
      "Epoch 43/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.8285 - mae: 0.5275 - val_loss: 0.7658 - val_mae: 0.5044\n",
      "Epoch 44/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.8050 - mae: 0.5173 - val_loss: 0.7444 - val_mae: 0.4908\n",
      "Epoch 45/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.7858 - mae: 0.5094 - val_loss: 0.7477 - val_mae: 0.5011\n",
      "Epoch 46/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.7682 - mae: 0.5006 - val_loss: 0.7113 - val_mae: 0.4732\n",
      "Epoch 47/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.7475 - mae: 0.4920 - val_loss: 0.6992 - val_mae: 0.4710\n",
      "Epoch 48/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.7337 - mae: 0.4836 - val_loss: 0.6844 - val_mae: 0.4613\n",
      "Epoch 49/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.7189 - mae: 0.4758 - val_loss: 0.6695 - val_mae: 0.4531\n",
      "Epoch 50/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.7026 - mae: 0.4669 - val_loss: 0.6622 - val_mae: 0.4454\n",
      "Epoch 51/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.6937 - mae: 0.4624 - val_loss: 0.6557 - val_mae: 0.4528\n",
      "Epoch 52/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.6776 - mae: 0.4529 - val_loss: 0.6357 - val_mae: 0.4264\n",
      "Epoch 53/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.6714 - mae: 0.4508 - val_loss: 0.6324 - val_mae: 0.4296\n",
      "Epoch 54/300\n",
      "159/159 [==============================] - 0s 3ms/step - loss: 0.6612 - mae: 0.4432 - val_loss: 0.6271 - val_mae: 0.4313\n",
      "Epoch 55/300\n",
      "159/159 [==============================] - 0s 3ms/step - loss: 0.6518 - mae: 0.4376 - val_loss: 0.6248 - val_mae: 0.4376\n",
      "Epoch 56/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.6415 - mae: 0.4333 - val_loss: 0.6195 - val_mae: 0.4421\n",
      "Epoch 57/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.6398 - mae: 0.4317 - val_loss: 0.5991 - val_mae: 0.4005\n",
      "Epoch 58/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.6305 - mae: 0.4247 - val_loss: 0.6103 - val_mae: 0.4282\n",
      "Epoch 59/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.6265 - mae: 0.4241 - val_loss: 0.5978 - val_mae: 0.4213\n",
      "Epoch 60/300\n",
      "159/159 [==============================] - 0s 3ms/step - loss: 0.6197 - mae: 0.4190 - val_loss: 0.6001 - val_mae: 0.4159\n",
      "Epoch 61/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.6148 - mae: 0.4132 - val_loss: 0.5982 - val_mae: 0.4223\n",
      "Epoch 62/300\n",
      "159/159 [==============================] - 0s 3ms/step - loss: 0.6132 - mae: 0.4155 - val_loss: 0.6029 - val_mae: 0.4294\n",
      "Epoch 63/300\n",
      "159/159 [==============================] - 0s 3ms/step - loss: 0.6041 - mae: 0.4065 - val_loss: 0.5773 - val_mae: 0.3817\n",
      "Epoch 64/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.6020 - mae: 0.4065 - val_loss: 0.5752 - val_mae: 0.3921\n",
      "Epoch 65/300\n",
      "159/159 [==============================] - 0s 3ms/step - loss: 0.5968 - mae: 0.4044 - val_loss: 0.5909 - val_mae: 0.4248\n",
      "Epoch 66/300\n",
      "159/159 [==============================] - 0s 3ms/step - loss: 0.5923 - mae: 0.4011 - val_loss: 0.5686 - val_mae: 0.3953\n",
      "Epoch 67/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5901 - mae: 0.3981 - val_loss: 0.5737 - val_mae: 0.4023\n",
      "Epoch 68/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5884 - mae: 0.3987 - val_loss: 0.5732 - val_mae: 0.3875\n",
      "Epoch 69/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5852 - mae: 0.3938 - val_loss: 0.5767 - val_mae: 0.3964\n",
      "Epoch 70/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5802 - mae: 0.3906 - val_loss: 0.5543 - val_mae: 0.3744\n",
      "Epoch 71/300\n",
      "159/159 [==============================] - 0s 3ms/step - loss: 0.5778 - mae: 0.3868 - val_loss: 0.5564 - val_mae: 0.3653\n",
      "Epoch 72/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5812 - mae: 0.3918 - val_loss: 0.5494 - val_mae: 0.3767\n",
      "Epoch 73/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5785 - mae: 0.3915 - val_loss: 0.5812 - val_mae: 0.4147\n",
      "Epoch 74/300\n",
      "159/159 [==============================] - 0s 3ms/step - loss: 0.5755 - mae: 0.3874 - val_loss: 0.5540 - val_mae: 0.3785\n",
      "Epoch 75/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5728 - mae: 0.3843 - val_loss: 0.5428 - val_mae: 0.3571\n",
      "Epoch 76/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5678 - mae: 0.3791 - val_loss: 0.5512 - val_mae: 0.3707\n",
      "Epoch 77/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5766 - mae: 0.3877 - val_loss: 0.5507 - val_mae: 0.3605\n",
      "Epoch 78/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5679 - mae: 0.3809 - val_loss: 0.5558 - val_mae: 0.3862\n",
      "Epoch 79/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5703 - mae: 0.3809 - val_loss: 0.5384 - val_mae: 0.3580\n",
      "Epoch 80/300\n",
      "159/159 [==============================] - 0s 3ms/step - loss: 0.5633 - mae: 0.3752 - val_loss: 0.5771 - val_mae: 0.4313\n",
      "Epoch 81/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5637 - mae: 0.3733 - val_loss: 0.5728 - val_mae: 0.4243\n",
      "Epoch 82/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5643 - mae: 0.3772 - val_loss: 0.5522 - val_mae: 0.3843\n",
      "Epoch 83/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5558 - mae: 0.3666 - val_loss: 0.5442 - val_mae: 0.3743\n",
      "Epoch 84/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5605 - mae: 0.3723 - val_loss: 0.5398 - val_mae: 0.3502\n",
      "Epoch 85/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5621 - mae: 0.3732 - val_loss: 0.5325 - val_mae: 0.3454\n",
      "Epoch 86/300\n",
      "159/159 [==============================] - 0s 3ms/step - loss: 0.5610 - mae: 0.3743 - val_loss: 0.5598 - val_mae: 0.3774\n",
      "Epoch 87/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5573 - mae: 0.3687 - val_loss: 0.5376 - val_mae: 0.3592\n",
      "Epoch 88/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5605 - mae: 0.3720 - val_loss: 0.5333 - val_mae: 0.3551\n",
      "Epoch 89/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5549 - mae: 0.3673 - val_loss: 0.5674 - val_mae: 0.3886\n",
      "Epoch 90/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5618 - mae: 0.3765 - val_loss: 0.5410 - val_mae: 0.3470\n",
      "Epoch 91/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5549 - mae: 0.3678 - val_loss: 0.5296 - val_mae: 0.3513\n",
      "Epoch 92/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5541 - mae: 0.3644 - val_loss: 0.5577 - val_mae: 0.4105\n",
      "Epoch 93/300\n",
      "159/159 [==============================] - 0s 3ms/step - loss: 0.5561 - mae: 0.3672 - val_loss: 0.5438 - val_mae: 0.3748\n",
      "Epoch 94/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5544 - mae: 0.3657 - val_loss: 0.5670 - val_mae: 0.3875\n",
      "Epoch 95/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5545 - mae: 0.3669 - val_loss: 0.5315 - val_mae: 0.3511\n",
      "Epoch 96/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5515 - mae: 0.3620 - val_loss: 0.5327 - val_mae: 0.3637\n",
      "Epoch 97/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5516 - mae: 0.3638 - val_loss: 0.5354 - val_mae: 0.3431\n",
      "Epoch 98/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5537 - mae: 0.3674 - val_loss: 0.5319 - val_mae: 0.3519\n",
      "Epoch 99/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5513 - mae: 0.3613 - val_loss: 0.5332 - val_mae: 0.3690\n",
      "Epoch 100/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5496 - mae: 0.3646 - val_loss: 0.5490 - val_mae: 0.3570\n",
      "Epoch 101/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5546 - mae: 0.3646 - val_loss: 0.5337 - val_mae: 0.3646\n",
      "Epoch 102/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5515 - mae: 0.3651 - val_loss: 0.5323 - val_mae: 0.3404\n",
      "Epoch 103/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5486 - mae: 0.3593 - val_loss: 0.5300 - val_mae: 0.3611\n",
      "Epoch 104/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5533 - mae: 0.3697 - val_loss: 0.5309 - val_mae: 0.3544\n",
      "Epoch 105/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5488 - mae: 0.3636 - val_loss: 0.5251 - val_mae: 0.3506\n",
      "Epoch 106/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5470 - mae: 0.3556 - val_loss: 0.5324 - val_mae: 0.3404\n",
      "Epoch 107/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5508 - mae: 0.3680 - val_loss: 0.5281 - val_mae: 0.3519\n",
      "Epoch 108/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5489 - mae: 0.3625 - val_loss: 0.5259 - val_mae: 0.3538\n",
      "Epoch 109/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5452 - mae: 0.3562 - val_loss: 0.5356 - val_mae: 0.3737\n",
      "Epoch 110/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5486 - mae: 0.3610 - val_loss: 0.5398 - val_mae: 0.3606\n",
      "Epoch 111/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5452 - mae: 0.3577 - val_loss: 0.5274 - val_mae: 0.3419\n",
      "Epoch 112/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5438 - mae: 0.3565 - val_loss: 0.5579 - val_mae: 0.4110\n",
      "Epoch 113/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5444 - mae: 0.3584 - val_loss: 0.5238 - val_mae: 0.3320\n",
      "Epoch 114/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5471 - mae: 0.3587 - val_loss: 0.5325 - val_mae: 0.3583\n",
      "Epoch 115/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5454 - mae: 0.3588 - val_loss: 0.5341 - val_mae: 0.3621\n",
      "Epoch 116/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5504 - mae: 0.3678 - val_loss: 0.5262 - val_mae: 0.3359\n",
      "Epoch 117/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5413 - mae: 0.3545 - val_loss: 0.5369 - val_mae: 0.3589\n",
      "Epoch 118/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5454 - mae: 0.3594 - val_loss: 0.5454 - val_mae: 0.3662\n",
      "Epoch 119/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5451 - mae: 0.3581 - val_loss: 0.5532 - val_mae: 0.3983\n",
      "Epoch 120/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5473 - mae: 0.3618 - val_loss: 0.5310 - val_mae: 0.3692\n",
      "Epoch 121/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5475 - mae: 0.3655 - val_loss: 0.5289 - val_mae: 0.3605\n",
      "Epoch 122/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5450 - mae: 0.3604 - val_loss: 0.5315 - val_mae: 0.3444\n",
      "Epoch 123/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5426 - mae: 0.3567 - val_loss: 0.5239 - val_mae: 0.3334\n",
      "Epoch 124/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5411 - mae: 0.3526 - val_loss: 0.5216 - val_mae: 0.3425\n",
      "Epoch 125/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5411 - mae: 0.3553 - val_loss: 0.5273 - val_mae: 0.3370\n",
      "Epoch 126/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5441 - mae: 0.3561 - val_loss: 0.5414 - val_mae: 0.3627\n",
      "Epoch 127/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5456 - mae: 0.3626 - val_loss: 0.5376 - val_mae: 0.3576\n",
      "Epoch 128/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5409 - mae: 0.3539 - val_loss: 0.5300 - val_mae: 0.3460\n",
      "Epoch 129/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5417 - mae: 0.3575 - val_loss: 0.5338 - val_mae: 0.3538\n",
      "Epoch 130/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5426 - mae: 0.3586 - val_loss: 0.5363 - val_mae: 0.3659\n",
      "Epoch 131/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5407 - mae: 0.3555 - val_loss: 0.5260 - val_mae: 0.3400\n",
      "Epoch 132/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5440 - mae: 0.3574 - val_loss: 0.5943 - val_mae: 0.4731\n",
      "Epoch 133/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5502 - mae: 0.3688 - val_loss: 0.5287 - val_mae: 0.3420\n",
      "Epoch 134/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5420 - mae: 0.3576 - val_loss: 0.5319 - val_mae: 0.3525\n",
      "Epoch 135/300\n",
      "159/159 [==============================] - 0s 3ms/step - loss: 0.5438 - mae: 0.3587 - val_loss: 0.5355 - val_mae: 0.3462\n",
      "Epoch 136/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5461 - mae: 0.3632 - val_loss: 0.5483 - val_mae: 0.3647\n",
      "Epoch 137/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5401 - mae: 0.3547 - val_loss: 0.5294 - val_mae: 0.3515\n",
      "Epoch 138/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5418 - mae: 0.3574 - val_loss: 0.5431 - val_mae: 0.3770\n",
      "Epoch 139/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5427 - mae: 0.3562 - val_loss: 0.5243 - val_mae: 0.3408\n",
      "Epoch 140/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5389 - mae: 0.3517 - val_loss: 0.5303 - val_mae: 0.3615\n",
      "Epoch 141/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5405 - mae: 0.3542 - val_loss: 0.5269 - val_mae: 0.3393\n",
      "Epoch 142/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5430 - mae: 0.3580 - val_loss: 0.5402 - val_mae: 0.3611\n",
      "Epoch 143/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5425 - mae: 0.3570 - val_loss: 0.5295 - val_mae: 0.3660\n",
      "Epoch 144/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5371 - mae: 0.3515 - val_loss: 0.5218 - val_mae: 0.3287\n",
      "Epoch 145/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5419 - mae: 0.3566 - val_loss: 0.5235 - val_mae: 0.3458\n",
      "Epoch 146/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5415 - mae: 0.3576 - val_loss: 0.5234 - val_mae: 0.3343\n",
      "Epoch 147/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5385 - mae: 0.3554 - val_loss: 0.5288 - val_mae: 0.3619\n",
      "Epoch 148/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5380 - mae: 0.3508 - val_loss: 0.5294 - val_mae: 0.3495\n",
      "Epoch 149/300\n",
      "159/159 [==============================] - 0s 3ms/step - loss: 0.5429 - mae: 0.3620 - val_loss: 0.5284 - val_mae: 0.3572\n",
      "Epoch 150/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5361 - mae: 0.3496 - val_loss: 0.5206 - val_mae: 0.3322\n",
      "Epoch 151/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5381 - mae: 0.3538 - val_loss: 0.5202 - val_mae: 0.3340\n",
      "Epoch 152/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5387 - mae: 0.3540 - val_loss: 0.5550 - val_mae: 0.3771\n",
      "Epoch 153/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5379 - mae: 0.3513 - val_loss: 0.5274 - val_mae: 0.3571\n",
      "Epoch 154/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5426 - mae: 0.3575 - val_loss: 0.5535 - val_mae: 0.4076\n",
      "Epoch 155/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5366 - mae: 0.3514 - val_loss: 0.5355 - val_mae: 0.3528\n",
      "Epoch 156/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5400 - mae: 0.3539 - val_loss: 0.5312 - val_mae: 0.3569\n",
      "Epoch 157/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5409 - mae: 0.3558 - val_loss: 0.5166 - val_mae: 0.3297\n",
      "Epoch 158/300\n",
      "159/159 [==============================] - 0s 3ms/step - loss: 0.5395 - mae: 0.3555 - val_loss: 0.5325 - val_mae: 0.3497\n",
      "Epoch 159/300\n",
      "159/159 [==============================] - 0s 3ms/step - loss: 0.5380 - mae: 0.3543 - val_loss: 0.5297 - val_mae: 0.3499\n",
      "Epoch 160/300\n",
      "159/159 [==============================] - 0s 3ms/step - loss: 0.5395 - mae: 0.3522 - val_loss: 0.5303 - val_mae: 0.3552\n",
      "Epoch 161/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5414 - mae: 0.3599 - val_loss: 0.5237 - val_mae: 0.3331\n",
      "Epoch 162/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5402 - mae: 0.3552 - val_loss: 0.5203 - val_mae: 0.3255\n",
      "Epoch 163/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5382 - mae: 0.3553 - val_loss: 0.5261 - val_mae: 0.3341\n",
      "Epoch 164/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5445 - mae: 0.3633 - val_loss: 0.5270 - val_mae: 0.3288\n",
      "Epoch 165/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5360 - mae: 0.3478 - val_loss: 0.5245 - val_mae: 0.3452\n",
      "Epoch 166/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5411 - mae: 0.3568 - val_loss: 0.5292 - val_mae: 0.3443\n",
      "Epoch 167/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5361 - mae: 0.3511 - val_loss: 0.5373 - val_mae: 0.3739\n",
      "Epoch 168/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5420 - mae: 0.3577 - val_loss: 0.5290 - val_mae: 0.3502\n",
      "Epoch 169/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5374 - mae: 0.3545 - val_loss: 0.5358 - val_mae: 0.3462\n",
      "Epoch 170/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5367 - mae: 0.3502 - val_loss: 0.5215 - val_mae: 0.3262\n",
      "Epoch 171/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5381 - mae: 0.3527 - val_loss: 0.5250 - val_mae: 0.3317\n",
      "Epoch 172/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5389 - mae: 0.3533 - val_loss: 0.5242 - val_mae: 0.3459\n",
      "Epoch 173/300\n",
      "159/159 [==============================] - 0s 3ms/step - loss: 0.5498 - mae: 0.3670 - val_loss: 0.5409 - val_mae: 0.3700\n",
      "Epoch 174/300\n",
      "159/159 [==============================] - 0s 3ms/step - loss: 0.5434 - mae: 0.3594 - val_loss: 0.5173 - val_mae: 0.3260\n",
      "Epoch 175/300\n",
      "159/159 [==============================] - 0s 3ms/step - loss: 0.5330 - mae: 0.3473 - val_loss: 0.5190 - val_mae: 0.3357\n",
      "Epoch 176/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5382 - mae: 0.3554 - val_loss: 0.5269 - val_mae: 0.3593\n",
      "Epoch 177/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5361 - mae: 0.3532 - val_loss: 0.5178 - val_mae: 0.3261\n",
      "Epoch 178/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5412 - mae: 0.3543 - val_loss: 0.5228 - val_mae: 0.3429\n",
      "Epoch 179/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5348 - mae: 0.3468 - val_loss: 0.5380 - val_mae: 0.3453\n",
      "Epoch 180/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5399 - mae: 0.3589 - val_loss: 0.5244 - val_mae: 0.3394\n",
      "Epoch 181/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5402 - mae: 0.3576 - val_loss: 0.5519 - val_mae: 0.3957\n",
      "Epoch 182/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5373 - mae: 0.3556 - val_loss: 0.5880 - val_mae: 0.4116\n",
      "Epoch 183/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5369 - mae: 0.3531 - val_loss: 0.5213 - val_mae: 0.3363\n",
      "Epoch 184/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5390 - mae: 0.3560 - val_loss: 0.5522 - val_mae: 0.3487\n",
      "Epoch 185/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5414 - mae: 0.3608 - val_loss: 0.5431 - val_mae: 0.3510\n",
      "Epoch 186/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5394 - mae: 0.3543 - val_loss: 0.5186 - val_mae: 0.3346\n",
      "Epoch 187/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5340 - mae: 0.3505 - val_loss: 0.5335 - val_mae: 0.3686\n",
      "Epoch 188/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5323 - mae: 0.3471 - val_loss: 0.5212 - val_mae: 0.3345\n",
      "Epoch 189/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5351 - mae: 0.3516 - val_loss: 0.5284 - val_mae: 0.3472\n",
      "Epoch 190/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5360 - mae: 0.3522 - val_loss: 0.5232 - val_mae: 0.3498\n",
      "Epoch 191/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5358 - mae: 0.3499 - val_loss: 0.5287 - val_mae: 0.3359\n",
      "Epoch 192/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5406 - mae: 0.3548 - val_loss: 0.5284 - val_mae: 0.3583\n",
      "Epoch 193/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5376 - mae: 0.3555 - val_loss: 0.5167 - val_mae: 0.3301\n",
      "Epoch 194/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5335 - mae: 0.3525 - val_loss: 0.5239 - val_mae: 0.3399\n",
      "Epoch 195/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5386 - mae: 0.3530 - val_loss: 0.5424 - val_mae: 0.3556\n",
      "Epoch 196/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5373 - mae: 0.3529 - val_loss: 0.5550 - val_mae: 0.3690\n",
      "Epoch 197/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5364 - mae: 0.3530 - val_loss: 0.5186 - val_mae: 0.3332\n",
      "Epoch 198/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5363 - mae: 0.3514 - val_loss: 0.5206 - val_mae: 0.3381\n",
      "Epoch 199/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5358 - mae: 0.3523 - val_loss: 0.5360 - val_mae: 0.3776\n",
      "Epoch 200/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5352 - mae: 0.3513 - val_loss: 0.5209 - val_mae: 0.3444\n",
      "Epoch 201/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5349 - mae: 0.3523 - val_loss: 0.5222 - val_mae: 0.3387\n",
      "Epoch 202/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5384 - mae: 0.3559 - val_loss: 0.5191 - val_mae: 0.3346\n",
      "Epoch 203/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5343 - mae: 0.3493 - val_loss: 0.5204 - val_mae: 0.3298\n",
      "Epoch 204/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5356 - mae: 0.3508 - val_loss: 0.5343 - val_mae: 0.3605\n",
      "Epoch 205/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5310 - mae: 0.3478 - val_loss: 0.5269 - val_mae: 0.3424\n",
      "Epoch 206/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5356 - mae: 0.3502 - val_loss: 0.5242 - val_mae: 0.3472\n",
      "Epoch 207/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5349 - mae: 0.3515 - val_loss: 0.5193 - val_mae: 0.3379\n",
      "Epoch 208/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5360 - mae: 0.3540 - val_loss: 0.5292 - val_mae: 0.3305\n",
      "Epoch 209/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5338 - mae: 0.3485 - val_loss: 0.5261 - val_mae: 0.3427\n",
      "Epoch 210/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5365 - mae: 0.3505 - val_loss: 0.5178 - val_mae: 0.3289\n",
      "Epoch 211/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5351 - mae: 0.3531 - val_loss: 0.5286 - val_mae: 0.3421\n",
      "Epoch 212/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5375 - mae: 0.3532 - val_loss: 0.5502 - val_mae: 0.3989\n",
      "Epoch 213/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5327 - mae: 0.3501 - val_loss: 0.5197 - val_mae: 0.3229\n",
      "Epoch 214/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5479 - mae: 0.3690 - val_loss: 0.5536 - val_mae: 0.3885\n",
      "Epoch 215/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5346 - mae: 0.3511 - val_loss: 0.5494 - val_mae: 0.3995\n",
      "Epoch 216/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5338 - mae: 0.3514 - val_loss: 0.5475 - val_mae: 0.3664\n",
      "Epoch 217/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5386 - mae: 0.3570 - val_loss: 0.5320 - val_mae: 0.3502\n",
      "Epoch 218/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5395 - mae: 0.3565 - val_loss: 0.5154 - val_mae: 0.3252\n",
      "Epoch 219/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5351 - mae: 0.3519 - val_loss: 0.5247 - val_mae: 0.3519\n",
      "Epoch 220/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5334 - mae: 0.3507 - val_loss: 0.5328 - val_mae: 0.3529\n",
      "Epoch 221/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5405 - mae: 0.3598 - val_loss: 0.5292 - val_mae: 0.3421\n",
      "Epoch 222/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5377 - mae: 0.3533 - val_loss: 0.5810 - val_mae: 0.4128\n",
      "Epoch 223/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5372 - mae: 0.3580 - val_loss: 0.5437 - val_mae: 0.3877\n",
      "Epoch 224/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5368 - mae: 0.3548 - val_loss: 0.5507 - val_mae: 0.3831\n",
      "Epoch 225/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5359 - mae: 0.3543 - val_loss: 0.5174 - val_mae: 0.3348\n",
      "Epoch 226/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5340 - mae: 0.3510 - val_loss: 0.5267 - val_mae: 0.3573\n",
      "Epoch 227/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5366 - mae: 0.3541 - val_loss: 0.5173 - val_mae: 0.3261\n",
      "Epoch 228/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5353 - mae: 0.3525 - val_loss: 0.5426 - val_mae: 0.3625\n",
      "Epoch 229/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5340 - mae: 0.3516 - val_loss: 0.5311 - val_mae: 0.3683\n",
      "Epoch 230/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5313 - mae: 0.3473 - val_loss: 0.5475 - val_mae: 0.3536\n",
      "Epoch 231/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5360 - mae: 0.3525 - val_loss: 0.5285 - val_mae: 0.3562\n",
      "Epoch 232/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5327 - mae: 0.3498 - val_loss: 0.5396 - val_mae: 0.3646\n",
      "Epoch 233/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5330 - mae: 0.3480 - val_loss: 0.5318 - val_mae: 0.3529\n",
      "Epoch 234/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5317 - mae: 0.3485 - val_loss: 0.5184 - val_mae: 0.3314\n",
      "Epoch 235/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5310 - mae: 0.3462 - val_loss: 0.5152 - val_mae: 0.3256\n",
      "Epoch 236/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5360 - mae: 0.3507 - val_loss: 0.5159 - val_mae: 0.3243\n",
      "Epoch 237/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5366 - mae: 0.3539 - val_loss: 0.5234 - val_mae: 0.3334\n",
      "Epoch 238/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5343 - mae: 0.3512 - val_loss: 0.5480 - val_mae: 0.3617\n",
      "Epoch 239/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5359 - mae: 0.3553 - val_loss: 0.5197 - val_mae: 0.3404\n",
      "Epoch 240/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5318 - mae: 0.3460 - val_loss: 0.5179 - val_mae: 0.3361\n",
      "Epoch 241/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5340 - mae: 0.3522 - val_loss: 0.5530 - val_mae: 0.3894\n",
      "Epoch 242/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5317 - mae: 0.3483 - val_loss: 0.5245 - val_mae: 0.3483\n",
      "Epoch 243/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5351 - mae: 0.3550 - val_loss: 0.5346 - val_mae: 0.3777\n",
      "Epoch 244/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5364 - mae: 0.3557 - val_loss: 0.5364 - val_mae: 0.3545\n",
      "Epoch 245/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5386 - mae: 0.3582 - val_loss: 0.5291 - val_mae: 0.3270\n",
      "Epoch 246/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5312 - mae: 0.3479 - val_loss: 0.5323 - val_mae: 0.3482\n",
      "Epoch 247/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5328 - mae: 0.3527 - val_loss: 0.5207 - val_mae: 0.3385\n",
      "Epoch 248/300\n",
      "159/159 [==============================] - 0s 3ms/step - loss: 0.5330 - mae: 0.3497 - val_loss: 0.5184 - val_mae: 0.3304\n",
      "Epoch 249/300\n",
      "159/159 [==============================] - 0s 3ms/step - loss: 0.5362 - mae: 0.3549 - val_loss: 0.5481 - val_mae: 0.3786\n",
      "Epoch 250/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5344 - mae: 0.3509 - val_loss: 0.5159 - val_mae: 0.3292\n",
      "Epoch 251/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5298 - mae: 0.3442 - val_loss: 0.5185 - val_mae: 0.3326\n",
      "Epoch 252/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5360 - mae: 0.3518 - val_loss: 0.5206 - val_mae: 0.3257\n",
      "Epoch 253/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5343 - mae: 0.3545 - val_loss: 0.5148 - val_mae: 0.3288\n",
      "Epoch 254/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5342 - mae: 0.3516 - val_loss: 0.5246 - val_mae: 0.3492\n",
      "Epoch 255/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5305 - mae: 0.3465 - val_loss: 0.5256 - val_mae: 0.3361\n",
      "Epoch 256/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5305 - mae: 0.3471 - val_loss: 0.5256 - val_mae: 0.3399\n",
      "Epoch 257/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5298 - mae: 0.3459 - val_loss: 0.5258 - val_mae: 0.3464\n",
      "Epoch 258/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5327 - mae: 0.3466 - val_loss: 0.5236 - val_mae: 0.3372\n",
      "Epoch 259/300\n",
      "159/159 [==============================] - 0s 3ms/step - loss: 0.5391 - mae: 0.3572 - val_loss: 0.5258 - val_mae: 0.3485\n",
      "Epoch 260/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5332 - mae: 0.3511 - val_loss: 0.5448 - val_mae: 0.3722\n",
      "Epoch 261/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5341 - mae: 0.3520 - val_loss: 0.5289 - val_mae: 0.3420\n",
      "Epoch 262/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5311 - mae: 0.3475 - val_loss: 0.5317 - val_mae: 0.3731\n",
      "Epoch 263/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5317 - mae: 0.3477 - val_loss: 0.5252 - val_mae: 0.3489\n",
      "Epoch 264/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5308 - mae: 0.3485 - val_loss: 0.5261 - val_mae: 0.3335\n",
      "Epoch 265/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5294 - mae: 0.3475 - val_loss: 0.5250 - val_mae: 0.3470\n",
      "Epoch 266/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5348 - mae: 0.3562 - val_loss: 0.5386 - val_mae: 0.3600\n",
      "Epoch 267/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5343 - mae: 0.3536 - val_loss: 0.5474 - val_mae: 0.3981\n",
      "Epoch 268/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5338 - mae: 0.3537 - val_loss: 0.5316 - val_mae: 0.3299\n",
      "Epoch 269/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5349 - mae: 0.3532 - val_loss: 0.5209 - val_mae: 0.3359\n",
      "Epoch 270/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5306 - mae: 0.3486 - val_loss: 0.5240 - val_mae: 0.3383\n",
      "Epoch 271/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5386 - mae: 0.3574 - val_loss: 0.5309 - val_mae: 0.3499\n",
      "Epoch 272/300\n",
      "159/159 [==============================] - 0s 3ms/step - loss: 0.5289 - mae: 0.3442 - val_loss: 0.5204 - val_mae: 0.3330\n",
      "Epoch 273/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5291 - mae: 0.3470 - val_loss: 0.5300 - val_mae: 0.3519\n",
      "Epoch 274/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5340 - mae: 0.3498 - val_loss: 0.5174 - val_mae: 0.3270\n",
      "Epoch 275/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5315 - mae: 0.3477 - val_loss: 0.5304 - val_mae: 0.3639\n",
      "Epoch 276/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5362 - mae: 0.3557 - val_loss: 0.5547 - val_mae: 0.3739\n",
      "Epoch 277/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5346 - mae: 0.3550 - val_loss: 0.5157 - val_mae: 0.3294\n",
      "Epoch 278/300\n",
      "159/159 [==============================] - 0s 3ms/step - loss: 0.5303 - mae: 0.3446 - val_loss: 0.5179 - val_mae: 0.3345\n",
      "Epoch 279/300\n",
      "159/159 [==============================] - 0s 3ms/step - loss: 0.5378 - mae: 0.3558 - val_loss: 0.5313 - val_mae: 0.3639\n",
      "Epoch 280/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5235 - mae: 0.3413 - val_loss: 0.5202 - val_mae: 0.3333\n",
      "Epoch 281/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5304 - mae: 0.3478 - val_loss: 0.5214 - val_mae: 0.3298\n",
      "Epoch 282/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5336 - mae: 0.3501 - val_loss: 0.5317 - val_mae: 0.3423\n",
      "Epoch 283/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5306 - mae: 0.3511 - val_loss: 0.5190 - val_mae: 0.3374\n",
      "Epoch 284/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5315 - mae: 0.3475 - val_loss: 0.5244 - val_mae: 0.3356\n",
      "Epoch 285/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5348 - mae: 0.3526 - val_loss: 0.5685 - val_mae: 0.4059\n",
      "Epoch 286/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5363 - mae: 0.3513 - val_loss: 0.5191 - val_mae: 0.3367\n",
      "Epoch 287/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5320 - mae: 0.3488 - val_loss: 0.5168 - val_mae: 0.3293\n",
      "Epoch 288/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5260 - mae: 0.3434 - val_loss: 0.5247 - val_mae: 0.3411\n",
      "Epoch 289/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5249 - mae: 0.3422 - val_loss: 0.5544 - val_mae: 0.3693\n",
      "Epoch 290/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5313 - mae: 0.3498 - val_loss: 0.5518 - val_mae: 0.3718\n",
      "Epoch 291/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5307 - mae: 0.3477 - val_loss: 0.5694 - val_mae: 0.4205\n",
      "Epoch 292/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5302 - mae: 0.3488 - val_loss: 0.5151 - val_mae: 0.3298\n",
      "Epoch 293/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5281 - mae: 0.3436 - val_loss: 0.5181 - val_mae: 0.3245\n",
      "Epoch 294/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5392 - mae: 0.3619 - val_loss: 0.5587 - val_mae: 0.3753\n",
      "Epoch 295/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5289 - mae: 0.3469 - val_loss: 0.5208 - val_mae: 0.3261\n",
      "Epoch 296/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5327 - mae: 0.3507 - val_loss: 0.5287 - val_mae: 0.3519\n",
      "Epoch 297/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5279 - mae: 0.3434 - val_loss: 0.5419 - val_mae: 0.3840\n",
      "Epoch 298/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5412 - mae: 0.3615 - val_loss: 0.5171 - val_mae: 0.3365\n",
      "Epoch 299/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5276 - mae: 0.3461 - val_loss: 0.5393 - val_mae: 0.3698\n",
      "Epoch 300/300\n",
      "159/159 [==============================] - 0s 2ms/step - loss: 0.5323 - mae: 0.3512 - val_loss: 0.5233 - val_mae: 0.3319\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train, validation_split=0.2, epochs=300, batch_size=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparameter tuning using Keras Tuner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_builder(hp):\n",
    "  model = tf.keras.Sequential()\n",
    "  hp_units1 = hp.Int('units1', min_value=32, max_value=512, step=32)\n",
    "  hp_units2 = hp.Int('units2', min_value=32, max_value=512, step=32)\n",
    "  hp_units3 = hp.Int('units2', min_value=32, max_value=512, step=32)\n",
    "  model.add(tf.keras.layers.Dense(units=hp_units1, input_dim=5, activation='relu'))\n",
    "  model.add(tf.keras.layers.Dense(units=hp_units2, activation='relu'))\n",
    "  model.add(tf.keras.layers.Dense(units=hp_units3, activation='relu'))\n",
    "  model.add(tf.keras.layers.Dense(1))\n",
    "  \n",
    "  hp_learning_rate = hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4, 1e-5])\n",
    "\n",
    "  model.compile(optimizer=Adam(learning_rate=hp_learning_rate),\n",
    "                loss='mse',\n",
    "                metrics=['mae'])\n",
    "\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner = kt.Hyperband(model_builder,\n",
    "                     objective='val_mae',\n",
    "                     max_epochs=50,\n",
    "                     directory='my_dir',\n",
    "                     project_name='magat_kt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_early = tf.keras.callbacks.EarlyStopping(monitor='val_mae', patience=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 90 Complete [00h 00m 42s]\n",
      "val_mae: 0.38230156898498535\n",
      "\n",
      "Best val_mae So Far: 0.3229147791862488\n",
      "Total elapsed time: 00h 16m 38s\n",
      "INFO:tensorflow:Oracle triggered exit\n"
     ]
    }
   ],
   "source": [
    "tuner.search(X_train, y_train, epochs=50, validation_split=0.2, callbacks=[stop_early])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "units1 480\n",
      "units2 416\n",
      "learning_rate 0.0001\n"
     ]
    }
   ],
   "source": [
    "for h_param in [f\"units{i}\" for i in range(1,3)] + ['learning_rate']:\n",
    "  print(h_param, tuner.get_best_hyperparameters()[0].get(h_param))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Retraining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(480, input_dim=5, activation='relu'))\n",
    "model.add(Dense(416, activation='relu'))\n",
    "model.add(Dense(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_13\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_46 (Dense)            (None, 480)               2880      \n",
      "                                                                 \n",
      " dense_47 (Dense)            (None, 416)               200096    \n",
      "                                                                 \n",
      " dense_48 (Dense)            (None, 1)                 417       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 203,393\n",
      "Trainable params: 203,393\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss='mse', optimizer=Adam(0.0001), metrics=['mae'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpointer = ModelCheckpoint(filepath='checkpoint_dir/training_0.ckpt', \n",
    "                               monitor='val_mae', mode='min',\n",
    "                               verbose=1, save_best_only=True)\n",
    "early_stopping = EarlyStopping(monitor='val_mae', mode='min', verbose=1, patience=20)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_mae', factor=0.1, patience=5, min_lr=0.00001, verbose=1)\n",
    "callbacks=[early_stopping, checkpointer]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\deguz\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\engine\\data_adapter.py:1699: FutureWarning: The behavior of `series[i:j]` with an integer-dtype index is deprecated. In a future version, this will be treated as *label-based* indexing, consistent with e.g. `series[i]` lookups. To retain the old behavior, use `series.iloc[i:j]`. To get the future behavior, use `series.loc[i:j]`.\n",
      "  return t[start:end]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "628/636 [============================>.] - ETA: 0s - loss: 2780.9497 - mae: 45.0440\n",
      "Epoch 1: val_mae improved from inf to 17.32417, saving model to checkpoint_dir\\training_0.ckpt\n",
      "INFO:tensorflow:Assets written to: checkpoint_dir\\training_0.ckpt\\assets\n",
      "636/636 [==============================] - 2s 3ms/step - loss: 2752.7922 - mae: 44.7218 - val_loss: 440.4384 - val_mae: 17.3242\n",
      "Epoch 2/1000\n",
      "620/636 [============================>.] - ETA: 0s - loss: 315.9128 - mae: 14.3881\n",
      "Epoch 2: val_mae improved from 17.32417 to 11.03116, saving model to checkpoint_dir\\training_0.ckpt\n",
      "INFO:tensorflow:Assets written to: checkpoint_dir\\training_0.ckpt\\assets\n",
      "636/636 [==============================] - 2s 3ms/step - loss: 312.9120 - mae: 14.3086 - val_loss: 190.1340 - val_mae: 11.0312\n",
      "Epoch 3/1000\n",
      "631/636 [============================>.] - ETA: 0s - loss: 122.1625 - mae: 8.5909\n",
      "Epoch 3: val_mae improved from 11.03116 to 6.09463, saving model to checkpoint_dir\\training_0.ckpt\n",
      "INFO:tensorflow:Assets written to: checkpoint_dir\\training_0.ckpt\\assets\n",
      "636/636 [==============================] - 2s 3ms/step - loss: 121.7743 - mae: 8.5744 - val_loss: 60.3634 - val_mae: 6.0946\n",
      "Epoch 4/1000\n",
      "631/636 [============================>.] - ETA: 0s - loss: 35.8151 - mae: 4.5332\n",
      "Epoch 4: val_mae improved from 6.09463 to 3.02872, saving model to checkpoint_dir\\training_0.ckpt\n",
      "INFO:tensorflow:Assets written to: checkpoint_dir\\training_0.ckpt\\assets\n",
      "636/636 [==============================] - 2s 3ms/step - loss: 35.6562 - mae: 4.5198 - val_loss: 15.9087 - val_mae: 3.0287\n",
      "Epoch 5/1000\n",
      "625/636 [============================>.] - ETA: 0s - loss: 10.1363 - mae: 2.3039\n",
      "Epoch 5: val_mae improved from 3.02872 to 1.61792, saving model to checkpoint_dir\\training_0.ckpt\n",
      "INFO:tensorflow:Assets written to: checkpoint_dir\\training_0.ckpt\\assets\n",
      "636/636 [==============================] - 2s 3ms/step - loss: 10.0568 - mae: 2.2929 - val_loss: 4.9703 - val_mae: 1.6179\n",
      "Epoch 6/1000\n",
      "621/636 [============================>.] - ETA: 0s - loss: 3.6872 - mae: 1.3145\n",
      "Epoch 6: val_mae improved from 1.61792 to 1.03225, saving model to checkpoint_dir\\training_0.ckpt\n",
      "INFO:tensorflow:Assets written to: checkpoint_dir\\training_0.ckpt\\assets\n",
      "636/636 [==============================] - 2s 3ms/step - loss: 3.6559 - mae: 1.3086 - val_loss: 2.2323 - val_mae: 1.0322\n",
      "Epoch 7/1000\n",
      "624/636 [============================>.] - ETA: 0s - loss: 1.8710 - mae: 0.8906\n",
      "Epoch 7: val_mae improved from 1.03225 to 0.75920, saving model to checkpoint_dir\\training_0.ckpt\n",
      "INFO:tensorflow:Assets written to: checkpoint_dir\\training_0.ckpt\\assets\n",
      "636/636 [==============================] - 2s 3ms/step - loss: 1.8615 - mae: 0.8884 - val_loss: 1.3113 - val_mae: 0.7592\n",
      "Epoch 8/1000\n",
      "635/636 [============================>.] - ETA: 0s - loss: 1.2001 - mae: 0.6786\n",
      "Epoch 8: val_mae improved from 0.75920 to 0.57904, saving model to checkpoint_dir\\training_0.ckpt\n",
      "INFO:tensorflow:Assets written to: checkpoint_dir\\training_0.ckpt\\assets\n",
      "636/636 [==============================] - 2s 3ms/step - loss: 1.2002 - mae: 0.6785 - val_loss: 0.9060 - val_mae: 0.5790\n",
      "Epoch 9/1000\n",
      "625/636 [============================>.] - ETA: 0s - loss: 0.8943 - mae: 0.5520\n",
      "Epoch 9: val_mae improved from 0.57904 to 0.48325, saving model to checkpoint_dir\\training_0.ckpt\n",
      "INFO:tensorflow:Assets written to: checkpoint_dir\\training_0.ckpt\\assets\n",
      "636/636 [==============================] - 2s 3ms/step - loss: 0.8943 - mae: 0.5519 - val_loss: 0.7223 - val_mae: 0.4832\n",
      "Epoch 10/1000\n",
      "627/636 [============================>.] - ETA: 0s - loss: 0.7481 - mae: 0.4789\n",
      "Epoch 10: val_mae improved from 0.48325 to 0.41683, saving model to checkpoint_dir\\training_0.ckpt\n",
      "INFO:tensorflow:Assets written to: checkpoint_dir\\training_0.ckpt\\assets\n",
      "636/636 [==============================] - 2s 3ms/step - loss: 0.7467 - mae: 0.4787 - val_loss: 0.6379 - val_mae: 0.4168\n",
      "Epoch 11/1000\n",
      "624/636 [============================>.] - ETA: 0s - loss: 0.6659 - mae: 0.4320\n",
      "Epoch 11: val_mae did not improve from 0.41683\n",
      "636/636 [==============================] - 1s 2ms/step - loss: 0.6628 - mae: 0.4316 - val_loss: 0.6323 - val_mae: 0.4283\n",
      "Epoch 12/1000\n",
      "611/636 [===========================>..] - ETA: 0s - loss: 0.6308 - mae: 0.4103\n",
      "Epoch 12: val_mae improved from 0.41683 to 0.39983, saving model to checkpoint_dir\\training_0.ckpt\n",
      "INFO:tensorflow:Assets written to: checkpoint_dir\\training_0.ckpt\\assets\n",
      "636/636 [==============================] - 2s 2ms/step - loss: 0.6269 - mae: 0.4089 - val_loss: 0.5840 - val_mae: 0.3998\n",
      "Epoch 13/1000\n",
      "631/636 [============================>.] - ETA: 0s - loss: 0.6137 - mae: 0.4055\n",
      "Epoch 13: val_mae improved from 0.39983 to 0.38343, saving model to checkpoint_dir\\training_0.ckpt\n",
      "INFO:tensorflow:Assets written to: checkpoint_dir\\training_0.ckpt\\assets\n",
      "636/636 [==============================] - 2s 3ms/step - loss: 0.6144 - mae: 0.4054 - val_loss: 0.5858 - val_mae: 0.3834\n",
      "Epoch 14/1000\n",
      "627/636 [============================>.] - ETA: 0s - loss: 0.6046 - mae: 0.4029\n",
      "Epoch 14: val_mae did not improve from 0.38343\n",
      "636/636 [==============================] - 1s 2ms/step - loss: 0.6066 - mae: 0.4038 - val_loss: 0.5790 - val_mae: 0.3993\n",
      "Epoch 15/1000\n",
      "634/636 [============================>.] - ETA: 0s - loss: 0.5959 - mae: 0.3978\n",
      "Epoch 15: val_mae improved from 0.38343 to 0.38272, saving model to checkpoint_dir\\training_0.ckpt\n",
      "INFO:tensorflow:Assets written to: checkpoint_dir\\training_0.ckpt\\assets\n",
      "636/636 [==============================] - 2s 2ms/step - loss: 0.5976 - mae: 0.3983 - val_loss: 0.5545 - val_mae: 0.3827\n",
      "Epoch 16/1000\n",
      "633/636 [============================>.] - ETA: 0s - loss: 0.5870 - mae: 0.3922\n",
      "Epoch 16: val_mae did not improve from 0.38272\n",
      "636/636 [==============================] - 1s 2ms/step - loss: 0.5888 - mae: 0.3928 - val_loss: 0.5465 - val_mae: 0.3835\n",
      "Epoch 17/1000\n",
      "612/636 [===========================>..] - ETA: 0s - loss: 0.5889 - mae: 0.3938\n",
      "Epoch 17: val_mae improved from 0.38272 to 0.36738, saving model to checkpoint_dir\\training_0.ckpt\n",
      "INFO:tensorflow:Assets written to: checkpoint_dir\\training_0.ckpt\\assets\n",
      "636/636 [==============================] - 2s 2ms/step - loss: 0.5883 - mae: 0.3932 - val_loss: 0.5478 - val_mae: 0.3674\n",
      "Epoch 18/1000\n",
      "609/636 [===========================>..] - ETA: 0s - loss: 0.5848 - mae: 0.3902\n",
      "Epoch 18: val_mae did not improve from 0.36738\n",
      "636/636 [==============================] - 1s 2ms/step - loss: 0.5783 - mae: 0.3896 - val_loss: 0.5879 - val_mae: 0.3709\n",
      "Epoch 19/1000\n",
      "615/636 [============================>.] - ETA: 0s - loss: 0.5820 - mae: 0.3964\n",
      "Epoch 19: val_mae did not improve from 0.36738\n",
      "636/636 [==============================] - 1s 2ms/step - loss: 0.5851 - mae: 0.3969 - val_loss: 0.5607 - val_mae: 0.3913\n",
      "Epoch 20/1000\n",
      "615/636 [============================>.] - ETA: 0s - loss: 0.5762 - mae: 0.3858\n",
      "Epoch 20: val_mae did not improve from 0.36738\n",
      "636/636 [==============================] - 1s 2ms/step - loss: 0.5788 - mae: 0.3860 - val_loss: 0.5663 - val_mae: 0.3992\n",
      "Epoch 21/1000\n",
      "635/636 [============================>.] - ETA: 0s - loss: 0.5797 - mae: 0.3872\n",
      "Epoch 21: val_mae did not improve from 0.36738\n",
      "636/636 [==============================] - 1s 2ms/step - loss: 0.5791 - mae: 0.3871 - val_loss: 0.5537 - val_mae: 0.3909\n",
      "Epoch 22/1000\n",
      "617/636 [============================>.] - ETA: 0s - loss: 0.5792 - mae: 0.3943\n",
      "Epoch 22: val_mae did not improve from 0.36738\n",
      "636/636 [==============================] - 1s 2ms/step - loss: 0.5782 - mae: 0.3929 - val_loss: 0.5853 - val_mae: 0.3804\n",
      "Epoch 23/1000\n",
      "635/636 [============================>.] - ETA: 0s - loss: 0.5707 - mae: 0.3822\n",
      "Epoch 23: val_mae improved from 0.36738 to 0.34465, saving model to checkpoint_dir\\training_0.ckpt\n",
      "INFO:tensorflow:Assets written to: checkpoint_dir\\training_0.ckpt\\assets\n",
      "636/636 [==============================] - 2s 3ms/step - loss: 0.5703 - mae: 0.3820 - val_loss: 0.5208 - val_mae: 0.3447\n",
      "Epoch 24/1000\n",
      "616/636 [============================>.] - ETA: 0s - loss: 0.5692 - mae: 0.3855\n",
      "Epoch 24: val_mae did not improve from 0.34465\n",
      "636/636 [==============================] - 1s 2ms/step - loss: 0.5717 - mae: 0.3864 - val_loss: 0.6266 - val_mae: 0.5003\n",
      "Epoch 25/1000\n",
      "631/636 [============================>.] - ETA: 0s - loss: 0.5744 - mae: 0.3889\n",
      "Epoch 25: val_mae improved from 0.34465 to 0.33177, saving model to checkpoint_dir\\training_0.ckpt\n",
      "INFO:tensorflow:Assets written to: checkpoint_dir\\training_0.ckpt\\assets\n",
      "636/636 [==============================] - 2s 2ms/step - loss: 0.5727 - mae: 0.3879 - val_loss: 0.5209 - val_mae: 0.3318\n",
      "Epoch 26/1000\n",
      "626/636 [============================>.] - ETA: 0s - loss: 0.5709 - mae: 0.3847\n",
      "Epoch 26: val_mae did not improve from 0.33177\n",
      "636/636 [==============================] - 1s 2ms/step - loss: 0.5721 - mae: 0.3842 - val_loss: 0.5176 - val_mae: 0.3321\n",
      "Epoch 27/1000\n",
      "619/636 [============================>.] - ETA: 0s - loss: 0.5671 - mae: 0.3850\n",
      "Epoch 27: val_mae did not improve from 0.33177\n",
      "636/636 [==============================] - 1s 2ms/step - loss: 0.5714 - mae: 0.3852 - val_loss: 0.5844 - val_mae: 0.4485\n",
      "Epoch 28/1000\n",
      "633/636 [============================>.] - ETA: 0s - loss: 0.5742 - mae: 0.3910\n",
      "Epoch 28: val_mae did not improve from 0.33177\n",
      "636/636 [==============================] - 1s 2ms/step - loss: 0.5731 - mae: 0.3907 - val_loss: 0.5517 - val_mae: 0.3779\n",
      "Epoch 29/1000\n",
      "633/636 [============================>.] - ETA: 0s - loss: 0.5707 - mae: 0.3879\n",
      "Epoch 29: val_mae did not improve from 0.33177\n",
      "636/636 [==============================] - 1s 2ms/step - loss: 0.5701 - mae: 0.3879 - val_loss: 0.6195 - val_mae: 0.4246\n",
      "Epoch 30/1000\n",
      "621/636 [============================>.] - ETA: 0s - loss: 0.5745 - mae: 0.3882\n",
      "Epoch 30: val_mae did not improve from 0.33177\n",
      "636/636 [==============================] - 1s 2ms/step - loss: 0.5765 - mae: 0.3892 - val_loss: 0.5541 - val_mae: 0.3803\n",
      "Epoch 31/1000\n",
      "619/636 [============================>.] - ETA: 0s - loss: 0.5747 - mae: 0.3870\n",
      "Epoch 31: val_mae improved from 0.33177 to 0.32359, saving model to checkpoint_dir\\training_0.ckpt\n",
      "INFO:tensorflow:Assets written to: checkpoint_dir\\training_0.ckpt\\assets\n",
      "636/636 [==============================] - 2s 2ms/step - loss: 0.5703 - mae: 0.3850 - val_loss: 0.5223 - val_mae: 0.3236\n",
      "Epoch 32/1000\n",
      "614/636 [===========================>..] - ETA: 0s - loss: 0.5601 - mae: 0.3775\n",
      "Epoch 32: val_mae did not improve from 0.32359\n",
      "636/636 [==============================] - 1s 2ms/step - loss: 0.5642 - mae: 0.3800 - val_loss: 0.5833 - val_mae: 0.4173\n",
      "Epoch 33/1000\n",
      "624/636 [============================>.] - ETA: 0s - loss: 0.5679 - mae: 0.3846\n",
      "Epoch 33: val_mae did not improve from 0.32359\n",
      "636/636 [==============================] - 1s 2ms/step - loss: 0.5689 - mae: 0.3848 - val_loss: 0.5839 - val_mae: 0.4138\n",
      "Epoch 34/1000\n",
      "616/636 [============================>.] - ETA: 0s - loss: 0.5686 - mae: 0.3789\n",
      "Epoch 34: val_mae did not improve from 0.32359\n",
      "636/636 [==============================] - 1s 2ms/step - loss: 0.5654 - mae: 0.3780 - val_loss: 0.5382 - val_mae: 0.3417\n",
      "Epoch 35/1000\n",
      "619/636 [============================>.] - ETA: 0s - loss: 0.5640 - mae: 0.3778\n",
      "Epoch 35: val_mae did not improve from 0.32359\n",
      "636/636 [==============================] - 1s 2ms/step - loss: 0.5644 - mae: 0.3781 - val_loss: 0.5582 - val_mae: 0.4164\n",
      "Epoch 36/1000\n",
      "620/636 [============================>.] - ETA: 0s - loss: 0.5612 - mae: 0.3793\n",
      "Epoch 36: val_mae did not improve from 0.32359\n",
      "636/636 [==============================] - 1s 2ms/step - loss: 0.5626 - mae: 0.3790 - val_loss: 0.5401 - val_mae: 0.3984\n",
      "Epoch 37/1000\n",
      "619/636 [============================>.] - ETA: 0s - loss: 0.5595 - mae: 0.3751\n",
      "Epoch 37: val_mae did not improve from 0.32359\n",
      "636/636 [==============================] - 1s 2ms/step - loss: 0.5595 - mae: 0.3762 - val_loss: 0.5391 - val_mae: 0.3659\n",
      "Epoch 38/1000\n",
      "615/636 [============================>.] - ETA: 0s - loss: 0.5639 - mae: 0.3809\n",
      "Epoch 38: val_mae did not improve from 0.32359\n",
      "636/636 [==============================] - 1s 2ms/step - loss: 0.5645 - mae: 0.3816 - val_loss: 0.5449 - val_mae: 0.3893\n",
      "Epoch 39/1000\n",
      "616/636 [============================>.] - ETA: 0s - loss: 0.5642 - mae: 0.3837\n",
      "Epoch 39: val_mae did not improve from 0.32359\n",
      "636/636 [==============================] - 1s 2ms/step - loss: 0.5674 - mae: 0.3854 - val_loss: 0.5488 - val_mae: 0.3871\n",
      "Epoch 40/1000\n",
      "621/636 [============================>.] - ETA: 0s - loss: 0.5599 - mae: 0.3787\n",
      "Epoch 40: val_mae did not improve from 0.32359\n",
      "636/636 [==============================] - 1s 2ms/step - loss: 0.5625 - mae: 0.3791 - val_loss: 0.6513 - val_mae: 0.5020\n",
      "Epoch 41/1000\n",
      "623/636 [============================>.] - ETA: 0s - loss: 0.5552 - mae: 0.3755\n",
      "Epoch 41: val_mae did not improve from 0.32359\n",
      "636/636 [==============================] - 1s 2ms/step - loss: 0.5546 - mae: 0.3753 - val_loss: 0.5781 - val_mae: 0.4419\n",
      "Epoch 42/1000\n",
      "632/636 [============================>.] - ETA: 0s - loss: 0.5652 - mae: 0.3814\n",
      "Epoch 42: val_mae did not improve from 0.32359\n",
      "636/636 [==============================] - 1s 2ms/step - loss: 0.5655 - mae: 0.3817 - val_loss: 0.5790 - val_mae: 0.4149\n",
      "Epoch 43/1000\n",
      "631/636 [============================>.] - ETA: 0s - loss: 0.5644 - mae: 0.3859\n",
      "Epoch 43: val_mae did not improve from 0.32359\n",
      "636/636 [==============================] - 1s 2ms/step - loss: 0.5640 - mae: 0.3861 - val_loss: 0.5899 - val_mae: 0.4243\n",
      "Epoch 44/1000\n",
      "636/636 [==============================] - ETA: 0s - loss: 0.5515 - mae: 0.3727\n",
      "Epoch 44: val_mae did not improve from 0.32359\n",
      "636/636 [==============================] - 1s 2ms/step - loss: 0.5515 - mae: 0.3727 - val_loss: 0.5569 - val_mae: 0.3538\n",
      "Epoch 45/1000\n",
      "615/636 [============================>.] - ETA: 0s - loss: 0.5600 - mae: 0.3760\n",
      "Epoch 45: val_mae did not improve from 0.32359\n",
      "636/636 [==============================] - 1s 2ms/step - loss: 0.5573 - mae: 0.3751 - val_loss: 0.5293 - val_mae: 0.3238\n",
      "Epoch 46/1000\n",
      "613/636 [===========================>..] - ETA: 0s - loss: 0.5680 - mae: 0.3865\n",
      "Epoch 46: val_mae did not improve from 0.32359\n",
      "636/636 [==============================] - 1s 2ms/step - loss: 0.5666 - mae: 0.3860 - val_loss: 0.5183 - val_mae: 0.3284\n",
      "Epoch 47/1000\n",
      "616/636 [============================>.] - ETA: 0s - loss: 0.5705 - mae: 0.3832\n",
      "Epoch 47: val_mae did not improve from 0.32359\n",
      "636/636 [==============================] - 1s 2ms/step - loss: 0.5646 - mae: 0.3812 - val_loss: 0.5272 - val_mae: 0.3361\n",
      "Epoch 48/1000\n",
      "633/636 [============================>.] - ETA: 0s - loss: 0.5614 - mae: 0.3752\n",
      "Epoch 48: val_mae did not improve from 0.32359\n",
      "636/636 [==============================] - 1s 2ms/step - loss: 0.5609 - mae: 0.3751 - val_loss: 0.5165 - val_mae: 0.3311\n",
      "Epoch 49/1000\n",
      "632/636 [============================>.] - ETA: 0s - loss: 0.5557 - mae: 0.3786\n",
      "Epoch 49: val_mae did not improve from 0.32359\n",
      "636/636 [==============================] - 1s 2ms/step - loss: 0.5559 - mae: 0.3785 - val_loss: 0.5651 - val_mae: 0.3593\n",
      "Epoch 50/1000\n",
      "635/636 [============================>.] - ETA: 0s - loss: 0.5518 - mae: 0.3680\n",
      "Epoch 50: val_mae did not improve from 0.32359\n",
      "636/636 [==============================] - 1s 2ms/step - loss: 0.5524 - mae: 0.3682 - val_loss: 0.5519 - val_mae: 0.3602\n",
      "Epoch 51/1000\n",
      "611/636 [===========================>..] - ETA: 0s - loss: 0.5570 - mae: 0.3729\n",
      "Epoch 51: val_mae did not improve from 0.32359\n",
      "636/636 [==============================] - 1s 2ms/step - loss: 0.5572 - mae: 0.3734 - val_loss: 0.5248 - val_mae: 0.3482\n",
      "Epoch 51: early stopping\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train, validation_split=0.2, epochs=1000, callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Going Deeper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(128, input_dim=5, activation='relu'))\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dense(1024, activation='relu'))\n",
    "model.add(Dense(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_15\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_54 (Dense)            (None, 128)               768       \n",
      "                                                                 \n",
      " dense_55 (Dense)            (None, 256)               33024     \n",
      "                                                                 \n",
      " dense_56 (Dense)            (None, 512)               131584    \n",
      "                                                                 \n",
      " dense_57 (Dense)            (None, 1024)              525312    \n",
      "                                                                 \n",
      " dense_58 (Dense)            (None, 1)                 1025      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 691,713\n",
      "Trainable params: 691,713\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss='mse', optimizer=Adam(0.01), metrics=['mae'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpointer = ModelCheckpoint(filepath='checkpoint_dir/training_1.ckpt', \n",
    "                               monitor='val_mae', mode='min',\n",
    "                               verbose=1, save_best_only=True)\n",
    "early_stopping = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=20)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_mae', factor=0.1, patience=3, min_lr=0.00001, verbose=1)\n",
    "callbacks=[early_stopping, checkpointer, reduce_lr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\deguz\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\engine\\data_adapter.py:1699: FutureWarning: The behavior of `series[i:j]` with an integer-dtype index is deprecated. In a future version, this will be treated as *label-based* indexing, consistent with e.g. `series[i]` lookups. To retain the old behavior, use `series.iloc[i:j]`. To get the future behavior, use `series.loc[i:j]`.\n",
      "  return t[start:end]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "631/636 [============================>.] - ETA: 0s - loss: 50.8453 - mae: 2.8147\n",
      "Epoch 1: val_mae improved from inf to 0.76234, saving model to checkpoint_dir\\training_1.ckpt\n",
      "INFO:tensorflow:Assets written to: checkpoint_dir\\training_1.ckpt\\assets\n",
      "636/636 [==============================] - 2s 3ms/step - loss: 50.4792 - mae: 2.8042 - val_loss: 1.2837 - val_mae: 0.7623 - lr: 0.0100\n",
      "Epoch 2/1000\n",
      "622/636 [============================>.] - ETA: 0s - loss: 3.3530 - mae: 1.2332\n",
      "Epoch 2: val_mae did not improve from 0.76234\n",
      "636/636 [==============================] - 2s 2ms/step - loss: 3.2974 - mae: 1.2195 - val_loss: 1.7336 - val_mae: 1.2003 - lr: 0.0100\n",
      "Epoch 3/1000\n",
      "623/636 [============================>.] - ETA: 0s - loss: 1.8002 - mae: 0.9795\n",
      "Epoch 3: val_mae did not improve from 0.76234\n",
      "636/636 [==============================] - 1s 2ms/step - loss: 1.8028 - mae: 0.9808 - val_loss: 1.2577 - val_mae: 0.9462 - lr: 0.0100\n",
      "Epoch 4/1000\n",
      "635/636 [============================>.] - ETA: 0s - loss: 1.1241 - mae: 0.7572\n",
      "Epoch 4: val_mae improved from 0.76234 to 0.68814, saving model to checkpoint_dir\\training_1.ckpt\n",
      "INFO:tensorflow:Assets written to: checkpoint_dir\\training_1.ckpt\\assets\n",
      "636/636 [==============================] - 2s 3ms/step - loss: 1.1276 - mae: 0.7586 - val_loss: 0.8420 - val_mae: 0.6881 - lr: 0.0100\n",
      "Epoch 5/1000\n",
      "629/636 [============================>.] - ETA: 0s - loss: 1.2281 - mae: 0.8082\n",
      "Epoch 5: val_mae improved from 0.68814 to 0.55064, saving model to checkpoint_dir\\training_1.ckpt\n",
      "INFO:tensorflow:Assets written to: checkpoint_dir\\training_1.ckpt\\assets\n",
      "636/636 [==============================] - 2s 3ms/step - loss: 1.2201 - mae: 0.8040 - val_loss: 0.7648 - val_mae: 0.5506 - lr: 0.0100\n",
      "Epoch 6/1000\n",
      "612/636 [===========================>..] - ETA: 0s - loss: 1.2369 - mae: 0.8037\n",
      "Epoch 6: val_mae improved from 0.55064 to 0.36426, saving model to checkpoint_dir\\training_1.ckpt\n",
      "INFO:tensorflow:Assets written to: checkpoint_dir\\training_1.ckpt\\assets\n",
      "636/636 [==============================] - 2s 3ms/step - loss: 1.2222 - mae: 0.7978 - val_loss: 0.5363 - val_mae: 0.3643 - lr: 0.0100\n",
      "Epoch 7/1000\n",
      "636/636 [==============================] - ETA: 0s - loss: 1.2341 - mae: 0.7951\n",
      "Epoch 7: val_mae did not improve from 0.36426\n",
      "636/636 [==============================] - 2s 2ms/step - loss: 1.2341 - mae: 0.7951 - val_loss: 1.2474 - val_mae: 1.0013 - lr: 0.0100\n",
      "Epoch 8/1000\n",
      "612/636 [===========================>..] - ETA: 0s - loss: 1.2345 - mae: 0.7984\n",
      "Epoch 8: val_mae did not improve from 0.36426\n",
      "636/636 [==============================] - 2s 2ms/step - loss: 1.3048 - mae: 0.8254 - val_loss: 2.7963 - val_mae: 1.4955 - lr: 0.0100\n",
      "Epoch 9/1000\n",
      "630/636 [============================>.] - ETA: 0s - loss: 1.5179 - mae: 0.9002\n",
      "Epoch 9: val_mae did not improve from 0.36426\n",
      "\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 0.0009999999776482583.\n",
      "636/636 [==============================] - 2s 2ms/step - loss: 1.5172 - mae: 0.8996 - val_loss: 1.3831 - val_mae: 1.0481 - lr: 0.0100\n",
      "Epoch 10/1000\n",
      "629/636 [============================>.] - ETA: 0s - loss: 0.5811 - mae: 0.3802\n",
      "Epoch 10: val_mae improved from 0.36426 to 0.34831, saving model to checkpoint_dir\\training_1.ckpt\n",
      "INFO:tensorflow:Assets written to: checkpoint_dir\\training_1.ckpt\\assets\n",
      "636/636 [==============================] - 3s 4ms/step - loss: 0.5841 - mae: 0.3815 - val_loss: 0.5477 - val_mae: 0.3483 - lr: 1.0000e-03\n",
      "Epoch 11/1000\n",
      "622/636 [============================>.] - ETA: 0s - loss: 0.5845 - mae: 0.3876\n",
      "Epoch 11: val_mae did not improve from 0.34831\n",
      "636/636 [==============================] - 2s 2ms/step - loss: 0.5859 - mae: 0.3876 - val_loss: 0.5385 - val_mae: 0.3778 - lr: 1.0000e-03\n",
      "Epoch 12/1000\n",
      "627/636 [============================>.] - ETA: 0s - loss: 0.6077 - mae: 0.4174\n",
      "Epoch 12: val_mae improved from 0.34831 to 0.33637, saving model to checkpoint_dir\\training_1.ckpt\n",
      "INFO:tensorflow:Assets written to: checkpoint_dir\\training_1.ckpt\\assets\n",
      "636/636 [==============================] - 2s 3ms/step - loss: 0.6071 - mae: 0.4172 - val_loss: 0.5248 - val_mae: 0.3364 - lr: 1.0000e-03\n",
      "Epoch 13/1000\n",
      "631/636 [============================>.] - ETA: 0s - loss: 0.5887 - mae: 0.3979\n",
      "Epoch 13: val_mae did not improve from 0.33637\n",
      "636/636 [==============================] - 2s 2ms/step - loss: 0.5879 - mae: 0.3991 - val_loss: 0.7470 - val_mae: 0.5565 - lr: 1.0000e-03\n",
      "Epoch 14/1000\n",
      "615/636 [============================>.] - ETA: 0s - loss: 0.6090 - mae: 0.4242\n",
      "Epoch 14: val_mae did not improve from 0.33637\n",
      "636/636 [==============================] - 2s 2ms/step - loss: 0.6108 - mae: 0.4244 - val_loss: 0.5491 - val_mae: 0.4065 - lr: 1.0000e-03\n",
      "Epoch 15/1000\n",
      "622/636 [============================>.] - ETA: 0s - loss: 0.6385 - mae: 0.4453\n",
      "Epoch 15: val_mae did not improve from 0.33637\n",
      "\n",
      "Epoch 15: ReduceLROnPlateau reducing learning rate to 9.999999310821295e-05.\n",
      "636/636 [==============================] - 2s 2ms/step - loss: 0.6458 - mae: 0.4503 - val_loss: 0.8257 - val_mae: 0.6128 - lr: 1.0000e-03\n",
      "Epoch 16/1000\n",
      "616/636 [============================>.] - ETA: 0s - loss: 0.5566 - mae: 0.3484\n",
      "Epoch 16: val_mae improved from 0.33637 to 0.32794, saving model to checkpoint_dir\\training_1.ckpt\n",
      "INFO:tensorflow:Assets written to: checkpoint_dir\\training_1.ckpt\\assets\n",
      "636/636 [==============================] - 2s 3ms/step - loss: 0.5510 - mae: 0.3469 - val_loss: 0.5100 - val_mae: 0.3279 - lr: 1.0000e-04\n",
      "Epoch 17/1000\n",
      "617/636 [============================>.] - ETA: 0s - loss: 0.5520 - mae: 0.3477\n",
      "Epoch 17: val_mae improved from 0.32794 to 0.31865, saving model to checkpoint_dir\\training_1.ckpt\n",
      "INFO:tensorflow:Assets written to: checkpoint_dir\\training_1.ckpt\\assets\n",
      "636/636 [==============================] - 2s 3ms/step - loss: 0.5478 - mae: 0.3464 - val_loss: 0.5078 - val_mae: 0.3187 - lr: 1.0000e-04\n",
      "Epoch 18/1000\n",
      "614/636 [===========================>..] - ETA: 0s - loss: 0.5485 - mae: 0.3459\n",
      "Epoch 18: val_mae did not improve from 0.31865\n",
      "636/636 [==============================] - 2s 2ms/step - loss: 0.5459 - mae: 0.3452 - val_loss: 0.5349 - val_mae: 0.3935 - lr: 1.0000e-04\n",
      "Epoch 19/1000\n",
      "626/636 [============================>.] - ETA: 0s - loss: 0.5471 - mae: 0.3486\n",
      "Epoch 19: val_mae did not improve from 0.31865\n",
      "636/636 [==============================] - 1s 2ms/step - loss: 0.5459 - mae: 0.3481 - val_loss: 0.5153 - val_mae: 0.3508 - lr: 1.0000e-04\n",
      "Epoch 20/1000\n",
      "628/636 [============================>.] - ETA: 0s - loss: 0.5444 - mae: 0.3465\n",
      "Epoch 20: val_mae did not improve from 0.31865\n",
      "\n",
      "Epoch 20: ReduceLROnPlateau reducing learning rate to 1e-05.\n",
      "636/636 [==============================] - 2s 2ms/step - loss: 0.5436 - mae: 0.3466 - val_loss: 0.5316 - val_mae: 0.3413 - lr: 1.0000e-04\n",
      "Epoch 21/1000\n",
      "615/636 [============================>.] - ETA: 0s - loss: 0.5384 - mae: 0.3342\n",
      "Epoch 21: val_mae did not improve from 0.31865\n",
      "636/636 [==============================] - 1s 2ms/step - loss: 0.5348 - mae: 0.3332 - val_loss: 0.5078 - val_mae: 0.3200 - lr: 1.0000e-05\n",
      "Epoch 22/1000\n",
      "614/636 [===========================>..] - ETA: 0s - loss: 0.5326 - mae: 0.3316\n",
      "Epoch 22: val_mae did not improve from 0.31865\n",
      "636/636 [==============================] - 2s 2ms/step - loss: 0.5339 - mae: 0.3320 - val_loss: 0.5092 - val_mae: 0.3305 - lr: 1.0000e-05\n",
      "Epoch 23/1000\n",
      "624/636 [============================>.] - ETA: 0s - loss: 0.5301 - mae: 0.3302\n",
      "Epoch 23: val_mae did not improve from 0.31865\n",
      "636/636 [==============================] - 2s 2ms/step - loss: 0.5332 - mae: 0.3310 - val_loss: 0.5095 - val_mae: 0.3325 - lr: 1.0000e-05\n",
      "Epoch 24/1000\n",
      "628/636 [============================>.] - ETA: 0s - loss: 0.5364 - mae: 0.3340\n",
      "Epoch 24: val_mae improved from 0.31865 to 0.31621, saving model to checkpoint_dir\\training_1.ckpt\n",
      "INFO:tensorflow:Assets written to: checkpoint_dir\\training_1.ckpt\\assets\n",
      "636/636 [==============================] - 2s 3ms/step - loss: 0.5334 - mae: 0.3332 - val_loss: 0.5082 - val_mae: 0.3162 - lr: 1.0000e-05\n",
      "Epoch 25/1000\n",
      "616/636 [============================>.] - ETA: 0s - loss: 0.5315 - mae: 0.3307\n",
      "Epoch 25: val_mae did not improve from 0.31621\n",
      "636/636 [==============================] - 2s 2ms/step - loss: 0.5327 - mae: 0.3314 - val_loss: 0.5073 - val_mae: 0.3246 - lr: 1.0000e-05\n",
      "Epoch 26/1000\n",
      "618/636 [============================>.] - ETA: 0s - loss: 0.5302 - mae: 0.3320\n",
      "Epoch 26: val_mae did not improve from 0.31621\n",
      "636/636 [==============================] - 2s 2ms/step - loss: 0.5323 - mae: 0.3320 - val_loss: 0.5072 - val_mae: 0.3253 - lr: 1.0000e-05\n",
      "Epoch 27/1000\n",
      "630/636 [============================>.] - ETA: 0s - loss: 0.5315 - mae: 0.3310\n",
      "Epoch 27: val_mae did not improve from 0.31621\n",
      "636/636 [==============================] - 1s 2ms/step - loss: 0.5324 - mae: 0.3313 - val_loss: 0.5065 - val_mae: 0.3220 - lr: 1.0000e-05\n",
      "Epoch 28/1000\n",
      "621/636 [============================>.] - ETA: 0s - loss: 0.5350 - mae: 0.3321\n",
      "Epoch 28: val_mae did not improve from 0.31621\n",
      "636/636 [==============================] - 1s 2ms/step - loss: 0.5322 - mae: 0.3313 - val_loss: 0.5073 - val_mae: 0.3272 - lr: 1.0000e-05\n",
      "Epoch 29/1000\n",
      "628/636 [============================>.] - ETA: 0s - loss: 0.5331 - mae: 0.3307\n",
      "Epoch 29: val_mae did not improve from 0.31621\n",
      "636/636 [==============================] - 1s 2ms/step - loss: 0.5325 - mae: 0.3304 - val_loss: 0.5061 - val_mae: 0.3189 - lr: 1.0000e-05\n",
      "Epoch 30/1000\n",
      "633/636 [============================>.] - ETA: 0s - loss: 0.5325 - mae: 0.3304\n",
      "Epoch 30: val_mae did not improve from 0.31621\n",
      "636/636 [==============================] - 2s 2ms/step - loss: 0.5315 - mae: 0.3301 - val_loss: 0.5082 - val_mae: 0.3302 - lr: 1.0000e-05\n",
      "Epoch 31/1000\n",
      "614/636 [===========================>..] - ETA: 0s - loss: 0.5344 - mae: 0.3313\n",
      "Epoch 31: val_mae did not improve from 0.31621\n",
      "636/636 [==============================] - 1s 2ms/step - loss: 0.5317 - mae: 0.3303 - val_loss: 0.5063 - val_mae: 0.3196 - lr: 1.0000e-05\n",
      "Epoch 32/1000\n",
      "619/636 [============================>.] - ETA: 0s - loss: 0.5309 - mae: 0.3308\n",
      "Epoch 32: val_mae did not improve from 0.31621\n",
      "636/636 [==============================] - 1s 2ms/step - loss: 0.5314 - mae: 0.3311 - val_loss: 0.5066 - val_mae: 0.3230 - lr: 1.0000e-05\n",
      "Epoch 33/1000\n",
      "615/636 [============================>.] - ETA: 0s - loss: 0.5318 - mae: 0.3318\n",
      "Epoch 33: val_mae did not improve from 0.31621\n",
      "636/636 [==============================] - 1s 2ms/step - loss: 0.5309 - mae: 0.3311 - val_loss: 0.5082 - val_mae: 0.3303 - lr: 1.0000e-05\n",
      "Epoch 34/1000\n",
      "621/636 [============================>.] - ETA: 0s - loss: 0.5297 - mae: 0.3301\n",
      "Epoch 34: val_mae did not improve from 0.31621\n",
      "636/636 [==============================] - 1s 2ms/step - loss: 0.5305 - mae: 0.3306 - val_loss: 0.5064 - val_mae: 0.3164 - lr: 1.0000e-05\n",
      "Epoch 35/1000\n",
      "613/636 [===========================>..] - ETA: 0s - loss: 0.5346 - mae: 0.3315\n",
      "Epoch 35: val_mae improved from 0.31621 to 0.31519, saving model to checkpoint_dir\\training_1.ckpt\n",
      "INFO:tensorflow:Assets written to: checkpoint_dir\\training_1.ckpt\\assets\n",
      "636/636 [==============================] - 2s 3ms/step - loss: 0.5310 - mae: 0.3305 - val_loss: 0.5066 - val_mae: 0.3152 - lr: 1.0000e-05\n",
      "Epoch 36/1000\n",
      "625/636 [============================>.] - ETA: 0s - loss: 0.5326 - mae: 0.3295\n",
      "Epoch 36: val_mae did not improve from 0.31519\n",
      "636/636 [==============================] - 2s 2ms/step - loss: 0.5303 - mae: 0.3293 - val_loss: 0.5056 - val_mae: 0.3170 - lr: 1.0000e-05\n",
      "Epoch 37/1000\n",
      "631/636 [============================>.] - ETA: 0s - loss: 0.5324 - mae: 0.3303\n",
      "Epoch 37: val_mae did not improve from 0.31519\n",
      "636/636 [==============================] - 1s 2ms/step - loss: 0.5302 - mae: 0.3297 - val_loss: 0.5058 - val_mae: 0.3169 - lr: 1.0000e-05\n",
      "Epoch 38/1000\n",
      "635/636 [============================>.] - ETA: 0s - loss: 0.5305 - mae: 0.3306\n",
      "Epoch 38: val_mae did not improve from 0.31519\n",
      "636/636 [==============================] - 2s 2ms/step - loss: 0.5301 - mae: 0.3306 - val_loss: 0.5054 - val_mae: 0.3211 - lr: 1.0000e-05\n",
      "Epoch 39/1000\n",
      "613/636 [===========================>..] - ETA: 0s - loss: 0.5279 - mae: 0.3301\n",
      "Epoch 39: val_mae did not improve from 0.31519\n",
      "636/636 [==============================] - 2s 2ms/step - loss: 0.5299 - mae: 0.3300 - val_loss: 0.5141 - val_mae: 0.3501 - lr: 1.0000e-05\n",
      "Epoch 40/1000\n",
      "626/636 [============================>.] - ETA: 0s - loss: 0.5305 - mae: 0.3296\n",
      "Epoch 40: val_mae did not improve from 0.31519\n",
      "636/636 [==============================] - 2s 2ms/step - loss: 0.5305 - mae: 0.3297 - val_loss: 0.5104 - val_mae: 0.3387 - lr: 1.0000e-05\n",
      "Epoch 41/1000\n",
      "627/636 [============================>.] - ETA: 0s - loss: 0.5305 - mae: 0.3302\n",
      "Epoch 41: val_mae did not improve from 0.31519\n",
      "636/636 [==============================] - 1s 2ms/step - loss: 0.5298 - mae: 0.3300 - val_loss: 0.5067 - val_mae: 0.3273 - lr: 1.0000e-05\n",
      "Epoch 42/1000\n",
      "636/636 [==============================] - ETA: 0s - loss: 0.5295 - mae: 0.3288\n",
      "Epoch 42: val_mae did not improve from 0.31519\n",
      "636/636 [==============================] - 2s 2ms/step - loss: 0.5295 - mae: 0.3288 - val_loss: 0.5131 - val_mae: 0.3465 - lr: 1.0000e-05\n",
      "Epoch 43/1000\n",
      "628/636 [============================>.] - ETA: 0s - loss: 0.5318 - mae: 0.3302\n",
      "Epoch 43: val_mae did not improve from 0.31519\n",
      "636/636 [==============================] - 2s 2ms/step - loss: 0.5296 - mae: 0.3297 - val_loss: 0.5052 - val_mae: 0.3200 - lr: 1.0000e-05\n",
      "Epoch 44/1000\n",
      "636/636 [==============================] - ETA: 0s - loss: 0.5290 - mae: 0.3291\n",
      "Epoch 44: val_mae did not improve from 0.31519\n",
      "636/636 [==============================] - 1s 2ms/step - loss: 0.5290 - mae: 0.3291 - val_loss: 0.5157 - val_mae: 0.3535 - lr: 1.0000e-05\n",
      "Epoch 45/1000\n",
      "624/636 [============================>.] - ETA: 0s - loss: 0.5287 - mae: 0.3286\n",
      "Epoch 45: val_mae did not improve from 0.31519\n",
      "636/636 [==============================] - 1s 2ms/step - loss: 0.5290 - mae: 0.3288 - val_loss: 0.5052 - val_mae: 0.3197 - lr: 1.0000e-05\n",
      "Epoch 46/1000\n",
      "625/636 [============================>.] - ETA: 0s - loss: 0.5320 - mae: 0.3297\n",
      "Epoch 46: val_mae did not improve from 0.31519\n",
      "636/636 [==============================] - 2s 2ms/step - loss: 0.5291 - mae: 0.3293 - val_loss: 0.5067 - val_mae: 0.3284 - lr: 1.0000e-05\n",
      "Epoch 47/1000\n",
      "633/636 [============================>.] - ETA: 0s - loss: 0.5294 - mae: 0.3290\n",
      "Epoch 47: val_mae did not improve from 0.31519\n",
      "636/636 [==============================] - 1s 2ms/step - loss: 0.5288 - mae: 0.3289 - val_loss: 0.5055 - val_mae: 0.3155 - lr: 1.0000e-05\n",
      "Epoch 48/1000\n",
      "613/636 [===========================>..] - ETA: 0s - loss: 0.5307 - mae: 0.3301\n",
      "Epoch 48: val_mae did not improve from 0.31519\n",
      "636/636 [==============================] - 2s 2ms/step - loss: 0.5292 - mae: 0.3296 - val_loss: 0.5054 - val_mae: 0.3219 - lr: 1.0000e-05\n",
      "Epoch 49/1000\n",
      "612/636 [===========================>..] - ETA: 0s - loss: 0.5214 - mae: 0.3259\n",
      "Epoch 49: val_mae did not improve from 0.31519\n",
      "636/636 [==============================] - 1s 2ms/step - loss: 0.5279 - mae: 0.3280 - val_loss: 0.5114 - val_mae: 0.3401 - lr: 1.0000e-05\n",
      "Epoch 50/1000\n",
      "633/636 [============================>.] - ETA: 0s - loss: 0.5279 - mae: 0.3281\n",
      "Epoch 50: val_mae did not improve from 0.31519\n",
      "636/636 [==============================] - 1s 2ms/step - loss: 0.5280 - mae: 0.3282 - val_loss: 0.5090 - val_mae: 0.3351 - lr: 1.0000e-05\n",
      "Epoch 51/1000\n",
      "626/636 [============================>.] - ETA: 0s - loss: 0.5276 - mae: 0.3291\n",
      "Epoch 51: val_mae did not improve from 0.31519\n",
      "636/636 [==============================] - 2s 2ms/step - loss: 0.5278 - mae: 0.3290 - val_loss: 0.5054 - val_mae: 0.3194 - lr: 1.0000e-05\n",
      "Epoch 52/1000\n",
      "631/636 [============================>.] - ETA: 0s - loss: 0.5281 - mae: 0.3282\n",
      "Epoch 52: val_mae did not improve from 0.31519\n",
      "636/636 [==============================] - 1s 2ms/step - loss: 0.5274 - mae: 0.3282 - val_loss: 0.5057 - val_mae: 0.3154 - lr: 1.0000e-05\n",
      "Epoch 53/1000\n",
      "628/636 [============================>.] - ETA: 0s - loss: 0.5285 - mae: 0.3279\n",
      "Epoch 53: val_mae did not improve from 0.31519\n",
      "636/636 [==============================] - 1s 2ms/step - loss: 0.5279 - mae: 0.3275 - val_loss: 0.5052 - val_mae: 0.3192 - lr: 1.0000e-05\n",
      "Epoch 54/1000\n",
      "612/636 [===========================>..] - ETA: 0s - loss: 0.5282 - mae: 0.3282\n",
      "Epoch 54: val_mae did not improve from 0.31519\n",
      "636/636 [==============================] - 1s 2ms/step - loss: 0.5274 - mae: 0.3276 - val_loss: 0.5063 - val_mae: 0.3242 - lr: 1.0000e-05\n",
      "Epoch 55/1000\n",
      "618/636 [============================>.] - ETA: 0s - loss: 0.5267 - mae: 0.3280\n",
      "Epoch 55: val_mae did not improve from 0.31519\n",
      "636/636 [==============================] - 2s 2ms/step - loss: 0.5273 - mae: 0.3275 - val_loss: 0.5065 - val_mae: 0.3266 - lr: 1.0000e-05\n",
      "Epoch 56/1000\n",
      "627/636 [============================>.] - ETA: 0s - loss: 0.5236 - mae: 0.3281\n",
      "Epoch 56: val_mae did not improve from 0.31519\n",
      "636/636 [==============================] - 2s 2ms/step - loss: 0.5268 - mae: 0.3287 - val_loss: 0.5054 - val_mae: 0.3233 - lr: 1.0000e-05\n",
      "Epoch 57/1000\n",
      "624/636 [============================>.] - ETA: 0s - loss: 0.5269 - mae: 0.3272\n",
      "Epoch 57: val_mae did not improve from 0.31519\n",
      "636/636 [==============================] - 1s 2ms/step - loss: 0.5274 - mae: 0.3275 - val_loss: 0.5093 - val_mae: 0.3359 - lr: 1.0000e-05\n",
      "Epoch 58/1000\n",
      "623/636 [============================>.] - ETA: 0s - loss: 0.5303 - mae: 0.3286\n",
      "Epoch 58: val_mae did not improve from 0.31519\n",
      "636/636 [==============================] - 1s 2ms/step - loss: 0.5278 - mae: 0.3280 - val_loss: 0.5055 - val_mae: 0.3193 - lr: 1.0000e-05\n",
      "Epoch 59/1000\n",
      "620/636 [============================>.] - ETA: 0s - loss: 0.5261 - mae: 0.3286\n",
      "Epoch 59: val_mae did not improve from 0.31519\n",
      "636/636 [==============================] - 2s 2ms/step - loss: 0.5266 - mae: 0.3285 - val_loss: 0.5048 - val_mae: 0.3189 - lr: 1.0000e-05\n",
      "Epoch 60/1000\n",
      "636/636 [==============================] - ETA: 0s - loss: 0.5264 - mae: 0.3282\n",
      "Epoch 60: val_mae did not improve from 0.31519\n",
      "636/636 [==============================] - 1s 2ms/step - loss: 0.5264 - mae: 0.3282 - val_loss: 0.5045 - val_mae: 0.3178 - lr: 1.0000e-05\n",
      "Epoch 61/1000\n",
      "635/636 [============================>.] - ETA: 0s - loss: 0.5266 - mae: 0.3285\n",
      "Epoch 61: val_mae did not improve from 0.31519\n",
      "636/636 [==============================] - 2s 2ms/step - loss: 0.5265 - mae: 0.3284 - val_loss: 0.5045 - val_mae: 0.3165 - lr: 1.0000e-05\n",
      "Epoch 62/1000\n",
      "614/636 [===========================>..] - ETA: 0s - loss: 0.5253 - mae: 0.3264\n",
      "Epoch 62: val_mae did not improve from 0.31519\n",
      "636/636 [==============================] - 1s 2ms/step - loss: 0.5260 - mae: 0.3266 - val_loss: 0.5093 - val_mae: 0.3360 - lr: 1.0000e-05\n",
      "Epoch 63/1000\n",
      "616/636 [============================>.] - ETA: 0s - loss: 0.5269 - mae: 0.3281\n",
      "Epoch 63: val_mae did not improve from 0.31519\n",
      "636/636 [==============================] - 1s 2ms/step - loss: 0.5266 - mae: 0.3284 - val_loss: 0.5058 - val_mae: 0.3233 - lr: 1.0000e-05\n",
      "Epoch 64/1000\n",
      "629/636 [============================>.] - ETA: 0s - loss: 0.5267 - mae: 0.3282\n",
      "Epoch 64: val_mae did not improve from 0.31519\n",
      "636/636 [==============================] - 1s 2ms/step - loss: 0.5258 - mae: 0.3281 - val_loss: 0.5050 - val_mae: 0.3202 - lr: 1.0000e-05\n",
      "Epoch 65/1000\n",
      "616/636 [============================>.] - ETA: 0s - loss: 0.5263 - mae: 0.3280\n",
      "Epoch 65: val_mae did not improve from 0.31519\n",
      "636/636 [==============================] - 1s 2ms/step - loss: 0.5255 - mae: 0.3277 - val_loss: 0.5066 - val_mae: 0.3283 - lr: 1.0000e-05\n",
      "Epoch 66/1000\n",
      "625/636 [============================>.] - ETA: 0s - loss: 0.5272 - mae: 0.3289\n",
      "Epoch 66: val_mae improved from 0.31519 to 0.31483, saving model to checkpoint_dir\\training_1.ckpt\n",
      "INFO:tensorflow:Assets written to: checkpoint_dir\\training_1.ckpt\\assets\n",
      "636/636 [==============================] - 2s 3ms/step - loss: 0.5250 - mae: 0.3282 - val_loss: 0.5050 - val_mae: 0.3148 - lr: 1.0000e-05\n",
      "Epoch 67/1000\n",
      "621/636 [============================>.] - ETA: 0s - loss: 0.5273 - mae: 0.3279\n",
      "Epoch 67: val_mae improved from 0.31483 to 0.31479, saving model to checkpoint_dir\\training_1.ckpt\n",
      "INFO:tensorflow:Assets written to: checkpoint_dir\\training_1.ckpt\\assets\n",
      "636/636 [==============================] - 2s 3ms/step - loss: 0.5255 - mae: 0.3275 - val_loss: 0.5051 - val_mae: 0.3148 - lr: 1.0000e-05\n",
      "Epoch 68/1000\n",
      "626/636 [============================>.] - ETA: 0s - loss: 0.5234 - mae: 0.3256\n",
      "Epoch 68: val_mae did not improve from 0.31479\n",
      "636/636 [==============================] - 2s 2ms/step - loss: 0.5250 - mae: 0.3262 - val_loss: 0.5204 - val_mae: 0.3644 - lr: 1.0000e-05\n",
      "Epoch 69/1000\n",
      "633/636 [============================>.] - ETA: 0s - loss: 0.5247 - mae: 0.3268\n",
      "Epoch 69: val_mae did not improve from 0.31479\n",
      "636/636 [==============================] - 1s 2ms/step - loss: 0.5251 - mae: 0.3268 - val_loss: 0.5112 - val_mae: 0.3418 - lr: 1.0000e-05\n",
      "Epoch 70/1000\n",
      "619/636 [============================>.] - ETA: 0s - loss: 0.5264 - mae: 0.3284\n",
      "Epoch 70: val_mae improved from 0.31479 to 0.31393, saving model to checkpoint_dir\\training_1.ckpt\n",
      "INFO:tensorflow:Assets written to: checkpoint_dir\\training_1.ckpt\\assets\n",
      "636/636 [==============================] - 2s 3ms/step - loss: 0.5256 - mae: 0.3280 - val_loss: 0.5049 - val_mae: 0.3139 - lr: 1.0000e-05\n",
      "Epoch 71/1000\n",
      "629/636 [============================>.] - ETA: 0s - loss: 0.5255 - mae: 0.3274\n",
      "Epoch 71: val_mae did not improve from 0.31393\n",
      "636/636 [==============================] - 2s 2ms/step - loss: 0.5247 - mae: 0.3275 - val_loss: 0.5051 - val_mae: 0.3194 - lr: 1.0000e-05\n",
      "Epoch 72/1000\n",
      "623/636 [============================>.] - ETA: 0s - loss: 0.5258 - mae: 0.3274\n",
      "Epoch 72: val_mae did not improve from 0.31393\n",
      "636/636 [==============================] - 2s 3ms/step - loss: 0.5250 - mae: 0.3270 - val_loss: 0.5047 - val_mae: 0.3166 - lr: 1.0000e-05\n",
      "Epoch 73/1000\n",
      "632/636 [============================>.] - ETA: 0s - loss: 0.5256 - mae: 0.3270\n",
      "Epoch 73: val_mae did not improve from 0.31393\n",
      "636/636 [==============================] - 1s 2ms/step - loss: 0.5242 - mae: 0.3269 - val_loss: 0.5063 - val_mae: 0.3273 - lr: 1.0000e-05\n",
      "Epoch 74/1000\n",
      "619/636 [============================>.] - ETA: 0s - loss: 0.5255 - mae: 0.3278\n",
      "Epoch 74: val_mae did not improve from 0.31393\n",
      "636/636 [==============================] - 1s 2ms/step - loss: 0.5246 - mae: 0.3278 - val_loss: 0.5044 - val_mae: 0.3201 - lr: 1.0000e-05\n",
      "Epoch 75/1000\n",
      "625/636 [============================>.] - ETA: 0s - loss: 0.5209 - mae: 0.3255\n",
      "Epoch 75: val_mae did not improve from 0.31393\n",
      "636/636 [==============================] - 1s 2ms/step - loss: 0.5241 - mae: 0.3265 - val_loss: 0.5041 - val_mae: 0.3202 - lr: 1.0000e-05\n",
      "Epoch 76/1000\n",
      "614/636 [===========================>..] - ETA: 0s - loss: 0.5253 - mae: 0.3275\n",
      "Epoch 76: val_mae did not improve from 0.31393\n",
      "636/636 [==============================] - 1s 2ms/step - loss: 0.5241 - mae: 0.3268 - val_loss: 0.5055 - val_mae: 0.3246 - lr: 1.0000e-05\n",
      "Epoch 77/1000\n",
      "635/636 [============================>.] - ETA: 0s - loss: 0.5242 - mae: 0.3266\n",
      "Epoch 77: val_mae did not improve from 0.31393\n",
      "636/636 [==============================] - 1s 2ms/step - loss: 0.5239 - mae: 0.3265 - val_loss: 0.5039 - val_mae: 0.3170 - lr: 1.0000e-05\n",
      "Epoch 78/1000\n",
      "635/636 [============================>.] - ETA: 0s - loss: 0.5245 - mae: 0.3263\n",
      "Epoch 78: val_mae did not improve from 0.31393\n",
      "636/636 [==============================] - 1s 2ms/step - loss: 0.5238 - mae: 0.3261 - val_loss: 0.5045 - val_mae: 0.3199 - lr: 1.0000e-05\n",
      "Epoch 79/1000\n",
      "628/636 [============================>.] - ETA: 0s - loss: 0.5211 - mae: 0.3254\n",
      "Epoch 79: val_mae did not improve from 0.31393\n",
      "636/636 [==============================] - 1s 2ms/step - loss: 0.5234 - mae: 0.3260 - val_loss: 0.5070 - val_mae: 0.3293 - lr: 1.0000e-05\n",
      "Epoch 80/1000\n",
      "623/636 [============================>.] - ETA: 0s - loss: 0.5242 - mae: 0.3254\n",
      "Epoch 80: val_mae did not improve from 0.31393\n",
      "636/636 [==============================] - 1s 2ms/step - loss: 0.5232 - mae: 0.3256 - val_loss: 0.5069 - val_mae: 0.3143 - lr: 1.0000e-05\n",
      "Epoch 81/1000\n",
      "627/636 [============================>.] - ETA: 0s - loss: 0.5256 - mae: 0.3281\n",
      "Epoch 81: val_mae improved from 0.31393 to 0.31346, saving model to checkpoint_dir\\training_1.ckpt\n",
      "INFO:tensorflow:Assets written to: checkpoint_dir\\training_1.ckpt\\assets\n",
      "636/636 [==============================] - 2s 3ms/step - loss: 0.5235 - mae: 0.3274 - val_loss: 0.5059 - val_mae: 0.3135 - lr: 1.0000e-05\n",
      "Epoch 82/1000\n",
      "622/636 [============================>.] - ETA: 0s - loss: 0.5254 - mae: 0.3256\n",
      "Epoch 82: val_mae did not improve from 0.31346\n",
      "636/636 [==============================] - 2s 2ms/step - loss: 0.5234 - mae: 0.3255 - val_loss: 0.5061 - val_mae: 0.3252 - lr: 1.0000e-05\n",
      "Epoch 83/1000\n",
      "631/636 [============================>.] - ETA: 0s - loss: 0.5243 - mae: 0.3267\n",
      "Epoch 83: val_mae did not improve from 0.31346\n",
      "636/636 [==============================] - 1s 2ms/step - loss: 0.5234 - mae: 0.3264 - val_loss: 0.5073 - val_mae: 0.3152 - lr: 1.0000e-05\n",
      "Epoch 84/1000\n",
      "634/636 [============================>.] - ETA: 0s - loss: 0.5222 - mae: 0.3262\n",
      "Epoch 84: val_mae did not improve from 0.31346\n",
      "636/636 [==============================] - 1s 2ms/step - loss: 0.5227 - mae: 0.3264 - val_loss: 0.5046 - val_mae: 0.3157 - lr: 1.0000e-05\n",
      "Epoch 85/1000\n",
      "620/636 [============================>.] - ETA: 0s - loss: 0.5223 - mae: 0.3258\n",
      "Epoch 85: val_mae did not improve from 0.31346\n",
      "636/636 [==============================] - 2s 2ms/step - loss: 0.5230 - mae: 0.3259 - val_loss: 0.5046 - val_mae: 0.3220 - lr: 1.0000e-05\n",
      "Epoch 86/1000\n",
      "636/636 [==============================] - ETA: 0s - loss: 0.5222 - mae: 0.3248\n",
      "Epoch 86: val_mae did not improve from 0.31346\n",
      "636/636 [==============================] - 2s 2ms/step - loss: 0.5222 - mae: 0.3248 - val_loss: 0.5039 - val_mae: 0.3186 - lr: 1.0000e-05\n",
      "Epoch 87/1000\n",
      "631/636 [============================>.] - ETA: 0s - loss: 0.5201 - mae: 0.3239\n",
      "Epoch 87: val_mae did not improve from 0.31346\n",
      "636/636 [==============================] - 2s 2ms/step - loss: 0.5218 - mae: 0.3243 - val_loss: 0.5044 - val_mae: 0.3145 - lr: 1.0000e-05\n",
      "Epoch 88/1000\n",
      "615/636 [============================>.] - ETA: 0s - loss: 0.5206 - mae: 0.3257\n",
      "Epoch 88: val_mae did not improve from 0.31346\n",
      "636/636 [==============================] - 2s 3ms/step - loss: 0.5224 - mae: 0.3258 - val_loss: 0.5052 - val_mae: 0.3195 - lr: 1.0000e-05\n",
      "Epoch 89/1000\n",
      "618/636 [============================>.] - ETA: 0s - loss: 0.5221 - mae: 0.3263\n",
      "Epoch 89: val_mae did not improve from 0.31346\n",
      "636/636 [==============================] - 1s 2ms/step - loss: 0.5229 - mae: 0.3263 - val_loss: 0.5044 - val_mae: 0.3208 - lr: 1.0000e-05\n",
      "Epoch 90/1000\n",
      "636/636 [==============================] - ETA: 0s - loss: 0.5224 - mae: 0.3259\n",
      "Epoch 90: val_mae improved from 0.31346 to 0.31338, saving model to checkpoint_dir\\training_1.ckpt\n",
      "INFO:tensorflow:Assets written to: checkpoint_dir\\training_1.ckpt\\assets\n",
      "636/636 [==============================] - 2s 3ms/step - loss: 0.5224 - mae: 0.3259 - val_loss: 0.5060 - val_mae: 0.3134 - lr: 1.0000e-05\n",
      "Epoch 91/1000\n",
      "613/636 [===========================>..] - ETA: 0s - loss: 0.5262 - mae: 0.3267\n",
      "Epoch 91: val_mae did not improve from 0.31338\n",
      "636/636 [==============================] - 2s 3ms/step - loss: 0.5218 - mae: 0.3254 - val_loss: 0.5071 - val_mae: 0.3152 - lr: 1.0000e-05\n",
      "Epoch 92/1000\n",
      "622/636 [============================>.] - ETA: 0s - loss: 0.5204 - mae: 0.3254\n",
      "Epoch 92: val_mae did not improve from 0.31338\n",
      "636/636 [==============================] - 2s 3ms/step - loss: 0.5217 - mae: 0.3255 - val_loss: 0.5055 - val_mae: 0.3263 - lr: 1.0000e-05\n",
      "Epoch 93/1000\n",
      "624/636 [============================>.] - ETA: 0s - loss: 0.5199 - mae: 0.3253\n",
      "Epoch 93: val_mae did not improve from 0.31338\n",
      "636/636 [==============================] - 2s 2ms/step - loss: 0.5218 - mae: 0.3257 - val_loss: 0.5170 - val_mae: 0.3563 - lr: 1.0000e-05\n",
      "Epoch 94/1000\n",
      "623/636 [============================>.] - ETA: 0s - loss: 0.5250 - mae: 0.3266\n",
      "Epoch 94: val_mae did not improve from 0.31338\n",
      "636/636 [==============================] - 2s 2ms/step - loss: 0.5218 - mae: 0.3255 - val_loss: 0.5051 - val_mae: 0.3141 - lr: 1.0000e-05\n",
      "Epoch 95/1000\n",
      "622/636 [============================>.] - ETA: 0s - loss: 0.5155 - mae: 0.3232\n",
      "Epoch 95: val_mae did not improve from 0.31338\n",
      "636/636 [==============================] - 2s 2ms/step - loss: 0.5211 - mae: 0.3246 - val_loss: 0.5047 - val_mae: 0.3230 - lr: 1.0000e-05\n",
      "Epoch 96/1000\n",
      "627/636 [============================>.] - ETA: 0s - loss: 0.5239 - mae: 0.3262\n",
      "Epoch 96: val_mae did not improve from 0.31338\n",
      "636/636 [==============================] - 2s 2ms/step - loss: 0.5213 - mae: 0.3256 - val_loss: 0.5047 - val_mae: 0.3221 - lr: 1.0000e-05\n",
      "Epoch 97/1000\n",
      "619/636 [============================>.] - ETA: 0s - loss: 0.5211 - mae: 0.3257\n",
      "Epoch 97: val_mae did not improve from 0.31338\n",
      "636/636 [==============================] - 2s 3ms/step - loss: 0.5207 - mae: 0.3257 - val_loss: 0.5070 - val_mae: 0.3319 - lr: 1.0000e-05\n",
      "Epoch 98/1000\n",
      "625/636 [============================>.] - ETA: 0s - loss: 0.5185 - mae: 0.3232\n",
      "Epoch 98: val_mae did not improve from 0.31338\n",
      "636/636 [==============================] - 2s 2ms/step - loss: 0.5208 - mae: 0.3241 - val_loss: 0.5073 - val_mae: 0.3306 - lr: 1.0000e-05\n",
      "Epoch 99/1000\n",
      "617/636 [============================>.] - ETA: 0s - loss: 0.5237 - mae: 0.3240\n",
      "Epoch 99: val_mae did not improve from 0.31338\n",
      "636/636 [==============================] - 2s 2ms/step - loss: 0.5208 - mae: 0.3241 - val_loss: 0.5044 - val_mae: 0.3151 - lr: 1.0000e-05\n",
      "Epoch 100/1000\n",
      "628/636 [============================>.] - ETA: 0s - loss: 0.5214 - mae: 0.3248\n",
      "Epoch 100: val_mae did not improve from 0.31338\n",
      "636/636 [==============================] - 2s 2ms/step - loss: 0.5206 - mae: 0.3245 - val_loss: 0.5042 - val_mae: 0.3134 - lr: 1.0000e-05\n",
      "Epoch 101/1000\n",
      "627/636 [============================>.] - ETA: 0s - loss: 0.5227 - mae: 0.3258\n",
      "Epoch 101: val_mae did not improve from 0.31338\n",
      "636/636 [==============================] - 2s 2ms/step - loss: 0.5208 - mae: 0.3255 - val_loss: 0.5038 - val_mae: 0.3174 - lr: 1.0000e-05\n",
      "Epoch 102/1000\n",
      "629/636 [============================>.] - ETA: 0s - loss: 0.5194 - mae: 0.3240\n",
      "Epoch 102: val_mae did not improve from 0.31338\n",
      "636/636 [==============================] - 2s 2ms/step - loss: 0.5200 - mae: 0.3243 - val_loss: 0.5055 - val_mae: 0.3248 - lr: 1.0000e-05\n",
      "Epoch 103/1000\n",
      "629/636 [============================>.] - ETA: 0s - loss: 0.5232 - mae: 0.3244\n",
      "Epoch 103: val_mae did not improve from 0.31338\n",
      "636/636 [==============================] - 2s 2ms/step - loss: 0.5202 - mae: 0.3235 - val_loss: 0.5046 - val_mae: 0.3138 - lr: 1.0000e-05\n",
      "Epoch 104/1000\n",
      "614/636 [===========================>..] - ETA: 0s - loss: 0.5215 - mae: 0.3254\n",
      "Epoch 104: val_mae did not improve from 0.31338\n",
      "636/636 [==============================] - 2s 2ms/step - loss: 0.5200 - mae: 0.3245 - val_loss: 0.5053 - val_mae: 0.3225 - lr: 1.0000e-05\n",
      "Epoch 105/1000\n",
      "634/636 [============================>.] - ETA: 0s - loss: 0.5181 - mae: 0.3249\n",
      "Epoch 105: val_mae did not improve from 0.31338\n",
      "636/636 [==============================] - 2s 2ms/step - loss: 0.5197 - mae: 0.3253 - val_loss: 0.5097 - val_mae: 0.3385 - lr: 1.0000e-05\n",
      "Epoch 106/1000\n",
      "615/636 [============================>.] - ETA: 0s - loss: 0.5192 - mae: 0.3245\n",
      "Epoch 106: val_mae did not improve from 0.31338\n",
      "636/636 [==============================] - 1s 2ms/step - loss: 0.5200 - mae: 0.3243 - val_loss: 0.5038 - val_mae: 0.3171 - lr: 1.0000e-05\n",
      "Epoch 107/1000\n",
      "635/636 [============================>.] - ETA: 0s - loss: 0.5196 - mae: 0.3244\n",
      "Epoch 107: val_mae did not improve from 0.31338\n",
      "636/636 [==============================] - 2s 2ms/step - loss: 0.5197 - mae: 0.3246 - val_loss: 0.5030 - val_mae: 0.3151 - lr: 1.0000e-05\n",
      "Epoch 108/1000\n",
      "624/636 [============================>.] - ETA: 0s - loss: 0.5152 - mae: 0.3225\n",
      "Epoch 108: val_mae did not improve from 0.31338\n",
      "636/636 [==============================] - 2s 2ms/step - loss: 0.5190 - mae: 0.3233 - val_loss: 0.5163 - val_mae: 0.3560 - lr: 1.0000e-05\n",
      "Epoch 109/1000\n",
      "620/636 [============================>.] - ETA: 0s - loss: 0.5240 - mae: 0.3269\n",
      "Epoch 109: val_mae improved from 0.31338 to 0.31189, saving model to checkpoint_dir\\training_1.ckpt\n",
      "INFO:tensorflow:Assets written to: checkpoint_dir\\training_1.ckpt\\assets\n",
      "636/636 [==============================] - 2s 3ms/step - loss: 0.5195 - mae: 0.3251 - val_loss: 0.5051 - val_mae: 0.3119 - lr: 1.0000e-05\n",
      "Epoch 110/1000\n",
      "616/636 [============================>.] - ETA: 0s - loss: 0.5228 - mae: 0.3251\n",
      "Epoch 110: val_mae did not improve from 0.31189\n",
      "636/636 [==============================] - 2s 3ms/step - loss: 0.5187 - mae: 0.3236 - val_loss: 0.5030 - val_mae: 0.3159 - lr: 1.0000e-05\n",
      "Epoch 111/1000\n",
      "621/636 [============================>.] - ETA: 0s - loss: 0.5198 - mae: 0.3252\n",
      "Epoch 111: val_mae did not improve from 0.31189\n",
      "636/636 [==============================] - 2s 2ms/step - loss: 0.5193 - mae: 0.3243 - val_loss: 0.5036 - val_mae: 0.3138 - lr: 1.0000e-05\n",
      "Epoch 112/1000\n",
      "626/636 [============================>.] - ETA: 0s - loss: 0.5156 - mae: 0.3242\n",
      "Epoch 112: val_mae did not improve from 0.31189\n",
      "636/636 [==============================] - 2s 2ms/step - loss: 0.5190 - mae: 0.3250 - val_loss: 0.5057 - val_mae: 0.3272 - lr: 1.0000e-05\n",
      "Epoch 113/1000\n",
      "618/636 [============================>.] - ETA: 0s - loss: 0.5169 - mae: 0.3241\n",
      "Epoch 113: val_mae did not improve from 0.31189\n",
      "636/636 [==============================] - 2s 2ms/step - loss: 0.5185 - mae: 0.3242 - val_loss: 0.5051 - val_mae: 0.3238 - lr: 1.0000e-05\n",
      "Epoch 114/1000\n",
      "618/636 [============================>.] - ETA: 0s - loss: 0.5228 - mae: 0.3260\n",
      "Epoch 114: val_mae did not improve from 0.31189\n",
      "636/636 [==============================] - 1s 2ms/step - loss: 0.5188 - mae: 0.3249 - val_loss: 0.5030 - val_mae: 0.3158 - lr: 1.0000e-05\n",
      "Epoch 115/1000\n",
      "627/636 [============================>.] - ETA: 0s - loss: 0.5226 - mae: 0.3248\n",
      "Epoch 115: val_mae did not improve from 0.31189\n",
      "636/636 [==============================] - 2s 2ms/step - loss: 0.5185 - mae: 0.3236 - val_loss: 0.5043 - val_mae: 0.3139 - lr: 1.0000e-05\n",
      "Epoch 116/1000\n",
      "633/636 [============================>.] - ETA: 0s - loss: 0.5193 - mae: 0.3235\n",
      "Epoch 116: val_mae did not improve from 0.31189\n",
      "636/636 [==============================] - 2s 2ms/step - loss: 0.5180 - mae: 0.3233 - val_loss: 0.5047 - val_mae: 0.3186 - lr: 1.0000e-05\n",
      "Epoch 117/1000\n",
      "632/636 [============================>.] - ETA: 0s - loss: 0.5182 - mae: 0.3231\n",
      "Epoch 117: val_mae did not improve from 0.31189\n",
      "636/636 [==============================] - 1s 2ms/step - loss: 0.5180 - mae: 0.3233 - val_loss: 0.5043 - val_mae: 0.3218 - lr: 1.0000e-05\n",
      "Epoch 118/1000\n",
      "635/636 [============================>.] - ETA: 0s - loss: 0.5179 - mae: 0.3232\n",
      "Epoch 118: val_mae did not improve from 0.31189\n",
      "636/636 [==============================] - 1s 2ms/step - loss: 0.5178 - mae: 0.3231 - val_loss: 0.5077 - val_mae: 0.3333 - lr: 1.0000e-05\n",
      "Epoch 119/1000\n",
      "621/636 [============================>.] - ETA: 0s - loss: 0.5196 - mae: 0.3249\n",
      "Epoch 119: val_mae did not improve from 0.31189\n",
      "636/636 [==============================] - 1s 2ms/step - loss: 0.5182 - mae: 0.3247 - val_loss: 0.5038 - val_mae: 0.3219 - lr: 1.0000e-05\n",
      "Epoch 120/1000\n",
      "629/636 [============================>.] - ETA: 0s - loss: 0.5181 - mae: 0.3230\n",
      "Epoch 120: val_mae did not improve from 0.31189\n",
      "636/636 [==============================] - 1s 2ms/step - loss: 0.5177 - mae: 0.3228 - val_loss: 0.5030 - val_mae: 0.3134 - lr: 1.0000e-05\n",
      "Epoch 121/1000\n",
      "618/636 [============================>.] - ETA: 0s - loss: 0.5151 - mae: 0.3224\n",
      "Epoch 121: val_mae did not improve from 0.31189\n",
      "636/636 [==============================] - 1s 2ms/step - loss: 0.5175 - mae: 0.3231 - val_loss: 0.5043 - val_mae: 0.3209 - lr: 1.0000e-05\n",
      "Epoch 122/1000\n",
      "622/636 [============================>.] - ETA: 0s - loss: 0.5188 - mae: 0.3241\n",
      "Epoch 122: val_mae did not improve from 0.31189\n",
      "636/636 [==============================] - 1s 2ms/step - loss: 0.5172 - mae: 0.3235 - val_loss: 0.5050 - val_mae: 0.3131 - lr: 1.0000e-05\n",
      "Epoch 123/1000\n",
      "622/636 [============================>.] - ETA: 0s - loss: 0.5183 - mae: 0.3240\n",
      "Epoch 123: val_mae did not improve from 0.31189\n",
      "636/636 [==============================] - 1s 2ms/step - loss: 0.5175 - mae: 0.3239 - val_loss: 0.5030 - val_mae: 0.3164 - lr: 1.0000e-05\n",
      "Epoch 124/1000\n",
      "631/636 [============================>.] - ETA: 0s - loss: 0.5186 - mae: 0.3237\n",
      "Epoch 124: val_mae did not improve from 0.31189\n",
      "636/636 [==============================] - 1s 2ms/step - loss: 0.5167 - mae: 0.3232 - val_loss: 0.5047 - val_mae: 0.3130 - lr: 1.0000e-05\n",
      "Epoch 125/1000\n",
      "622/636 [============================>.] - ETA: 0s - loss: 0.5158 - mae: 0.3223\n",
      "Epoch 125: val_mae did not improve from 0.31189\n",
      "636/636 [==============================] - 1s 2ms/step - loss: 0.5167 - mae: 0.3223 - val_loss: 0.5035 - val_mae: 0.3178 - lr: 1.0000e-05\n",
      "Epoch 126/1000\n",
      "629/636 [============================>.] - ETA: 0s - loss: 0.5195 - mae: 0.3227\n",
      "Epoch 126: val_mae did not improve from 0.31189\n",
      "636/636 [==============================] - 1s 2ms/step - loss: 0.5167 - mae: 0.3221 - val_loss: 0.5042 - val_mae: 0.3155 - lr: 1.0000e-05\n",
      "Epoch 127/1000\n",
      "632/636 [============================>.] - ETA: 0s - loss: 0.5180 - mae: 0.3244\n",
      "Epoch 127: val_mae did not improve from 0.31189\n",
      "636/636 [==============================] - 1s 2ms/step - loss: 0.5171 - mae: 0.3242 - val_loss: 0.5066 - val_mae: 0.3136 - lr: 1.0000e-05\n",
      "Epoch 128/1000\n",
      "620/636 [============================>.] - ETA: 0s - loss: 0.5152 - mae: 0.3240\n",
      "Epoch 128: val_mae did not improve from 0.31189\n",
      "636/636 [==============================] - 1s 2ms/step - loss: 0.5168 - mae: 0.3241 - val_loss: 0.5024 - val_mae: 0.3135 - lr: 1.0000e-05\n",
      "Epoch 129/1000\n",
      "617/636 [============================>.] - ETA: 0s - loss: 0.5202 - mae: 0.3247\n",
      "Epoch 129: val_mae did not improve from 0.31189\n",
      "636/636 [==============================] - 1s 2ms/step - loss: 0.5168 - mae: 0.3237 - val_loss: 0.5026 - val_mae: 0.3139 - lr: 1.0000e-05\n",
      "Epoch 130/1000\n",
      "623/636 [============================>.] - ETA: 0s - loss: 0.5173 - mae: 0.3237\n",
      "Epoch 130: val_mae did not improve from 0.31189\n",
      "636/636 [==============================] - 1s 2ms/step - loss: 0.5158 - mae: 0.3232 - val_loss: 0.5034 - val_mae: 0.3188 - lr: 1.0000e-05\n",
      "Epoch 131/1000\n",
      "626/636 [============================>.] - ETA: 0s - loss: 0.5157 - mae: 0.3220\n",
      "Epoch 131: val_mae did not improve from 0.31189\n",
      "636/636 [==============================] - 1s 2ms/step - loss: 0.5159 - mae: 0.3221 - val_loss: 0.5047 - val_mae: 0.3240 - lr: 1.0000e-05\n",
      "Epoch 132/1000\n",
      "635/636 [============================>.] - ETA: 0s - loss: 0.5164 - mae: 0.3231\n",
      "Epoch 132: val_mae did not improve from 0.31189\n",
      "636/636 [==============================] - 2s 2ms/step - loss: 0.5159 - mae: 0.3229 - val_loss: 0.5034 - val_mae: 0.3183 - lr: 1.0000e-05\n",
      "Epoch 133/1000\n",
      "629/636 [============================>.] - ETA: 0s - loss: 0.5142 - mae: 0.3229\n",
      "Epoch 133: val_mae did not improve from 0.31189\n",
      "636/636 [==============================] - 1s 2ms/step - loss: 0.5160 - mae: 0.3233 - val_loss: 0.5037 - val_mae: 0.3142 - lr: 1.0000e-05\n",
      "Epoch 134/1000\n",
      "634/636 [============================>.] - ETA: 0s - loss: 0.5171 - mae: 0.3233\n",
      "Epoch 134: val_mae did not improve from 0.31189\n",
      "636/636 [==============================] - 2s 2ms/step - loss: 0.5165 - mae: 0.3232 - val_loss: 0.5062 - val_mae: 0.3290 - lr: 1.0000e-05\n",
      "Epoch 135/1000\n",
      "632/636 [============================>.] - ETA: 0s - loss: 0.5150 - mae: 0.3228\n",
      "Epoch 135: val_mae did not improve from 0.31189\n",
      "636/636 [==============================] - 1s 2ms/step - loss: 0.5162 - mae: 0.3229 - val_loss: 0.5032 - val_mae: 0.3190 - lr: 1.0000e-05\n",
      "Epoch 136/1000\n",
      "612/636 [===========================>..] - ETA: 0s - loss: 0.5133 - mae: 0.3213\n",
      "Epoch 136: val_mae did not improve from 0.31189\n",
      "636/636 [==============================] - 1s 2ms/step - loss: 0.5157 - mae: 0.3225 - val_loss: 0.5039 - val_mae: 0.3216 - lr: 1.0000e-05\n",
      "Epoch 137/1000\n",
      "622/636 [============================>.] - ETA: 0s - loss: 0.5179 - mae: 0.3235\n",
      "Epoch 137: val_mae did not improve from 0.31189\n",
      "636/636 [==============================] - 2s 2ms/step - loss: 0.5149 - mae: 0.3229 - val_loss: 0.5024 - val_mae: 0.3130 - lr: 1.0000e-05\n",
      "Epoch 138/1000\n",
      "615/636 [============================>.] - ETA: 0s - loss: 0.5148 - mae: 0.3220\n",
      "Epoch 138: val_mae did not improve from 0.31189\n",
      "636/636 [==============================] - 1s 2ms/step - loss: 0.5155 - mae: 0.3221 - val_loss: 0.5039 - val_mae: 0.3137 - lr: 1.0000e-05\n",
      "Epoch 139/1000\n",
      "630/636 [============================>.] - ETA: 0s - loss: 0.5132 - mae: 0.3218\n",
      "Epoch 139: val_mae did not improve from 0.31189\n",
      "636/636 [==============================] - 1s 2ms/step - loss: 0.5151 - mae: 0.3225 - val_loss: 0.5027 - val_mae: 0.3162 - lr: 1.0000e-05\n",
      "Epoch 140/1000\n",
      "623/636 [============================>.] - ETA: 0s - loss: 0.5117 - mae: 0.3207\n",
      "Epoch 140: val_mae did not improve from 0.31189\n",
      "636/636 [==============================] - 1s 2ms/step - loss: 0.5144 - mae: 0.3215 - val_loss: 0.5072 - val_mae: 0.3318 - lr: 1.0000e-05\n",
      "Epoch 141/1000\n",
      "617/636 [============================>.] - ETA: 0s - loss: 0.5192 - mae: 0.3248\n",
      "Epoch 141: val_mae did not improve from 0.31189\n",
      "636/636 [==============================] - 1s 2ms/step - loss: 0.5158 - mae: 0.3234 - val_loss: 0.5031 - val_mae: 0.3151 - lr: 1.0000e-05\n",
      "Epoch 142/1000\n",
      "629/636 [============================>.] - ETA: 0s - loss: 0.5146 - mae: 0.3219\n",
      "Epoch 142: val_mae did not improve from 0.31189\n",
      "636/636 [==============================] - 2s 2ms/step - loss: 0.5148 - mae: 0.3219 - val_loss: 0.5054 - val_mae: 0.3134 - lr: 1.0000e-05\n",
      "Epoch 143/1000\n",
      "626/636 [============================>.] - ETA: 0s - loss: 0.5198 - mae: 0.3235\n",
      "Epoch 143: val_mae did not improve from 0.31189\n",
      "636/636 [==============================] - 1s 2ms/step - loss: 0.5147 - mae: 0.3224 - val_loss: 0.5027 - val_mae: 0.3119 - lr: 1.0000e-05\n",
      "Epoch 144/1000\n",
      "616/636 [============================>.] - ETA: 0s - loss: 0.5169 - mae: 0.3222\n",
      "Epoch 144: val_mae did not improve from 0.31189\n",
      "636/636 [==============================] - 1s 2ms/step - loss: 0.5149 - mae: 0.3215 - val_loss: 0.5026 - val_mae: 0.3134 - lr: 1.0000e-05\n",
      "Epoch 145/1000\n",
      "616/636 [============================>.] - ETA: 0s - loss: 0.5131 - mae: 0.3216\n",
      "Epoch 145: val_mae did not improve from 0.31189\n",
      "636/636 [==============================] - 2s 2ms/step - loss: 0.5140 - mae: 0.3219 - val_loss: 0.5033 - val_mae: 0.3140 - lr: 1.0000e-05\n",
      "Epoch 146/1000\n",
      "626/636 [============================>.] - ETA: 0s - loss: 0.5162 - mae: 0.3225\n",
      "Epoch 146: val_mae did not improve from 0.31189\n",
      "636/636 [==============================] - 1s 2ms/step - loss: 0.5149 - mae: 0.3222 - val_loss: 0.5037 - val_mae: 0.3172 - lr: 1.0000e-05\n",
      "Epoch 147/1000\n",
      "621/636 [============================>.] - ETA: 0s - loss: 0.5103 - mae: 0.3211\n",
      "Epoch 147: val_mae did not improve from 0.31189\n",
      "636/636 [==============================] - 2s 2ms/step - loss: 0.5142 - mae: 0.3220 - val_loss: 0.5032 - val_mae: 0.3200 - lr: 1.0000e-05\n",
      "Epoch 148/1000\n",
      "635/636 [============================>.] - ETA: 0s - loss: 0.5143 - mae: 0.3234\n",
      "Epoch 148: val_mae did not improve from 0.31189\n",
      "636/636 [==============================] - 2s 2ms/step - loss: 0.5142 - mae: 0.3234 - val_loss: 0.5022 - val_mae: 0.3167 - lr: 1.0000e-05\n",
      "Epoch 149/1000\n",
      "621/636 [============================>.] - ETA: 0s - loss: 0.5168 - mae: 0.3245\n",
      "Epoch 149: val_mae improved from 0.31189 to 0.31154, saving model to checkpoint_dir\\training_1.ckpt\n",
      "INFO:tensorflow:Assets written to: checkpoint_dir\\training_1.ckpt\\assets\n",
      "636/636 [==============================] - 2s 3ms/step - loss: 0.5139 - mae: 0.3235 - val_loss: 0.5036 - val_mae: 0.3115 - lr: 1.0000e-05\n",
      "Epoch 150/1000\n",
      "636/636 [==============================] - ETA: 0s - loss: 0.5136 - mae: 0.3214\n",
      "Epoch 150: val_mae did not improve from 0.31154\n",
      "636/636 [==============================] - 2s 3ms/step - loss: 0.5136 - mae: 0.3214 - val_loss: 0.5041 - val_mae: 0.3210 - lr: 1.0000e-05\n",
      "Epoch 151/1000\n",
      "629/636 [============================>.] - ETA: 0s - loss: 0.5120 - mae: 0.3207\n",
      "Epoch 151: val_mae did not improve from 0.31154\n",
      "636/636 [==============================] - 2s 2ms/step - loss: 0.5134 - mae: 0.3211 - val_loss: 0.5035 - val_mae: 0.3139 - lr: 1.0000e-05\n",
      "Epoch 152/1000\n",
      "633/636 [============================>.] - ETA: 0s - loss: 0.5137 - mae: 0.3227\n",
      "Epoch 152: val_mae did not improve from 0.31154\n",
      "636/636 [==============================] - 2s 2ms/step - loss: 0.5138 - mae: 0.3227 - val_loss: 0.5035 - val_mae: 0.3117 - lr: 1.0000e-05\n",
      "Epoch 153/1000\n",
      "621/636 [============================>.] - ETA: 0s - loss: 0.5134 - mae: 0.3215\n",
      "Epoch 153: val_mae did not improve from 0.31154\n",
      "636/636 [==============================] - 2s 2ms/step - loss: 0.5130 - mae: 0.3216 - val_loss: 0.5028 - val_mae: 0.3161 - lr: 1.0000e-05\n",
      "Epoch 154/1000\n",
      "629/636 [============================>.] - ETA: 0s - loss: 0.5130 - mae: 0.3211\n",
      "Epoch 154: val_mae did not improve from 0.31154\n",
      "636/636 [==============================] - 2s 2ms/step - loss: 0.5139 - mae: 0.3214 - val_loss: 0.5047 - val_mae: 0.3251 - lr: 1.0000e-05\n",
      "Epoch 155/1000\n",
      "631/636 [============================>.] - ETA: 0s - loss: 0.5130 - mae: 0.3210\n",
      "Epoch 155: val_mae did not improve from 0.31154\n",
      "636/636 [==============================] - 2s 2ms/step - loss: 0.5131 - mae: 0.3211 - val_loss: 0.5027 - val_mae: 0.3167 - lr: 1.0000e-05\n",
      "Epoch 156/1000\n",
      "615/636 [============================>.] - ETA: 0s - loss: 0.5093 - mae: 0.3206\n",
      "Epoch 156: val_mae did not improve from 0.31154\n",
      "636/636 [==============================] - 2s 2ms/step - loss: 0.5129 - mae: 0.3216 - val_loss: 0.5035 - val_mae: 0.3206 - lr: 1.0000e-05\n",
      "Epoch 157/1000\n",
      "622/636 [============================>.] - ETA: 0s - loss: 0.5149 - mae: 0.3231\n",
      "Epoch 157: val_mae improved from 0.31154 to 0.30944, saving model to checkpoint_dir\\training_1.ckpt\n",
      "INFO:tensorflow:Assets written to: checkpoint_dir\\training_1.ckpt\\assets\n",
      "636/636 [==============================] - 2s 3ms/step - loss: 0.5128 - mae: 0.3227 - val_loss: 0.5025 - val_mae: 0.3094 - lr: 1.0000e-05\n",
      "Epoch 158/1000\n",
      "620/636 [============================>.] - ETA: 0s - loss: 0.5150 - mae: 0.3224\n",
      "Epoch 158: val_mae did not improve from 0.30944\n",
      "636/636 [==============================] - 2s 3ms/step - loss: 0.5133 - mae: 0.3216 - val_loss: 0.5027 - val_mae: 0.3105 - lr: 1.0000e-05\n",
      "Epoch 159/1000\n",
      "614/636 [===========================>..] - ETA: 0s - loss: 0.5159 - mae: 0.3216\n",
      "Epoch 159: val_mae did not improve from 0.30944\n",
      "636/636 [==============================] - 2s 2ms/step - loss: 0.5126 - mae: 0.3211 - val_loss: 0.5027 - val_mae: 0.3173 - lr: 1.0000e-05\n",
      "Epoch 160/1000\n",
      "625/636 [============================>.] - ETA: 0s - loss: 0.5124 - mae: 0.3209\n",
      "Epoch 160: val_mae did not improve from 0.30944\n",
      "636/636 [==============================] - 2s 2ms/step - loss: 0.5127 - mae: 0.3210 - val_loss: 0.5037 - val_mae: 0.3206 - lr: 1.0000e-05\n",
      "Epoch 161/1000\n",
      "634/636 [============================>.] - ETA: 0s - loss: 0.5114 - mae: 0.3205\n",
      "Epoch 161: val_mae did not improve from 0.30944\n",
      "636/636 [==============================] - 2s 2ms/step - loss: 0.5126 - mae: 0.3208 - val_loss: 0.5042 - val_mae: 0.3228 - lr: 1.0000e-05\n",
      "Epoch 162/1000\n",
      "635/636 [============================>.] - ETA: 0s - loss: 0.5128 - mae: 0.3214\n",
      "Epoch 162: val_mae did not improve from 0.30944\n",
      "636/636 [==============================] - 2s 2ms/step - loss: 0.5122 - mae: 0.3212 - val_loss: 0.5034 - val_mae: 0.3120 - lr: 1.0000e-05\n",
      "Epoch 163/1000\n",
      "634/636 [============================>.] - ETA: 0s - loss: 0.5108 - mae: 0.3209\n",
      "Epoch 163: val_mae did not improve from 0.30944\n",
      "636/636 [==============================] - 2s 2ms/step - loss: 0.5120 - mae: 0.3211 - val_loss: 0.5037 - val_mae: 0.3197 - lr: 1.0000e-05\n",
      "Epoch 164/1000\n",
      "615/636 [============================>.] - ETA: 0s - loss: 0.5137 - mae: 0.3217\n",
      "Epoch 164: val_mae did not improve from 0.30944\n",
      "636/636 [==============================] - 1s 2ms/step - loss: 0.5121 - mae: 0.3214 - val_loss: 0.5025 - val_mae: 0.3177 - lr: 1.0000e-05\n",
      "Epoch 165/1000\n",
      "635/636 [============================>.] - ETA: 0s - loss: 0.5119 - mae: 0.3211\n",
      "Epoch 165: val_mae did not improve from 0.30944\n",
      "636/636 [==============================] - 2s 2ms/step - loss: 0.5114 - mae: 0.3209 - val_loss: 0.5039 - val_mae: 0.3264 - lr: 1.0000e-05\n",
      "Epoch 166/1000\n",
      "617/636 [============================>.] - ETA: 0s - loss: 0.5093 - mae: 0.3198\n",
      "Epoch 166: val_mae did not improve from 0.30944\n",
      "636/636 [==============================] - 1s 2ms/step - loss: 0.5119 - mae: 0.3204 - val_loss: 0.5016 - val_mae: 0.3147 - lr: 1.0000e-05\n",
      "Epoch 167/1000\n",
      "633/636 [============================>.] - ETA: 0s - loss: 0.5129 - mae: 0.3214\n",
      "Epoch 167: val_mae did not improve from 0.30944\n",
      "636/636 [==============================] - 1s 2ms/step - loss: 0.5122 - mae: 0.3213 - val_loss: 0.5033 - val_mae: 0.3215 - lr: 1.0000e-05\n",
      "Epoch 168/1000\n",
      "636/636 [==============================] - ETA: 0s - loss: 0.5113 - mae: 0.3206\n",
      "Epoch 168: val_mae did not improve from 0.30944\n",
      "636/636 [==============================] - 2s 3ms/step - loss: 0.5113 - mae: 0.3206 - val_loss: 0.5023 - val_mae: 0.3129 - lr: 1.0000e-05\n",
      "Epoch 169/1000\n",
      "635/636 [============================>.] - ETA: 0s - loss: 0.5095 - mae: 0.3207\n",
      "Epoch 169: val_mae did not improve from 0.30944\n",
      "636/636 [==============================] - 2s 3ms/step - loss: 0.5113 - mae: 0.3209 - val_loss: 0.5053 - val_mae: 0.3127 - lr: 1.0000e-05\n",
      "Epoch 170/1000\n",
      "621/636 [============================>.] - ETA: 0s - loss: 0.5061 - mae: 0.3186\n",
      "Epoch 170: val_mae did not improve from 0.30944\n",
      "636/636 [==============================] - 2s 3ms/step - loss: 0.5108 - mae: 0.3203 - val_loss: 0.5043 - val_mae: 0.3209 - lr: 1.0000e-05\n",
      "Epoch 171/1000\n",
      "632/636 [============================>.] - ETA: 0s - loss: 0.5115 - mae: 0.3206\n",
      "Epoch 171: val_mae did not improve from 0.30944\n",
      "636/636 [==============================] - 1s 2ms/step - loss: 0.5114 - mae: 0.3209 - val_loss: 0.5044 - val_mae: 0.3225 - lr: 1.0000e-05\n",
      "Epoch 172/1000\n",
      "635/636 [============================>.] - ETA: 0s - loss: 0.5106 - mae: 0.3200\n",
      "Epoch 172: val_mae did not improve from 0.30944\n",
      "636/636 [==============================] - 1s 2ms/step - loss: 0.5106 - mae: 0.3201 - val_loss: 0.5018 - val_mae: 0.3159 - lr: 1.0000e-05\n",
      "Epoch 173/1000\n",
      "625/636 [============================>.] - ETA: 0s - loss: 0.5094 - mae: 0.3207\n",
      "Epoch 173: val_mae did not improve from 0.30944\n",
      "636/636 [==============================] - 1s 2ms/step - loss: 0.5109 - mae: 0.3211 - val_loss: 0.5035 - val_mae: 0.3209 - lr: 1.0000e-05\n",
      "Epoch 174/1000\n",
      "635/636 [============================>.] - ETA: 0s - loss: 0.5113 - mae: 0.3204\n",
      "Epoch 174: val_mae did not improve from 0.30944\n",
      "636/636 [==============================] - 2s 3ms/step - loss: 0.5107 - mae: 0.3203 - val_loss: 0.5021 - val_mae: 0.3169 - lr: 1.0000e-05\n",
      "Epoch 175/1000\n",
      "633/636 [============================>.] - ETA: 0s - loss: 0.5129 - mae: 0.3215\n",
      "Epoch 175: val_mae did not improve from 0.30944\n",
      "636/636 [==============================] - 2s 3ms/step - loss: 0.5107 - mae: 0.3206 - val_loss: 0.5027 - val_mae: 0.3126 - lr: 1.0000e-05\n",
      "Epoch 176/1000\n",
      "620/636 [============================>.] - ETA: 0s - loss: 0.5100 - mae: 0.3199\n",
      "Epoch 176: val_mae did not improve from 0.30944\n",
      "636/636 [==============================] - 2s 3ms/step - loss: 0.5098 - mae: 0.3204 - val_loss: 0.5021 - val_mae: 0.3125 - lr: 1.0000e-05\n",
      "Epoch 177/1000\n",
      "631/636 [============================>.] - ETA: 0s - loss: 0.5126 - mae: 0.3217\n",
      "Epoch 177: val_mae did not improve from 0.30944\n",
      "636/636 [==============================] - 2s 2ms/step - loss: 0.5103 - mae: 0.3211 - val_loss: 0.5031 - val_mae: 0.3215 - lr: 1.0000e-05\n",
      "Epoch 178/1000\n",
      "617/636 [============================>.] - ETA: 0s - loss: 0.5126 - mae: 0.3213\n",
      "Epoch 178: val_mae did not improve from 0.30944\n",
      "636/636 [==============================] - 2s 3ms/step - loss: 0.5098 - mae: 0.3201 - val_loss: 0.5029 - val_mae: 0.3130 - lr: 1.0000e-05\n",
      "Epoch 179/1000\n",
      "621/636 [============================>.] - ETA: 0s - loss: 0.5125 - mae: 0.3201\n",
      "Epoch 179: val_mae did not improve from 0.30944\n",
      "636/636 [==============================] - 2s 3ms/step - loss: 0.5099 - mae: 0.3194 - val_loss: 0.5026 - val_mae: 0.3195 - lr: 1.0000e-05\n",
      "Epoch 180/1000\n",
      "631/636 [============================>.] - ETA: 0s - loss: 0.5071 - mae: 0.3202\n",
      "Epoch 180: val_mae did not improve from 0.30944\n",
      "636/636 [==============================] - 2s 4ms/step - loss: 0.5094 - mae: 0.3205 - val_loss: 0.5025 - val_mae: 0.3155 - lr: 1.0000e-05\n",
      "Epoch 181/1000\n",
      "632/636 [============================>.] - ETA: 0s - loss: 0.5108 - mae: 0.3199\n",
      "Epoch 181: val_mae did not improve from 0.30944\n",
      "636/636 [==============================] - 2s 3ms/step - loss: 0.5096 - mae: 0.3197 - val_loss: 0.5028 - val_mae: 0.3171 - lr: 1.0000e-05\n",
      "Epoch 182/1000\n",
      "635/636 [============================>.] - ETA: 0s - loss: 0.5090 - mae: 0.3204\n",
      "Epoch 182: val_mae did not improve from 0.30944\n",
      "636/636 [==============================] - 2s 2ms/step - loss: 0.5098 - mae: 0.3207 - val_loss: 0.5027 - val_mae: 0.3118 - lr: 1.0000e-05\n",
      "Epoch 183/1000\n",
      "618/636 [============================>.] - ETA: 0s - loss: 0.5094 - mae: 0.3198\n",
      "Epoch 183: val_mae did not improve from 0.30944\n",
      "636/636 [==============================] - 2s 3ms/step - loss: 0.5097 - mae: 0.3202 - val_loss: 0.5052 - val_mae: 0.3273 - lr: 1.0000e-05\n",
      "Epoch 184/1000\n",
      "634/636 [============================>.] - ETA: 0s - loss: 0.5095 - mae: 0.3199\n",
      "Epoch 184: val_mae did not improve from 0.30944\n",
      "636/636 [==============================] - 2s 3ms/step - loss: 0.5092 - mae: 0.3198 - val_loss: 0.5026 - val_mae: 0.3129 - lr: 1.0000e-05\n",
      "Epoch 185/1000\n",
      "612/636 [===========================>..] - ETA: 0s - loss: 0.5080 - mae: 0.3192\n",
      "Epoch 185: val_mae did not improve from 0.30944\n",
      "636/636 [==============================] - 2s 2ms/step - loss: 0.5091 - mae: 0.3197 - val_loss: 0.5028 - val_mae: 0.3188 - lr: 1.0000e-05\n",
      "Epoch 186/1000\n",
      "620/636 [============================>.] - ETA: 0s - loss: 0.5059 - mae: 0.3211\n",
      "Epoch 186: val_mae did not improve from 0.30944\n",
      "636/636 [==============================] - 2s 2ms/step - loss: 0.5095 - mae: 0.3215 - val_loss: 0.5022 - val_mae: 0.3118 - lr: 1.0000e-05\n",
      "Epoch 186: early stopping\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train, validation_split=0.2, epochs=1000, callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading the saved weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(128, input_dim=5, activation='relu'))\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dense(1024, activation='relu'))\n",
    "model.add(Dense(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='mse', optimizer=Adam(0.01), metrics=['mae'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.checkpoint.checkpoint.CheckpointLoadStatus at 0x2ba0dc2c400>"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_weights('checkpoint_dir/training_1.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "341/341 [==============================] - 1s 2ms/step - loss: 0.5603 - mae: 0.3266\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Saving the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: saved_model/dnn_unit1\\assets\n"
     ]
    }
   ],
   "source": [
    "model.save('saved_model/dnn_unit1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model = tf.keras.models.load_model('saved_model/dnn_unit1')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_22\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_80 (Dense)            (None, 128)               768       \n",
      "                                                                 \n",
      " dense_81 (Dense)            (None, 256)               33024     \n",
      "                                                                 \n",
      " dense_82 (Dense)            (None, 512)               131584    \n",
      "                                                                 \n",
      " dense_83 (Dense)            (None, 1024)              525312    \n",
      "                                                                 \n",
      " dense_84 (Dense)            (None, 1)                 1025      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 691,713\n",
      "Trainable params: 691,713\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "new_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "341/341 [==============================] - 1s 2ms/step - loss: 0.5603 - mae: 0.3266\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.560325562953949, 0.3265897035598755]"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_model.evaluate(X_test, y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "cd38e5a88334fb4f0a3a7cb4807489c1d2190f8b1b2649dc0fd7c6a431f8e87e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
